"""
╔═══════════════════════════════════════════════════════════════════════════════════╗
║ ZeMosaic / ZeSeestarStacker Project                                               ║
║                                                                                   ║
║ Auteur  : Tinystork, seigneur des couteaux à beurre (aka Tristan Nauleau)         ║
║ Partenaire : J.A.R.V.I.S. (/ˈdʒɑːrvɪs/) — Just a Rather Very Intelligent System   ║
║              (aka ChatGPT, Grand Maître du ciselage de code)                      ║
║                                                                                   ║
║ Licence : GNU General Public License v3.0 (GPL-3.0)                               ║
║                                                                                   ║
║ Description :                                                                     ║
║   Ce programme a été forgé à la lueur des pixels et de la caféine,                ║
║   dans le but noble de transformer des nuages de photons en art                   ║
║   astronomique. Si vous l’utilisez, pensez à dire “merci”,                        ║
║   à lever les yeux vers le ciel, ou à citer Tinystork et J.A.R.V.I.S.             ║
║   (le karma des développeurs en dépend).                                          ║
║                                                                                   ║
║ Avertissement :                                                                   ║
║   Aucune IA ni aucun couteau à beurre n’a été blessé durant le                    ║
║   développement de ce code.                                                       ║
╚═══════════════════════════════════════════════════════════════════════════════════╝


╔═══════════════════════════════════════════════════════════════════════════════════╗
║ ZeMosaic / ZeSeestarStacker Project                                               ║
║                                                                                   ║
║ Author  : Tinystork, Lord of the Butter Knives (aka Tristan Nauleau)              ║
║ Partner : J.A.R.V.I.S. (/ˈdʒɑːrvɪs/) — Just a Rather Very Intelligent System      ║
║           (aka ChatGPT, Grand Master of Code Chiseling)                           ║
║                                                                                   ║
║ License : GNU General Public License v3.0 (GPL-3.0)                               ║
║                                                                                   ║
║ Description:                                                                      ║
║   This program was forged under the sacred light of pixels and                    ║
║   caffeine, with the noble intent of turning clouds of photons into               ║
║   astronomical art. If you use it, please consider saying “thanks,”               ║
║   gazing at the stars, or crediting Tinystork and J.A.R.V.I.S. —                  ║
║   developer karma depends on it.                                                  ║
║                                                                                   ║
║ Disclaimer:                                                                       ║
║   No AIs or butter knives were harmed in the making of this code.                 ║
╚═══════════════════════════════════════════════════════════════════════════════════╝
"""

from __future__ import annotations

# zemosaic_worker.py

import os
import copy
import inspect
import shutil
import time
import traceback
import gc
import logging
import inspect  # Pas utilisé directement ici, mais peut être utile pour des introspections futures
import math
import hashlib
import json
from datetime import datetime
import psutil
import tempfile
import glob
import uuid
import multiprocessing
import threading
import itertools
import platform
import importlib.util
from pathlib import Path
from threading import Lock
from dataclasses import dataclass, replace
from typing import Callable, Any, Iterable, Optional
from types import SimpleNamespace

import numpy as np

try:
    import grid_mode
except Exception:
    grid_mode = None

from zemosaic_resource_telemetry import (
    ResourceTelemetryController,
    _sample_runtime_resources_for_telemetry,
)

from core.path_helpers import (
    casefold_path,
    expand_to_path,
    normpath_segments,
    safe_path_exists,
    safe_path_getsize,
    safe_path_isdir,
    safe_path_isfile,
)

try:
    import lecropper  # noqa: F401

    _LECROPPER_AVAILABLE = True
except Exception:
    lecropper = None
    _LECROPPER_AVAILABLE = False


from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, wait, FIRST_COMPLETED, as_completed
# BrokenProcessPool moved under concurrent.futures.process in modern Python
from concurrent.futures.process import BrokenProcessPool

try:
    from parallel_utils import (
        ParallelCapabilities,
        ParallelPlan,
        auto_tune_parallel_plan,
        detect_parallel_capabilities,
    )

    PARALLEL_HELPERS_AVAILABLE = True
except Exception:
    ParallelCapabilities = None  # type: ignore
    ParallelPlan = None  # type: ignore

    def detect_parallel_capabilities():
        return None

    def auto_tune_parallel_plan(*args, **kwargs):
        return None

    PARALLEL_HELPERS_AVAILABLE = False

# ZeQualityMT (quality gate for Master Tiles)
try:
    from zequalityMT import quality_metrics as _zq_quality_metrics
except Exception:
    _zq_quality_metrics = None

_ZEQUALITY_WARNING_EMITTED = False
_ZEQUALITY_WARNING_LOCK = threading.Lock()

def _fallback_runtime_temp_dir() -> Path:
    root = Path(tempfile.gettempdir()) / "zemosaic_runtime"
    try:
        root.mkdir(parents=True, exist_ok=True)
    except Exception:
        root = Path(tempfile.gettempdir())
    return root


try:
    from zemosaic_utils import EXCLUDED_DIRS, is_path_excluded, get_runtime_temp_dir
except Exception:
    EXCLUDED_DIRS = frozenset({"unaligned_by_zemosaic"})

    def is_path_excluded(path, excluded_dirs=None):
        import os

        parts = set(_normpath_parts(path))
        dirs = set(excluded_dirs) if excluded_dirs else set()
        return any(d in parts for d in (dirs or {"unaligned_by_zemosaic"}))

    get_runtime_temp_dir = _fallback_runtime_temp_dir


try:
    from zemosaic_align_stack import _poststack_rgb_equalization
except Exception:
    _poststack_rgb_equalization = None


UNALIGNED_DIRNAME = "unaligned_by_zemosaic"
_UNALIGNED_LOCK = Lock()

ALPHA_OPACITY_THRESHOLD = 0.5  # Mask values >= threshold are treated as opaque
QUALITY_GATE_ALPHA_SOFT_THRESHOLD = 0.85  # Partially transparent pixels are ignored during quality gate

GLOBAL_COVERAGE_SUMMARY_THRESHOLD_FRAC = 0.0025
GLOBAL_COVERAGE_SUMMARY_MIN_ABS = 1e-3


def _dbg_rgb_stats(
    label: str,
    rgb: np.ndarray | None,
    *,
    coverage: np.ndarray | None = None,
    alpha: np.ndarray | None = None,
    logger: logging.Logger | None = None,
) -> None:
    """Emit compact RGB stats for debugging when DEBUG logging is enabled."""

    if logger is None or not logger.isEnabledFor(logging.DEBUG):
        return
    if rgb is None:
        return

    try:
        arr = np.asarray(rgb)
        if arr.size == 0:
            return
        if arr.ndim == 2:
            arr = arr[..., None]
        if arr.ndim == 3 and arr.shape[-1] != 3:
            if arr.shape[0] == 3 and arr.shape[-1] != 3:
                arr = np.moveaxis(arr, 0, -1)
            if arr.shape[-1] == 1:
                arr = np.repeat(arr, 3, axis=-1)
            elif arr.shape[-1] > 3:
                arr = arr[..., :3]
        arr = np.asarray(arr, dtype=np.float32, order="C")
    except Exception:
        return

    if arr.ndim != 3 or arr.shape[-1] != 3:
        return

    h, w, _ = arr.shape
    total_px = float(max(1, h * w))

    valid_mask = np.all(np.isfinite(arr), axis=-1)

    if alpha is not None:
        try:
            alpha_arr = np.asarray(alpha)
            if alpha_arr.ndim >= 2:
                alpha_hw = alpha_arr[..., 0] if alpha_arr.shape[-1] == 1 else alpha_arr
                alpha_hw = np.asarray(alpha_hw, dtype=np.float32)
                if alpha_hw.shape[:2] == valid_mask.shape:
                    valid_mask &= alpha_hw > ALPHA_OPACITY_THRESHOLD
        except Exception:
            pass

    cov_weights: np.ndarray | None = None
    if coverage is not None:
        try:
            coverage_arr = np.asarray(coverage, dtype=np.float32)
            if coverage_arr.ndim >= 2:
                coverage_hw = coverage_arr[..., 0] if coverage_arr.shape[-1] == 1 else coverage_arr
                if coverage_hw.shape[:2] == valid_mask.shape:
                    cov_weights = coverage_hw
                    valid_mask &= coverage_hw > 0
        except Exception:
            cov_weights = None

    if not np.any(valid_mask):
        logger.debug("[DBG_RGB] %s valid=0", label)
        return

    stats = []
    medians = []
    mins = []
    means = []
    for c in range(3):
        channel = arr[..., c]
        masked = np.where(valid_mask, channel, np.nan)
        mins.append(float(np.nanmin(masked)))
        means.append(float(np.nanmean(masked)))
        medians.append(float(np.nanmedian(masked)))

    valid_fraction = float(np.count_nonzero(valid_mask)) / total_px
    eps = 1e-6
    ratio_g_r = medians[1] / max(abs(medians[0]), eps)
    ratio_g_b = medians[1] / max(abs(medians[2]), eps)

    weighted_mean = None
    if cov_weights is not None:
        try:
            weights = np.where(valid_mask, cov_weights, 0.0)
            weight_sum = float(np.sum(weights))
            if weight_sum > 0:
                weighted_mean = [
                    float(np.sum(arr[..., c] * weights) / weight_sum) for c in range(3)
                ]
        except Exception:
            weighted_mean = None

    def _fmt_triplet(values: list[float]) -> str:
        return "[" + ",".join(f"{v:.6g}" for v in values) + "]"

    stats.append(f"valid={valid_fraction:.3f}")
    stats.append(f"min={_fmt_triplet(mins)}")
    stats.append(f"mean={_fmt_triplet(means)}")
    stats.append(f"median={_fmt_triplet(medians)}")
    stats.append(f"ratio_G_R={ratio_g_r:.6g}")
    stats.append(f"ratio_G_B={ratio_g_b:.6g}")
    if weighted_mean is not None:
        stats.append(f"cov_weighted_mean={_fmt_triplet(weighted_mean)}")

    logger.debug(
        "[DBG_RGB] %s shape=%s dtype=%s %s",
        label,
        tuple(arr.shape),
        arr.dtype,
        " ".join(stats),
    )


def _coerce_bool_flag(value) -> bool | None:
    """Interpret various truthy/falsy representations coming from configs/UI."""

    if value is None:
        return None
    if isinstance(value, bool):
        return value
    if isinstance(value, (int, float)) and not isinstance(value, bool):
        return value != 0
    if isinstance(value, str):
        normalized = value.strip().lower()
        if not normalized:
            return None
        if normalized in {"1", "true", "yes", "on", "enable", "enabled"}:
            return True
        if normalized in {"0", "false", "no", "off", "disable", "disabled"}:
            return False
    try:
        return bool(value)
    except Exception:
        return None


def _select_debug_tile_ids(tile_order: list[int]) -> set[int]:
    """Select a small sample of tile ids for debug instrumentation."""

    if not tile_order:
        return set()

    indices = {0, len(tile_order) // 2, len(tile_order) - 1}
    selected: set[int] = set()
    for idx in indices:
        if 0 <= idx < len(tile_order):
            try:
                selected.add(int(tile_order[idx]))
            except Exception:
                continue
    return selected


def _safe_basename(path: str | os.PathLike | None) -> str:
    """Return a best-effort basename for logging/UI purposes."""

    if not path:
        return "<unknown>"
    try:
        name = Path(path).name
        return name or str(path)
    except Exception:
        return str(path)


def _path_exists(path: str | os.PathLike | None) -> bool:
    return safe_path_exists(path)


def _path_isfile(path: str | os.PathLike | None) -> bool:
    return safe_path_isfile(path)


def _path_isdir(path: str | os.PathLike | None) -> bool:
    return safe_path_isdir(path)


def _path_getsize(path: str | os.PathLike | None) -> int:
    return safe_path_getsize(path)


def _normcase_path(path: str | os.PathLike | None) -> str:
    return casefold_path(path, expanduser=True)


def _normpath_parts(path: str | os.PathLike | None) -> tuple[str, ...]:
    return normpath_segments(path)


def detect_grid_mode(input_folder: str | os.PathLike | None) -> bool:
    """Return True when Grid/Survey mode should be activated (stack_plan.csv present)."""

    if grid_mode and hasattr(grid_mode, "detect_grid_mode"):
        try:
            return bool(grid_mode.detect_grid_mode(input_folder))  # type: ignore[attr-defined]
        except Exception as exc:
            try:
                logger.debug("[GRID] detect_grid_mode failed: %s", exc)
            except Exception:
                pass
    try:
        return Path(input_folder or "").expanduser().joinpath("stack_plan.csv").is_file()
    except Exception:
        return False


def _move_to_unaligned_safe(
    src_path: str | os.PathLike,
    input_root: str | os.PathLike,
    *,
    logger: logging.Logger = logging.getLogger(__name__),
) -> tuple[str, Path | None]:
    """Thread-safe move helper for ``unaligned_by_zemosaic``.

    Parameters
    ----------
    src_path : str | os.PathLike
        Path to the source file that should be relocated.
    input_root : str | os.PathLike
        Root input directory that owns the ``unaligned`` folder.
    logger : logging.Logger, optional
        Logger instance used for diagnostics.

    Returns
    -------
    tuple[str, Path | None]
        ``(status, destination_path)`` where ``status`` is one of
        ``{"moved", "skipped_excluded", "missing", "already_moved", "conflict", "failed"}``.
        ``destination_path`` is provided when a target directory is known.
    """

    try:
        src = Path(src_path).expanduser().resolve(strict=False)
    except Exception:
        src = Path(src_path)

    try:
        root = Path(input_root).expanduser().resolve(strict=False)
    except Exception:
        root = Path(input_root)

    # Skip if the source is already part of an excluded directory
    try:
        if is_path_excluded(src, EXCLUDED_DIRS):
            try:
                logger.warning("Skip move: path already excluded: %s", src)
            except Exception:
                pass
            return "skipped_excluded", None
    except Exception:
        if UNALIGNED_DIRNAME in set(src.parts):
            try:
                logger.warning("Skip move: path already excluded: %s", src)
            except Exception:
                pass
            return "skipped_excluded", None

    if not src.exists():
        try:
            logger.debug("Skip move: source missing (likely already moved): %s", src)
        except Exception:
            pass
        return "missing", None

    target_dir = root / UNALIGNED_DIRNAME

    with _UNALIGNED_LOCK:
        try:
            target_dir.mkdir(parents=True, exist_ok=True)
        except Exception as exc:
            try:
                logger.debug("Failed to ensure unaligned dir exists (%s): %s", target_dir, exc)
            except Exception:
                pass

    dst = target_dir / src.name

    try:
        shutil.move(str(src), str(dst))
        try:
            logger.info("Moved to '%s': %s", UNALIGNED_DIRNAME, dst)
        except Exception:
            pass
        return "moved", dst
    except FileExistsError:
        if not src.exists():
            try:
                logger.debug("Move raced (already exists), keeping destination: %s", dst)
            except Exception:
                pass
            return "already_moved", dst
        try:
            logger.debug("Move skipped (destination exists): %s", dst)
        except Exception:
            pass
        return "conflict", dst
    except FileNotFoundError:
        try:
            logger.debug("Move skipped: source vanished before move: %s", src)
        except Exception:
            pass
        return "already_moved", dst
    except Exception as exc:
        try:
            logger.warning(
                "Move to '%s' failed for %s: %s", UNALIGNED_DIRNAME, src, exc
            )
        except Exception:
            pass
        return "failed", dst


def _normalize_alpha_mask(
    mask: np.ndarray | None,
    target_hw: tuple[int, int] | None = None,
    *,
    opacity_threshold: float | None = None,
) -> np.ndarray | None:
    """Normalize any mask-like array to uint8 alpha (0-255), optionally resizing."""

    if mask is None:
        return None
    alpha = np.asarray(mask)
    if alpha.ndim == 3 and alpha.shape[-1] == 1:
        alpha = alpha[..., 0]
    alpha = np.squeeze(alpha)
    if alpha.ndim != 2:
        return None

    alpha = np.nan_to_num(alpha, nan=0.0, posinf=0.0, neginf=0.0)
    max_val = float(np.nanmax(alpha)) if alpha.size else 0.0
    if alpha.dtype.kind in {"i", "u"} and max_val > 1.0:
        alpha = alpha.astype(np.float32) / 255.0
    elif alpha.dtype.kind not in {"f"}:
        alpha = alpha.astype(np.float32)
    alpha = np.clip(alpha, 0.0, 1.0).astype(np.float32, copy=False)
    if opacity_threshold is not None:
        thresh = float(np.clip(opacity_threshold, 0.0, 1.0))
        alpha = np.where(alpha >= thresh, 1.0, 0.0).astype(np.float32, copy=False)

    if target_hw and alpha.shape != target_hw:
        try:
            import cv2  # type: ignore

            alpha = cv2.resize(
                alpha,
                (target_hw[1], target_hw[0]),
                interpolation=cv2.INTER_NEAREST,
            )
        except Exception:
            return None

    return np.clip(alpha * 255.0 + 0.5, 0, 255).astype(np.uint8)


def _combine_alpha_masks(primary: np.ndarray | None, secondary: np.ndarray | None) -> np.ndarray | None:
    """Combine two uint8 alpha maps by multiplying their transparencies."""

    if secondary is None:
        return primary
    if primary is None:
        return secondary
    if primary.shape != secondary.shape:
        try:
            import cv2  # type: ignore

            secondary = cv2.resize(
                secondary,
                (primary.shape[1], primary.shape[0]),
                interpolation=cv2.INTER_NEAREST,
            )
        except Exception:
            logger.warning(
                "Alpha combination skipped due to incompatible shapes: %s vs %s",
                primary.shape,
                secondary.shape,
            )
            return primary

    p = primary.astype(np.float32) / 255.0
    s = secondary.astype(np.float32) / 255.0
    merged = np.clip(p * s, 0.0, 1.0)
    return np.clip(merged * 255.0 + 0.5, 0, 255).astype(np.uint8)


def _apply_lecropper_pipeline(
    arr: np.ndarray | None, cfg: dict | None
) -> tuple[np.ndarray | None, np.ndarray | None]:
    """Apply quality crop + Alt-Az cleanup if the new lecropper is available.

    Returns (processed_array, optional_alpha_mask_float).
    """

    if not (_LECROPPER_AVAILABLE and isinstance(arr, np.ndarray) and arr.size):
        return arr, None

    cfg = cfg or {}
    try:
        q_enabled = bool(cfg.get("quality_crop_enabled", False))
    except Exception:
        q_enabled = False
    try:
        band_px = int(cfg.get("quality_crop_band_px", 32))
    except Exception:
        band_px = 32
    try:
        k_sigma = float(cfg.get("quality_crop_k_sigma", 2.0))
    except Exception:
        k_sigma = 2.0
    try:
        margin_px = int(cfg.get("quality_crop_margin_px", 8))
    except Exception:
        margin_px = 8
    try:
        min_run = int(cfg.get("quality_crop_min_run", 2))
    except Exception:
        min_run = 2

    try:
        az_enabled = bool(cfg.get("altaz_cleanup_enabled", False))
    except Exception:
        az_enabled = False
    try:
        az_margin = float(cfg.get("altaz_margin_percent", 5.0))
    except Exception:
        az_margin = 5.0
    try:
        az_decay = float(cfg.get("altaz_decay", 0.15))
    except Exception:
        az_decay = 0.15
    try:
        az_nanize = bool(cfg.get("altaz_nanize", True))
    except Exception:
        az_nanize = True

    out = arr
    alpha_mask_norm: np.ndarray | None = None
    try:
        if q_enabled and hasattr(lecropper, "quality_crop"):
            out = lecropper.quality_crop(
                out,
                band_px=band_px,
                k_sigma=k_sigma,
                margin_px=margin_px,
                min_run=min_run,
            )

        if az_enabled:
            mask2d = None
            mask_helper = getattr(lecropper, "mask_altaz_artifacts", None)
            base_for_mask = out
            if callable(mask_helper):
                try:
                    masked, mask2d = mask_helper(
                        base_for_mask,
                        margin_percent=az_margin,
                        decay_ratio=az_decay,
                        fill_value=None,
                        return_mask=True,
                    )
                except TypeError:
                    masked = mask_helper(
                        base_for_mask,
                        margin_percent=az_margin,
                        decay_ratio=az_decay,
                        fill_value=None,
                    )
                    mask2d = None
                if mask2d is None:
                    out = masked
                else:
                    out = base_for_mask
            elif hasattr(lecropper, "altZ_cleanup"):
                cleaned = lecropper.altZ_cleanup(
                    base_for_mask,
                    margin_percent=az_margin,
                    decay=az_decay,
                    nanize=az_nanize,
                )
                if isinstance(cleaned, (tuple, list)) and len(cleaned) >= 2:
                    out, mask2d = cleaned[:2]
                else:
                    out = cleaned
            elif hasattr(lecropper, "apply_altaz_cleanup"):
                cleaned = lecropper.apply_altaz_cleanup(
                    base_for_mask,
                    margin_percent=az_margin,
                    decay=az_decay,
                    nanize=az_nanize,
                )
                if isinstance(cleaned, (tuple, list)) and len(cleaned) >= 2:
                    out, mask2d = cleaned[:2]
                else:
                    out = cleaned

            if mask2d is not None:
                alpha_mask_norm = np.asarray(mask2d, dtype=np.float32, copy=False)
                hard_threshold = 1e-3
                mask_zero = alpha_mask_norm <= hard_threshold
                if base_for_mask.ndim == 3:
                    mask_zero = mask_zero[..., None]
                if az_nanize:
                    out = np.where(mask_zero, np.nan, base_for_mask)
                else:
                    out = np.where(mask_zero, 0.0, base_for_mask)
    except Exception as exc:
        try:
            logger.warning("lecropper pipeline skipped (err=%s)", exc)
        except Exception:
            pass
        return arr, None

    return out, alpha_mask_norm


def _apply_final_mosaic_quality_pipeline(
    final_mosaic_data: np.ndarray | None,
    final_mosaic_coverage: np.ndarray | None,
    final_alpha_map: np.ndarray | None,
    pipeline_cfg: dict[str, Any] | None,
) -> tuple[np.ndarray | None, np.ndarray | None, np.ndarray | None]:
    """Apply the lecropper-based quality pipeline to the final mosaic."""

    if (
        final_mosaic_data is None
        or not pipeline_cfg
        or not (
            pipeline_cfg.get("quality_crop_enabled")
            or pipeline_cfg.get("altaz_cleanup_enabled")
        )
    ):
        return final_mosaic_data, final_mosaic_coverage, final_alpha_map

    processed, pipeline_alpha_mask = _apply_lecropper_pipeline(final_mosaic_data, pipeline_cfg)
    if processed is not None:
        final_mosaic_data = processed
    if pipeline_alpha_mask is not None and final_mosaic_data is not None:
        pipeline_alpha_u8 = _normalize_alpha_mask(
            pipeline_alpha_mask,
            target_hw=final_mosaic_data.shape[:2],
            opacity_threshold=ALPHA_OPACITY_THRESHOLD,
        )
        if pipeline_alpha_u8 is not None:
            final_alpha_map = _combine_alpha_masks(final_alpha_map, pipeline_alpha_u8)
            if final_mosaic_coverage is not None:
                keep_mask = (pipeline_alpha_u8 > 0).astype(np.float32, copy=False)
                final_mosaic_coverage = np.where(
                    keep_mask > 0, final_mosaic_coverage, 0.0
                ).astype(np.float32, copy=False)
    return final_mosaic_data, final_mosaic_coverage, final_alpha_map


def _apply_master_tile_crop_mask_to_mosaic(
    final_mosaic_data: np.ndarray | None,
    final_mosaic_coverage: np.ndarray | None,
    final_alpha_map: np.ndarray | None,
    *,
    crop_percent: float,
) -> tuple[np.ndarray | None, np.ndarray | None, np.ndarray | None]:
    """Apply a border mask equivalent to the master-tile crop percentage."""

    if final_mosaic_data is None:
        return final_mosaic_data, final_mosaic_coverage, final_alpha_map
    try:
        percent = float(crop_percent or 0.0)
    except Exception:
        percent = 0.0
    if percent <= 0.0:
        return final_mosaic_data, final_mosaic_coverage, final_alpha_map

    h, w = final_mosaic_data.shape[:2]
    if h <= 2 or w <= 2:
        return final_mosaic_data, final_mosaic_coverage, final_alpha_map
    margin_y = max(1, int(round(h * (percent / 100.0))))
    margin_x = max(1, int(round(w * (percent / 100.0))))
    margin_y = min(margin_y, h // 2)
    margin_x = min(margin_x, w // 2)
    mask = np.ones((h, w), dtype=np.float32)
    mask[:margin_y, :] = 0.0
    mask[h - margin_y :, :] = 0.0
    mask[:, :margin_x] = 0.0
    mask[:, w - margin_x :] = 0.0

    if final_mosaic_data.ndim == 3:
        final_mosaic_data = np.where(mask[..., None] > 0, final_mosaic_data, np.nan)
    else:
        final_mosaic_data = np.where(mask > 0, final_mosaic_data, np.nan)
    if final_mosaic_coverage is not None:
        final_mosaic_coverage = np.where(mask > 0, final_mosaic_coverage, 0.0).astype(
            np.float32, copy=False
        )
    if final_alpha_map is not None:
        final_alpha_map = np.where(mask > 0, final_alpha_map, 0).astype(
            np.uint8, copy=False
        )
    return final_mosaic_data, final_mosaic_coverage, final_alpha_map


def _compute_coverage_stats(
    coverage_array: np.ndarray | None,
    *,
    threshold_fraction: float = GLOBAL_COVERAGE_SUMMARY_THRESHOLD_FRAC,
    min_absolute: float = GLOBAL_COVERAGE_SUMMARY_MIN_ABS,
) -> dict[str, Any] | None:
    """Compute coarse stats (fraction + bbox) for a coverage array."""

    if coverage_array is None:
        return None
    try:
        arr = np.asarray(coverage_array, dtype=np.float32, copy=False)
    except Exception:
        return None
    if arr.ndim != 2:
        arr = np.squeeze(arr)
        if arr.ndim != 2:
            return None
    if arr.size <= 0:
        return None
    arr = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)
    try:
        max_val = float(np.nanmax(arr))
    except ValueError:
        return None
    if not np.isfinite(max_val) or max_val <= 0:
        return None
    threshold = max(float(min_absolute), float(max_val) * float(threshold_fraction))
    mask = arr > threshold
    coverage_pixels = int(np.count_nonzero(mask))
    total_pixels = int(mask.size)
    if coverage_pixels <= 0 or total_pixels <= 0:
        return None
    coords = np.argwhere(mask)
    if coords.size == 0:
        return None
    y_vals = coords[:, 0]
    x_vals = coords[:, 1]
    y0 = int(np.min(y_vals))
    y1 = int(np.max(y_vals) + 1)
    x0 = int(np.min(x_vals))
    x1 = int(np.max(x_vals) + 1)
    return {
        "fraction": float(coverage_pixels / total_pixels),
        "coverage_pixels": coverage_pixels,
        "total_pixels": total_pixels,
        "y0": y0,
        "y1": y1,
        "x0": x0,
        "x1": x1,
        "threshold": float(threshold),
        "array_height": int(arr.shape[0]),
        "array_width": int(arr.shape[1]),
    }


def _emit_coverage_summary_log(
    pcb_fn: Callable | None,
    *,
    coverage_array: np.ndarray | None,
    width: int,
    height: int,
    log_key: str,
    base_payload: dict[str, Any] | None = None,
    label: str | None = None,
) -> dict[str, Any] | None:
    """Emit a localized coverage summary log entry if possible."""

    if pcb_fn is None:
        return None
    stats = _compute_coverage_stats(coverage_array)
    if not stats:
        return None
    arr_h = max(1, int(stats.get("array_height") or 1))
    arr_w = max(1, int(stats.get("array_width") or 1))
    try:
        scale_y = float(height) / float(arr_h)
    except Exception:
        scale_y = 1.0
    try:
        scale_x = float(width) / float(arr_w)
    except Exception:
        scale_x = 1.0
    bbox_y0_px = int(max(0, min(int(height), math.floor(stats["y0"] * scale_y))))
    bbox_y1_px = int(max(0, min(int(height), math.ceil(stats["y1"] * scale_y))))
    bbox_x0_px = int(max(0, min(int(width), math.floor(stats["x0"] * scale_x))))
    bbox_x1_px = int(max(0, min(int(width), math.ceil(stats["x1"] * scale_x))))
    payload = {
        "coverage_fraction": float(stats["fraction"]),
        "coverage_pixels": int(stats["coverage_pixels"]),
        "width": int(width),
        "height": int(height),
        "bbox_y0": bbox_y0_px,
        "bbox_y1": bbox_y1_px,
        "bbox_x0": bbox_x0_px,
        "bbox_x1": bbox_x1_px,
        "threshold": float(stats["threshold"]),
        "grid_height": arr_h,
        "grid_width": arr_w,
    }
    if base_payload:
        payload.update(base_payload)
    if label and "label" not in payload:
        payload["label"] = label
    pcb_fn(log_key, prog=None, lvl="INFO_DETAIL", **payload)
    if log_key == "global_coadd_coverage_summary" and stats["fraction"] < 0.6:
        sparse_payload = dict(payload)
        sparse_payload["coverage_fraction"] = float(stats["fraction"])
        pcb_fn(
            "global_coadd_coverage_hint_sparse",
            prog=None,
            lvl="INFO_DETAIL",
            **sparse_payload,
        )
    return stats


def _auto_crop_global_mosaic_if_requested(
    final_mosaic_data: np.ndarray | None,
    final_mosaic_coverage: np.ndarray | None,
    final_alpha_map: np.ndarray | None,
    *,
    enable_autocrop: bool,
    margin_px: int,
    pcb: Callable | None = None,
) -> tuple[np.ndarray | None, np.ndarray | None, np.ndarray | None, dict[str, int] | None]:
    """Crop the global mosaic to the tight coverage bbox when requested."""

    if not enable_autocrop:
        return final_mosaic_data, final_mosaic_coverage, final_alpha_map, None
    stats = _compute_coverage_stats(final_mosaic_coverage)
    if not stats:
        return final_mosaic_data, final_mosaic_coverage, final_alpha_map, None
    arr_h = int(stats["array_height"])
    arr_w = int(stats["array_width"])
    margin = max(0, int(margin_px or 0))
    y0 = max(0, stats["y0"] - margin)
    y1 = min(arr_h, stats["y1"] + margin)
    x0 = max(0, stats["x0"] - margin)
    x1 = min(arr_w, stats["x1"] + margin)
    if y1 - y0 <= 0 or x1 - x0 <= 0:
        return final_mosaic_data, final_mosaic_coverage, final_alpha_map, None
    if y0 == 0 and y1 >= arr_h and x0 == 0 and x1 >= arr_w:
        return final_mosaic_data, final_mosaic_coverage, final_alpha_map, None

    def _crop_array(array: np.ndarray | None) -> np.ndarray | None:
        if array is None:
            return None
        try:
            arr = np.asarray(array)
        except Exception:
            return array
        if arr.ndim < 2:
            return array
        slicer = (slice(y0, y1), slice(x0, x1))
        if arr.ndim > 2:
            slicer = slicer + tuple(slice(None) for _ in range(arr.ndim - 2))
        return arr[slicer]

    cropped_data = _crop_array(final_mosaic_data)
    cropped_coverage = _crop_array(final_mosaic_coverage)
    cropped_alpha = _crop_array(final_alpha_map)
    if pcb is not None:
        pcb(
            "global_coadd_info_autocrop_applied",
            prog=None,
            lvl="INFO_DETAIL",
            crop_y0=int(y0),
            crop_y1=int(y1),
            crop_x0=int(x0),
            crop_x1=int(x1),
            original_height=int(arr_h),
            original_width=int(arr_w),
            margin_px=int(margin),
        )
    crop_meta = {
        "x0": int(x0),
        "y0": int(y0),
        "width": int(x1 - x0),
        "height": int(y1 - y0),
    }
    return cropped_data, cropped_coverage, cropped_alpha, crop_meta


def _apply_autocrop_to_global_plan(
    plan: dict[str, Any] | None,
    crop_meta: dict[str, int] | None,
) -> None:
    """Update the in-memory global WCS plan so that CRPIX/W×H match the cropped canvas."""

    if not plan or not crop_meta:
        return
    if plan.get("autocrop_applied"):
        try:
            existing = plan.get("autocrop_offsets") or {}
            if (
                int(existing.get("x0", -1)) == int(crop_meta.get("x0", -2))
                and int(existing.get("y0", -1)) == int(crop_meta.get("y0", -2))
                and int(existing.get("width", -1)) == int(crop_meta.get("width", -2))
                and int(existing.get("height", -1)) == int(crop_meta.get("height", -2))
            ):
                return
        except Exception:
            return
    width = int(crop_meta.get("width") or 0)
    height = int(crop_meta.get("height") or 0)
    if width <= 0 or height <= 0:
        return
    x0 = int(crop_meta.get("x0") or 0)
    y0 = int(crop_meta.get("y0") or 0)
    new_wcs = None
    wcs_obj = plan.get("wcs")
    if wcs_obj is not None:
        try:
            header = wcs_obj.to_header(relax=True)
            header["CRPIX1"] = float(header.get("CRPIX1", 0.0)) - float(x0)
            header["CRPIX2"] = float(header.get("CRPIX2", 0.0)) - float(y0)
            header["NAXIS1"] = width
            header["NAXIS2"] = height
            new_wcs = WCS(header) if (ASTROPY_AVAILABLE and WCS) else None
        except Exception:
            new_wcs = None
        if new_wcs is None and hasattr(wcs_obj, "wcs"):
            try:
                new_wcs = copy.deepcopy(wcs_obj)
                new_wcs.wcs.crpix[0] -= float(x0)
                new_wcs.wcs.crpix[1] -= float(y0)
            except Exception:
                new_wcs = None
    if new_wcs is not None:
        plan["wcs"] = new_wcs
    plan["width"] = width
    plan["height"] = height
    descriptor = plan.get("descriptor")
    if isinstance(descriptor, dict):
        descriptor["width"] = width
        descriptor["height"] = height
        descriptor["autocrop_offsets"] = {
            "x0": x0,
            "y0": y0,
            "width": width,
            "height": height,
        }
        header_obj = descriptor.get("header")
        if header_obj is not None:
            try:
                header_obj["NAXIS1"] = width
                header_obj["NAXIS2"] = height
                header_obj["CRPIX1"] = float(header_obj.get("CRPIX1", 0.0)) - float(x0)
                header_obj["CRPIX2"] = float(header_obj.get("CRPIX2", 0.0)) - float(y0)
            except Exception:
                pass
    plan["autocrop_offsets"] = {
        "x0": x0,
        "y0": y0,
        "width": width,
        "height": height,
    }
    plan["autocrop_applied"] = True


def _prepare_tiles_for_two_pass(
    collected_tiles: list[tuple] | None,
    fallback_loader: Callable[[], tuple] | None,
) -> tuple[list[np.ndarray], list[Any], list[np.ndarray | None]]:
    """Build the tile, WCS, and coverage lists used during the second pass."""

    tiles: list[np.ndarray] = []
    wcs_list: list[Any] = []
    coverage_list: list[np.ndarray | None] = []

    def _coerce_tile_payload(
        tile_entry,
    ) -> tuple[np.ndarray | None, Any | None, np.ndarray | None]:
        try:
            if isinstance(tile_entry, (list, tuple)):
                arr = tile_entry[0] if len(tile_entry) >= 1 else None
                twcs = tile_entry[1] if len(tile_entry) >= 2 else None
                cov = tile_entry[2] if len(tile_entry) >= 3 else None
                return arr, twcs, cov
        except Exception:
            pass
        return None, None, None

    if collected_tiles:
        for entry in collected_tiles:
            arr, twcs, cov = _coerce_tile_payload(entry)
            if arr is None or twcs is None:
                continue
            arr_np = np.asarray(arr, dtype=np.float32)
            tiles.append(arr_np)
            wcs_list.append(twcs)
            if cov is not None:
                cov_np = np.asarray(cov, dtype=np.float32, copy=False)
                if cov_np.ndim > 2:
                    cov_np = np.squeeze(cov_np)
                coverage_list.append(np.nan_to_num(cov_np, nan=0.0, posinf=0.0, neginf=0.0))
            else:
                if arr_np.ndim >= 3:
                    mask = np.any(np.isfinite(arr_np), axis=-1)
                else:
                    mask = np.isfinite(arr_np)
                coverage_list.append(mask.astype(np.float32))
    elif fallback_loader is not None:
        try:
            loader_payload = fallback_loader()
        except Exception:
            loader_payload = ([], [], [])
        tiles_from_loader: list[np.ndarray] = []
        wcs_from_loader: list[Any] = []
        coverage_from_loader: list[np.ndarray | None] = []
        if isinstance(loader_payload, tuple) and len(loader_payload) >= 2:
            tiles_from_loader = list(loader_payload[0] or [])
            wcs_from_loader = list(loader_payload[1] or [])
            if len(loader_payload) >= 3:
                coverage_from_loader = list(loader_payload[2] or [])
        for idx, arr in enumerate(tiles_from_loader):
            twcs = wcs_from_loader[idx] if idx < len(wcs_from_loader) else None
            if arr is None or twcs is None:
                continue
            tiles.append(np.asarray(arr, dtype=np.float32))
            wcs_list.append(twcs)
            cov = coverage_from_loader[idx] if idx < len(coverage_from_loader) else None
            if cov is None:
                coverage_list.append(None)
            else:
                cov_np = np.asarray(cov, dtype=np.float32, copy=False)
                if cov_np.ndim > 2:
                    cov_np = np.squeeze(cov_np)
                coverage_list.append(np.nan_to_num(cov_np, nan=0.0, posinf=0.0, neginf=0.0))

    return tiles, wcs_list, coverage_list


def _apply_two_pass_coverage_renorm_if_requested(
    final_mosaic_data: np.ndarray | None,
    final_mosaic_coverage: np.ndarray | None,
    *,
    two_pass_enabled: bool,
    two_pass_sigma_px: int,
    gain_clip_tuple: tuple[float, float],
    final_output_wcs: Any,
    final_output_shape_hw: tuple[int, int] | None,
    use_gpu_two_pass: bool,
    logger: logging.Logger | None,
    collected_tiles: list[tuple[np.ndarray, Any]] | None = None,
    fallback_tile_loader: Callable[[], tuple[list[np.ndarray], list[Any]]] | None = None,
    parallel_plan: ParallelPlan | None = None,
    telemetry_ctrl: ResourceTelemetryController | None = None,
) -> tuple[np.ndarray | None, np.ndarray | None]:
    """Run the coverage renormalization second pass if configured."""

    if (
        not two_pass_enabled
        or final_mosaic_data is None
        or final_mosaic_coverage is None
        or final_output_wcs is None
        or not final_output_shape_hw
    ):
        return final_mosaic_data, final_mosaic_coverage

    if logger:
        logger.info(
            "[TwoPass] Second pass requested (sigma=%s, clip=%s); coverage shape=%s",
            two_pass_sigma_px,
            gain_clip_tuple,
            getattr(final_mosaic_coverage, "shape", None),
        )

    (
        tiles_for_second_pass,
        wcs_for_second_pass,
        coverage_for_second_pass,
    ) = _prepare_tiles_for_two_pass(
        collected_tiles,
        fallback_tile_loader,
    )
    if logger:
        logger.debug(
            "[TwoPass] Prepared %d tiles for second pass (collected=%s, fallback=%s)",
            len(tiles_for_second_pass),
            bool(collected_tiles),
            fallback_tile_loader is not None,
        )
    if not tiles_for_second_pass or not wcs_for_second_pass:
        if logger:
            logger.warning(
                "[TwoPass] No tiles available for coverage renorm; keeping first-pass outputs"
            )
        return final_mosaic_data, final_mosaic_coverage

    try:
        result = run_second_pass_coverage_renorm(
            tiles_for_second_pass,
            wcs_for_second_pass,
            final_output_wcs,
            final_mosaic_coverage,
            final_output_shape_hw,
            sigma_px=two_pass_sigma_px,
            gain_clip=gain_clip_tuple,
            logger=logger,
            use_gpu_two_pass=use_gpu_two_pass,
            tiles_coverage=coverage_for_second_pass,
            parallel_plan=parallel_plan,
            telemetry_ctrl=telemetry_ctrl,
        )
        if result is not None:
            final_mosaic_data, final_mosaic_coverage = result
            if logger:
                logger.info(
                    "[TwoPass] coverage-renorm OK (σ=%s, clip=[%.3f, %.3f])",
                    two_pass_sigma_px,
                    gain_clip_tuple[0],
                    gain_clip_tuple[1],
                )
        else:
            if logger:
                logger.warning("[TwoPass] renorm failed → keeping first-pass outputs")
    except Exception:
        if logger:
            logger.exception("[TwoPass] renorm exception → keeping first-pass outputs")
    return final_mosaic_data, final_mosaic_coverage


def _apply_final_mosaic_rgb_equalization(
    final_mosaic_data: np.ndarray | None,
    *,
    zconfig=None,
    logger: logging.Logger | None = None,
):
    """Apply final RGB equalization to the mosaic using the poststack helper."""

    default_info = {
        "enabled": False,
        "applied": False,
        "gain_r": 1.0,
        "gain_g": 1.0,
        "gain_b": 1.0,
        "target_median": float("nan"),
    }

    if final_mosaic_data is None or not isinstance(final_mosaic_data, np.ndarray):
        return final_mosaic_data, default_info

    if _poststack_rgb_equalization is None:
        if logger:
            logger.debug("[RGB-EQ] final mosaic equalization skipped: helper unavailable")
        return final_mosaic_data, default_info

    try:
        info = _poststack_rgb_equalization(final_mosaic_data, zconfig=zconfig, stack_metadata=None)
    except Exception as exc:
        if logger:
            logger.warning("[RGB-EQ] final mosaic equalization failed: %s", exc)
        return final_mosaic_data, default_info

    if not isinstance(info, dict):
        info = default_info
    else:
        for key, value in default_info.items():
            info.setdefault(key, value)

    if logger is not None and info.get("applied"):
        logger.info(
            "[RGB-EQ] final mosaic: applied=True, gains=(%.6f, %.6f, %.6f), target_median=%.2f",
            info.get("gain_r", 1.0),
            info.get("gain_g", 1.0),
            info.get("gain_b", 1.0),
            info.get("target_median", float("nan")),
        )

    return final_mosaic_data, info


def equalize_black_point_rgb(
    mosaic_hwc: np.ndarray | None,
    *,
    alpha_mask: np.ndarray | None = None,
    coverage_hw: np.ndarray | None = None,
    percentile: float = 0.1,
    max_samples: int = 5_000_000,
    logger: logging.Logger | None = None,
) -> tuple[np.ndarray | None, dict[str, Any]]:
    """Align RGB black points by subtracting a low-percentile pedestal.

    Percentile is estimated on valid pixels only (alpha/coverage/finite),
    then subtracted per channel and clipped to non-negative.
    """

    info: dict[str, Any] = {
        "enabled": True,
        "applied": False,
        "percentile": float(percentile),
        "pedestal_r": 0.0,
        "pedestal_g": 0.0,
        "pedestal_b": 0.0,
        "samples": 0,
    }

    if mosaic_hwc is None or not isinstance(mosaic_hwc, np.ndarray):
        return mosaic_hwc, info

    if mosaic_hwc.ndim != 3 or mosaic_hwc.shape[2] != 3:
        return mosaic_hwc, info

    try:
        arr = np.asarray(mosaic_hwc, dtype=np.float32, order="C")
    except Exception:
        arr = mosaic_hwc

    if not isinstance(arr, np.ndarray) or arr.ndim != 3 or arr.shape[2] != 3:
        return mosaic_hwc, info

    if not arr.flags.writeable:
        try:
            arr = arr.copy()
        except Exception:
            return mosaic_hwc, info

    valid_mask = None
    try:
        if isinstance(alpha_mask, np.ndarray):
            alpha_arr = np.asarray(alpha_mask)
            if alpha_arr.ndim == 3 and alpha_arr.shape[-1] == 1:
                alpha_arr = alpha_arr[..., 0]
            elif alpha_arr.ndim > 2:
                alpha_arr = np.squeeze(alpha_arr)
            if alpha_arr.shape != arr.shape[:2]:
                try:
                    alpha_arr = alpha_arr.reshape(arr.shape[:2])
                except Exception:
                    alpha_arr = None
            if alpha_arr is not None:
                valid_mask = alpha_arr > 0

        if valid_mask is None and isinstance(coverage_hw, np.ndarray):
            cov_arr = np.asarray(coverage_hw, dtype=np.float32)
            if cov_arr.ndim > 2:
                cov_arr = np.squeeze(cov_arr)
            if cov_arr.shape != arr.shape[:2]:
                try:
                    cov_arr = cov_arr.reshape(arr.shape[:2])
                except Exception:
                    cov_arr = None
            if cov_arr is not None:
                valid_mask = (cov_arr > 1e-6) & np.isfinite(cov_arr)
    except Exception:
        valid_mask = None

    if valid_mask is None:
        valid_mask = np.any(np.isfinite(arr), axis=-1)

    n_valid = int(np.count_nonzero(valid_mask))
    if n_valid <= 0:
        return arr, info

    target_samples = n_valid
    if max_samples and int(max_samples) > 0:
        target_samples = min(n_valid, int(max_samples))

    flat_arr = arr.reshape(-1, 3)
    sample_vals: np.ndarray | None = None

    if n_valid <= target_samples:
        try:
            sample_vals = arr[valid_mask]
        except Exception:
            sample_vals = None
    else:
        valid_flat = np.asarray(valid_mask, dtype=bool).ravel()
        total_pixels = int(valid_flat.size)
        rng = np.random.default_rng(0)
        needed = int(target_samples)
        chunks: list[np.ndarray] = []
        attempt = 0
        draw_cap = 2_000_000
        while needed > 0 and attempt < 16:
            draw = int(min(total_pixels, max(1024, min(draw_cap, needed * 2))))
            idx = rng.integers(0, total_pixels, size=draw, dtype=np.int64)
            idx = idx[valid_flat[idx]]
            if idx.size:
                if idx.size > needed:
                    idx = idx[:needed]
                chunks.append(flat_arr[idx])
                needed -= int(idx.size)
            attempt += 1
        if chunks:
            try:
                sample_vals = np.concatenate(chunks, axis=0)
            except Exception:
                sample_vals = None

    if sample_vals is None or sample_vals.size == 0:
        return arr, info

    samples = int(sample_vals.shape[0])

    try:
        pedestals = np.nanpercentile(sample_vals, float(percentile), axis=0).astype(np.float32)
    except Exception:
        return arr, info

    if not np.all(np.isfinite(pedestals)):
        return arr, info

    try:
        arr -= pedestals[None, None, :]
        np.clip(arr, 0.0, None, out=arr)
    except Exception:
        return arr, info

    info.update(
        applied=True,
        pedestal_r=float(pedestals[0]),
        pedestal_g=float(pedestals[1]),
        pedestal_b=float(pedestals[2]),
        samples=samples,
    )

    if logger is not None:
        logger.info(
            "[BlackPoint] Final mosaic RGB pedestal subtracted (p=%.3f%%): "
            "pedestals=(%.3f, %.3f, %.3f) on %d samples",
            float(percentile),
            float(pedestals[0]),
            float(pedestals[1]),
            float(pedestals[2]),
            samples,
        )

    return arr, info


def _equalize_rgb_black_level_hwc(
    rgb_hwc: np.ndarray,
    *,
    alpha_mask: np.ndarray | None = None,
    coverage_mask: np.ndarray | None = None,
    p_low: float = 0.1,
    logger: logging.Logger | None = None,
):
    info = {"applied": False, "p_low": float(p_low), "offsets": [0.0, 0.0, 0.0]}

    if rgb_hwc is None or not isinstance(rgb_hwc, np.ndarray) or rgb_hwc.ndim != 3 or rgb_hwc.shape[-1] != 3:
        return rgb_hwc, info

    rgb = rgb_hwc.astype(np.float32, copy=False)

    finite = np.isfinite(rgb).all(axis=-1)
    valid = finite

    if alpha_mask is not None:
        a = np.asarray(alpha_mask)
        if a.ndim > 2:
            a = a[..., 0]
        if a.shape[:2] == rgb.shape[:2]:
            valid = valid & (a > 0)

    if coverage_mask is not None:
        cov = np.asarray(coverage_mask)
        if cov.ndim > 2:
            cov = cov[..., 0]
        if cov.shape[:2] == rgb.shape[:2]:
            valid = valid & np.isfinite(cov) & (cov > 0)

    if not np.any(valid):
        return rgb_hwc, info

    offsets: list[float] = []
    for c in range(3):
        vals = rgb[..., c][valid]
        try:
            p = float(np.nanpercentile(vals, p_low))
        except Exception:
            p = float("nan")
        if np.isfinite(p) and p > 0:
            offsets.append(p)
        else:
            offsets.append(0.0)

    valid_px = int(np.count_nonzero(valid))
    if any(o > 0 for o in offsets):
        out = rgb.copy()
        for c, o in enumerate(offsets):
            if o > 0:
                out[..., c] -= np.float32(o)
        out = np.maximum(out, 0.0)
        info["applied"] = True
        info["offsets"] = offsets
        if logger:
            try:
                mins = [
                    float(np.nanmin(out[..., c][valid])) if np.any(valid) else float("nan")
                    for c in range(3)
                ]
            except Exception:
                mins = []
            if mins:
                logger.info(
                    "[RGB-BL] applied=True p_low=%.3f offsets=(%.3f, %.3f, %.3f) valid_min=(%.5f, %.5f, %.5f) valid_px=%d",
                    p_low,
                    offsets[0],
                    offsets[1],
                    offsets[2],
                    mins[0],
                    mins[1],
                    mins[2],
                    valid_px,
                )
            else:
                logger.info(
                    "[RGB-BL] applied=True p_low=%.3f offsets=(%.3f, %.3f, %.3f) valid_px=%d",
                    p_low,
                    offsets[0],
                    offsets[1],
                    offsets[2],
                    valid_px,
                )
        return out, info

    return rgb_hwc, info


def _apply_phase5_post_stack_pipeline(
    final_mosaic_data: np.ndarray | None,
    final_mosaic_coverage: np.ndarray | None,
    final_alpha_map: np.ndarray | None,
    *,
    enable_lecropper_pipeline: bool,
    pipeline_cfg: dict[str, Any] | None,
    enable_master_tile_crop: bool,
    master_tile_crop_percent: float,
    two_pass_enabled: bool,
    two_pass_sigma_px: int,
    two_pass_gain_clip: tuple[float, float],
    final_output_wcs: Any,
    final_output_shape_hw: tuple[int, int] | None,
    use_gpu_two_pass: bool,
    logger: logging.Logger | None,
    collected_tiles: list[tuple[np.ndarray, Any]] | None = None,
    fallback_two_pass_loader: Callable[[], tuple[list[np.ndarray], list[Any]]] | None = None,
    parallel_plan: ParallelPlan | None = None,
    telemetry_ctrl: ResourceTelemetryController | None = None,
) -> tuple[np.ndarray | None, np.ndarray | None, np.ndarray | None]:
    """Run the reusable Phase 5 post-stack operations."""

    if enable_lecropper_pipeline:
        (
            final_mosaic_data,
            final_mosaic_coverage,
            final_alpha_map,
        ) = _apply_final_mosaic_quality_pipeline(
            final_mosaic_data,
            final_mosaic_coverage,
            final_alpha_map,
            pipeline_cfg,
        )

    if enable_master_tile_crop:
        (
            final_mosaic_data,
            final_mosaic_coverage,
            final_alpha_map,
        ) = _apply_master_tile_crop_mask_to_mosaic(
            final_mosaic_data,
            final_mosaic_coverage,
            final_alpha_map,
            crop_percent=master_tile_crop_percent,
        )

    final_mosaic_data, final_mosaic_coverage = _apply_two_pass_coverage_renorm_if_requested(
        final_mosaic_data,
        final_mosaic_coverage,
        two_pass_enabled=two_pass_enabled,
        two_pass_sigma_px=two_pass_sigma_px,
        gain_clip_tuple=two_pass_gain_clip,
        final_output_wcs=final_output_wcs,
        final_output_shape_hw=final_output_shape_hw,
        use_gpu_two_pass=use_gpu_two_pass,
        logger=logger,
        collected_tiles=collected_tiles,
        fallback_tile_loader=fallback_two_pass_loader,
        parallel_plan=parallel_plan,
        telemetry_ctrl=telemetry_ctrl,
    )
    return final_mosaic_data, final_mosaic_coverage, final_alpha_map


def _mask_sds_low_coverage_pixels(
    mosaic_hwc: np.ndarray | None,
    coverage_hw: np.ndarray | None,
    *,
    min_keep_fraction: float,
    target_hw: tuple[int, int] | None = None,
) -> tuple[np.ndarray | None, np.ndarray | None, dict[str, float]]:
    """Normalize coverage and mask low-coverage pixels for SDS mosaics."""

    summary = {"max_cov": 0.0, "masked_pixels": 0}
    if coverage_hw is None:
        return mosaic_hwc, coverage_hw, summary
    try:
        coverage_arr = np.asarray(coverage_hw, dtype=np.float32, order="C")
    except Exception:
        return mosaic_hwc, coverage_hw, summary
    if coverage_arr.ndim > 2:
        coverage_arr = np.squeeze(coverage_arr)
    if coverage_arr.ndim != 2 and target_hw:
        try:
            if coverage_arr.size == int(target_hw[0]) * int(target_hw[1]):
                coverage_arr = coverage_arr.reshape(target_hw)
        except Exception:
            pass
    if coverage_arr.ndim != 2:
        return mosaic_hwc, coverage_arr, summary
    coverage_arr = np.where(np.isfinite(coverage_arr), coverage_arr, 0.0)
    max_cov = float(np.nanmax(coverage_arr)) if coverage_arr.size else 0.0
    summary["max_cov"] = max_cov
    frac = max(0.0, min(1.0, float(min_keep_fraction)))
    if max_cov <= 0.0 or frac <= 0.0:
        return mosaic_hwc, coverage_arr, summary
    coverage_norm = coverage_arr / max_cov if max_cov > 0 else coverage_arr
    lowcov_mask = coverage_norm < frac
    masked_pixels = int(np.count_nonzero(lowcov_mask))
    summary["masked_pixels"] = masked_pixels
    if masked_pixels == 0:
        return mosaic_hwc, coverage_arr, summary
    coverage_arr = coverage_arr.copy()
    coverage_arr[lowcov_mask] = 0.0
    if mosaic_hwc is not None:
        if mosaic_hwc.ndim == 3:
            mosaic_hwc = np.where((~lowcov_mask)[..., None], mosaic_hwc, np.nan)
        else:
            mosaic_hwc = np.where(~lowcov_mask, mosaic_hwc, np.nan)
    return mosaic_hwc, coverage_arr, summary


def _sanitize_sds_megatile_payload(
    mosaic_hwc: np.ndarray | None,
    coverage_hw: np.ndarray | None,
    alpha_hw: np.ndarray | None = None,
) -> tuple[np.ndarray | None, np.ndarray | None, np.ndarray | None]:
    """Mask mega-tiles outside their coverage map so stacking ignores empty pixels."""

    if mosaic_hwc is None:
        return None, coverage_hw, None

    mosaic_arr = np.asarray(mosaic_hwc, dtype=np.float32, order="C")
    coverage_arr = None
    if coverage_hw is not None:
        coverage_arr = np.asarray(coverage_hw, dtype=np.float32, order="C")
    elif alpha_hw is not None:
        try:
            alpha_arr = np.asarray(alpha_hw, dtype=np.float32, order="C")
            if alpha_arr.ndim > 2:
                alpha_arr = np.squeeze(alpha_arr)
            max_alpha = float(np.nanmax(alpha_arr)) if alpha_arr.size else 0.0
            if max_alpha > 0.0:
                coverage_arr = (alpha_arr / max_alpha).astype(np.float32, copy=False)
        except Exception:
            coverage_arr = None

    target_shape = mosaic_arr.shape[:2]

    def _coerce_hw_array(arr: np.ndarray | None, *, fill_value: float = 0.0) -> np.ndarray | None:
        if arr is None:
            return None
        arr_np = np.asarray(arr, dtype=np.float32, order="C")
        if arr_np.ndim > 2:
            arr_np = np.squeeze(arr_np)
        if arr_np.ndim == 1:
            # 1D arrays cannot be meaningfully reshaped -> drop.
            return None
        if arr_np.ndim != 2:
            try:
                arr_np = arr_np.reshape(target_shape)
            except Exception:
                return None
        if arr_np.shape != target_shape:
            sanitized = np.full(target_shape, fill_value, dtype=np.float32)
            h = min(target_shape[0], arr_np.shape[0])
            w = min(target_shape[1], arr_np.shape[1])
            sanitized[:h, :w] = arr_np[:h, :w]
            arr_np = sanitized
        return arr_np

    if coverage_arr is None:
        coverage_arr = np.ones(mosaic_arr.shape[:2], dtype=np.float32)
    else:
        coerced = _coerce_hw_array(coverage_arr, fill_value=0.0)
        coverage_arr = coerced if coerced is not None else np.ones(target_shape, dtype=np.float32)
    coverage_arr = np.nan_to_num(coverage_arr, nan=0.0, posinf=0.0, neginf=0.0)
    valid_mask = (coverage_arr > 1e-6) & np.isfinite(coverage_arr)
    if mosaic_arr.ndim == 3:
        mosaic_arr = np.where(valid_mask[..., None], mosaic_arr, np.nan)
    else:
        mosaic_arr = np.where(valid_mask, mosaic_arr, np.nan)
    coverage_arr = np.where(valid_mask, coverage_arr, 0.0).astype(np.float32, copy=False)
    alpha_arr_sanitized: np.ndarray | None = None
    if alpha_hw is not None:
        alpha_float = _coerce_hw_array(alpha_hw, fill_value=0.0)
        if alpha_float is not None:
            alpha_float = np.where(valid_mask, alpha_float, 0.0)
            alpha_arr_sanitized = np.clip(alpha_float, 0.0, 255.0).astype(np.uint8, copy=False)
    if alpha_arr_sanitized is None:
        max_cov = float(np.nanmax(coverage_arr)) if coverage_arr.size else 0.0
        if max_cov > 0.0:
            normalized_alpha = np.where(
                valid_mask,
                (coverage_arr / max_cov) * 255.0,
                0.0,
            )
            alpha_arr_sanitized = np.clip(normalized_alpha, 0.0, 255.0).astype(np.uint8, copy=False)
    return mosaic_arr, coverage_arr, alpha_arr_sanitized


def _sds_compute_tile_payload(
    tile_arr: np.ndarray,
    coverage_arr: np.ndarray | None,
) -> tuple[np.ndarray, float, dict[str, float]]:
    """Return (tile_array_float32, positive_median, coverage_stats)."""

    arr = np.asarray(tile_arr, dtype=np.float32, order="C")
    stats = {"coverage_weight": 0.0, "coverage_pixels": 0, "coverage_max": 0.0}
    median_val = float(np.nanmedian(arr)) if arr.size else 1.0
    cov_np: np.ndarray | None = None
    if coverage_arr is not None:
        try:
            cov_np = np.asarray(coverage_arr, dtype=np.float32, order="C")
        except Exception:
            cov_np = None
        if cov_np is not None:
            if cov_np.ndim > 2:
                cov_np = np.squeeze(cov_np)
            if cov_np.ndim != 2:
                if arr.ndim >= 2 and cov_np.size == arr.shape[0] * arr.shape[1]:
                    try:
                        cov_np = cov_np.reshape(arr.shape[0], arr.shape[1])
                    except Exception:
                        cov_np = None
                else:
                    cov_np = None
    if cov_np is not None:
        cov_np = np.nan_to_num(cov_np, nan=0.0, posinf=0.0, neginf=0.0)
        stats["coverage_weight"] = float(np.nansum(cov_np))
        stats["coverage_pixels"] = int(np.count_nonzero(cov_np > 0.0))
        stats["coverage_max"] = float(np.nanmax(cov_np)) if cov_np.size else 0.0
        threshold = 0.0
        if stats["coverage_max"] > 0.0:
            # Ignore extremely low coverage (<1% of the local peak) to avoid noisy edges.
            threshold = 0.01 * stats["coverage_max"]
        mask = cov_np > threshold
        if not np.any(mask):
            mask = cov_np > 0.0
        if np.any(mask):
            masked_vals = arr[mask]
            if masked_vals.size:
                median_val = float(np.nanmedian(masked_vals))

    if not math.isfinite(median_val):
        median_val = 1.0
    median_val = float(abs(median_val))
    if median_val <= 0.0:
        fallback = float(np.nanmedian(np.abs(arr))) if arr.size else 1.0
        if not math.isfinite(fallback) or fallback <= 0.0:
            median_val = 1.0
        else:
            median_val = fallback

    return arr, median_val, stats


def _sds_choose_reference_index(
    payloads: list[tuple[np.ndarray, float, dict[str, float]]],
    requested_index: int | None,
) -> int:
    """Select a deterministic reference megatile index."""

    count = len(payloads)
    if count == 0:
        return 0
    if isinstance(requested_index, int) and 0 <= requested_index < count:
        return requested_index
    best_idx: int | None = None
    best_weight = -1.0
    for idx, (_, _, stats) in enumerate(payloads):
        weight = float(stats.get("coverage_weight", 0.0) or 0.0)
        if weight > best_weight and weight > 0.0:
            best_idx = idx
            best_weight = weight
    if best_idx is not None:
        return best_idx
    # Fallback to central tile when no usable coverage weight is available.
    central_idx = count // 2
    if 0 <= central_idx < count:
        return central_idx
    return 0


def _normalize_sds_megatiles_photometry(
    mega_tiles: list[np.ndarray],
    coverages: list[np.ndarray | None] | None = None,
    *,
    ref_index: int | None = None,
    pcb: Callable | None = None,
) -> list[np.ndarray]:
    """
    Normalize mega-tiles against a reference median (coverage-aware when available).

    Returns a new list of float32 mega-tiles.
    """

    if not mega_tiles:
        return []

    payloads: list[tuple[np.ndarray, float, dict[str, float]]] = []
    for idx, tile in enumerate(mega_tiles):
        cov_arr = coverages[idx] if coverages and idx < len(coverages or []) else None
        payloads.append(_sds_compute_tile_payload(tile, cov_arr))

    selected_ref = _sds_choose_reference_index(payloads, ref_index)
    selected_ref = max(0, min(selected_ref, len(payloads) - 1))
    _, ref_median, ref_stats = payloads[selected_ref]
    if not math.isfinite(ref_median) or ref_median <= 0.0:
        ref_median = 1.0

    if pcb:
        try:
            pcb(
                "sds_megatile_reference",
                prog=None,
                lvl="INFO_DETAIL",
                ref_index=int(selected_ref),
                ref_median=float(ref_median),
                ref_cov_weight=float(ref_stats.get("coverage_weight", 0.0)),
                ref_cov_pixels=int(ref_stats.get("coverage_pixels", 0)),
            )
        except Exception:
            pass

    normalized_tiles: list[np.ndarray] = []
    log_indices = {0, len(payloads) - 1, selected_ref}
    for idx, (tile_arr, tile_median, stats) in enumerate(payloads):
        gain = 1.0
        if idx != selected_ref and tile_median > 0.0:
            gain = ref_median / tile_median
        if not math.isfinite(gain) or gain <= 0.0:
            gain = 1.0
        if idx == selected_ref:
            normalized_tiles.append(np.asarray(tile_arr, dtype=np.float32, order="C"))
        else:
            normalized_tiles.append((tile_arr * np.float32(gain)).astype(np.float32, copy=False))
        if pcb and idx in log_indices:
            try:
                pcb(
                    "[SDS] sds_megatile_gain",
                    prog=None,
                    lvl="INFO_DETAIL",
                    tile_index=int(idx),
                    ref_index=int(selected_ref),
                    gain=float(gain),
                    ref_median=float(ref_median),
                    tile_median=float(tile_median),
                    coverage_weight=float(stats.get("coverage_weight", 0.0)),
                    coverage_pixels=int(stats.get("coverage_pixels", 0)),
                )
            except Exception:
                pass

    return normalized_tiles


def _finalize_sds_global_mosaic(
    mosaic_hwc: np.ndarray | None,
    coverage_hw: np.ndarray | None,
    *,
    zconfig,
    pcb,
    sds_config: dict[str, Any] | None = None,
    collected_tiles: list[tuple[np.ndarray, Any]] | None = None,
    final_output_wcs: Any = None,
    final_output_shape_hw: tuple[int, int] | None = None,
    pipeline_cfg: dict[str, Any] | None = None,
    enable_lecropper_pipeline: bool = False,
    enable_master_tile_crop: bool = False,
    master_tile_crop_percent: float = 0.0,
    two_pass_enabled: bool = False,
    two_pass_sigma_px: int = 0,
    two_pass_gain_clip: tuple[float, float] = (0.85, 1.18),
    use_gpu_two_pass: bool = False,
    enable_autocrop: bool = False,
    autocrop_margin_px: int = 0,
    global_plan: dict[str, Any] | None = None,
    fallback_two_pass_loader=None,
    parallel_plan: ParallelPlan | None = None,
) -> tuple[np.ndarray | None, np.ndarray | None, np.ndarray | None]:
    """
    Pipeline spécifique SDS appliqué après la construction de la mosaïque globale.

    Retourne (final_mosaic_data_HWC, final_mosaic_coverage_HW, final_alpha_u8).
    """

    mosaic_arr = None if mosaic_hwc is None else np.asarray(mosaic_hwc, dtype=np.float32, order="C")
    coverage_arr = None
    if coverage_hw is not None:
        coverage_arr = np.asarray(coverage_hw, dtype=np.float32, order="C")
    elif mosaic_arr is not None:
        coverage_arr = np.ones(mosaic_arr.shape[:2], dtype=np.float32)

    try:
        min_keep = float(getattr(zconfig, "sds_min_coverage_keep", 0.0))
    except Exception:
        min_keep = 0.0
    if sds_config and isinstance(sds_config, dict) and "min_coverage_keep" in sds_config:
        try:
            min_keep = float(sds_config.get("min_coverage_keep"))
        except Exception:
            pass

    mosaic_arr, coverage_arr, summary = _mask_sds_low_coverage_pixels(
        mosaic_arr,
        coverage_arr,
        min_keep_fraction=min_keep,
        target_hw=mosaic_arr.shape[:2] if mosaic_arr is not None else None,
    )
    try:
        pcb(
            "sds_mask_cov_summary",
            prog=None,
            lvl="INFO_DETAIL",
            max_cov=summary.get("max_cov", 0.0),
            masked_pixels=summary.get("masked_pixels", 0),
            min_keep_fraction=min_keep,
        )
    except Exception:
        pass

    if mosaic_arr is None:
        return None, coverage_arr, None

    final_output_shape = final_output_shape_hw or mosaic_arr.shape[:2]
    final_alpha_map = None
    mosaic_arr, coverage_arr, final_alpha_map = _apply_phase5_post_stack_pipeline(
        mosaic_arr,
        coverage_arr,
        final_alpha_map,
        enable_lecropper_pipeline=enable_lecropper_pipeline,
        pipeline_cfg=pipeline_cfg or {},
        enable_master_tile_crop=enable_master_tile_crop,
        master_tile_crop_percent=master_tile_crop_percent,
        two_pass_enabled=two_pass_enabled,
        two_pass_sigma_px=two_pass_sigma_px,
        two_pass_gain_clip=two_pass_gain_clip,
        final_output_wcs=final_output_wcs,
        final_output_shape_hw=final_output_shape,
        use_gpu_two_pass=use_gpu_two_pass,
        logger=logger,
        collected_tiles=collected_tiles,
        fallback_two_pass_loader=fallback_two_pass_loader,
        parallel_plan=parallel_plan,
    )

    autocrop_meta: dict[str, int] | None = None
    if enable_autocrop and mosaic_arr is not None:
        mosaic_arr, coverage_arr, final_alpha_map, autocrop_meta = _auto_crop_global_mosaic_if_requested(
            mosaic_arr,
            coverage_arr,
            final_alpha_map,
            enable_autocrop=True,
            margin_px=max(0, int(autocrop_margin_px)),
            pcb=pcb,
        )
        if autocrop_meta and global_plan is not None:
            _apply_autocrop_to_global_plan(global_plan, autocrop_meta)

    final_alpha_u8 = _derive_final_alpha_mask(final_alpha_map, mosaic_arr, coverage_arr, logger)
    return mosaic_arr, coverage_arr, final_alpha_u8


_MASTER_TILE_ID_LOCK = Lock()
_MASTER_TILE_ID_REGISTRY: dict[str, str] = {}


def _normalize_tile_path(path: str | os.PathLike | None) -> str | None:
    """Return a stable absolute path for registry keys."""

    if path is None:
        return None
    try:
        return str(Path(path).expanduser().resolve(strict=False))
    except Exception:
        try:
            return str(Path(path))
        except Exception:
            return str(path)


def _path_display_name(path: str | os.PathLike | None, default: str = "unknown") -> str:
    """Return a human-friendly file name for logging."""

    if not path:
        return default
    try:
        name = Path(path).name
        return name or str(path)
    except Exception:
        return str(path)


def _register_master_tile_identity(path: str | os.PathLike | None, tile_id: str | int | None) -> None:
    """Remember the logical ``tile_id`` associated with a saved tile path."""

    if path is None or tile_id is None:
        return
    normalized = _normalize_tile_path(path)
    if not normalized:
        return
    with _MASTER_TILE_ID_LOCK:
        _MASTER_TILE_ID_REGISTRY[normalized] = str(tile_id)


def _lookup_master_tile_identity(path: str | os.PathLike | None) -> str | None:
    """Fetch a previously registered ``tile_id`` for ``path`` if available."""

    if path is None:
        return None
    normalized = _normalize_tile_path(path)
    if not normalized:
        return None
    with _MASTER_TILE_ID_LOCK:
        return _MASTER_TILE_ID_REGISTRY.get(normalized)


def _resolve_tile_identifier(
    path: str | os.PathLike | None,
    header: Any | None,
    fallback_idx: int | None,
) -> str:
    """Resolve or synthesize a deterministic ``tile_id`` for a tile entry."""

    registered = _lookup_master_tile_identity(path)
    if registered:
        return registered

    candidate = None
    if header is not None:
        try:
            if "ZMT_SUPID" in header:
                candidate = str(header["ZMT_SUPID"])
        except Exception:
            candidate = None
        if candidate is None:
            try:
                if "ZMT_ID" in header:
                    candidate = f"tile:{int(header['ZMT_ID'])}"
            except Exception:
                try:
                    candidate = str(header.get("ZMT_ID")) if header.get("ZMT_ID") is not None else None
                except Exception:
                    candidate = None

    if candidate is None and fallback_idx is not None:
        candidate = f"tile:{int(fallback_idx):04d}"
    elif candidate is None and path:
        try:
            base = Path(path).stem
        except Exception:
            base = str(path)
        candidate = f"path:{base}"
    elif candidate is None:
        candidate = "tile:unknown"

    _register_master_tile_identity(path, candidate)
    return candidate

# Nombre maximum de tentatives d'alignement avant abandon définitif
MAX_ALIGNMENT_RETRY_ATTEMPTS = 3

SYSTEM_NAME = platform.system().lower()
IS_WINDOWS = SYSTEM_NAME == "windows"
CUPY_AVAILABLE = importlib.util.find_spec("cupy") is not None and IS_WINDOWS

GLOBAL_SEESTAR_TOKENS = ("seestar", "s30", "s50")


def _is_seestar_label(label: str | None) -> bool:
    """Return True when *label* indicates a Seestar instrument."""

    if not label:
        return False
    lowered = str(label).strip().lower()
    if not lowered:
        return False
    return any(token in lowered for token in GLOBAL_SEESTAR_TOKENS)


def _entry_is_seestar(entry: dict) -> bool:
    """Best-effort detection of Seestar raws from worker metadata."""

    if not isinstance(entry, dict):
        return False
    instrument = entry.get("instrument")
    if instrument and _is_seestar_label(instrument):
        return True
    header = entry.get("header") or entry.get("phase0_header")
    if header is not None:
        try:
            raw_instr = header.get("INSTRUME") if hasattr(header, "get") else header["INSTRUME"]
        except Exception:
            raw_instr = None
        if raw_instr and _is_seestar_label(raw_instr):
            return True
        try:
            creator = header.get("CREATOR") if hasattr(header, "get") else header["CREATOR"]
        except Exception:
            creator = None
        if creator and _is_seestar_label(creator):
            return True
    return False


def _coerce_to_builtin(value):
    """Recursively convert numpy/scalar objects into builtin Python types."""

    if isinstance(value, np.generic):
        try:
            return value.item()
        except Exception:
            return value
    if isinstance(value, dict):
        return {k: _coerce_to_builtin(v) for k, v in value.items()}
    if isinstance(value, (list, tuple)):
        return [_coerce_to_builtin(v) for v in value]
    return value


def _ensure_hwc_master_tile(
    data: np.ndarray,
    tile_label: str | None = None,
) -> np.ndarray:
    """Normalize master tile data to ``H x W x C`` float32 layout.

    Parameters
    ----------
    data : np.ndarray
        Array loaded from FITS (typically via ``hdul[0].data``).
    tile_label : str | None
        Optional identifier used for logging in case of shape issues.

    Returns
    -------
    np.ndarray
        Array with shape ``(H, W, C)`` and dtype ``float32``.

    Raises
    ------
    ValueError
        If the array dimensionality is unsupported or cannot be coerced
        into an ``HWC`` representation.
    """

    arr = np.asarray(data, dtype=np.float32)

    if arr.ndim == 2:
        arr = arr[..., np.newaxis]
    elif arr.ndim == 3:
        if arr.shape[-1] in (1, 3):
            pass
        elif arr.shape[0] in (1, 3):
            arr = np.moveaxis(arr, 0, -1)
        else:
            msg = (
                f"Unexpected tile shape for RGB master tile: {arr.shape}"
            )
            if tile_label:
                msg += f" (tile: {tile_label})"
            logger.error(msg)
            raise ValueError(msg)
    else:
        msg = f"Unsupported tile dimensionality: {arr.shape}"
        if tile_label:
            msg += f" (tile: {tile_label})"
        logger.error(msg)
        raise ValueError(msg)

    return np.asarray(arr, dtype=np.float32, order="C")


def load_image_with_optional_alpha(
    path: str,
    *,
    tile_label: str | None = None,
) -> tuple[np.ndarray, np.ndarray | None, np.ndarray | None]:
    """Load a FITS image alongside its optional ALPHA extension."""

    if not (ASTROPY_AVAILABLE and fits):
        raise RuntimeError("Astropy FITS support unavailable while loading image")

    with fits.open(path, memmap=True, do_not_scale_image_data=True) as hdul:
        primary = hdul[0].data
        alpha_hdu = hdul["ALPHA"] if "ALPHA" in hdul else None
        alpha = None
        if alpha_hdu is not None and alpha_hdu.data is not None:
            try:
                alpha = np.asarray(alpha_hdu.data, dtype=np.uint8)
            except Exception:
                alpha = None

    label = tile_label or _path_display_name(path)
    data = _ensure_hwc_master_tile(primary, label)
    data = np.asarray(data, dtype=np.float32, order="C", copy=False)

    weights: np.ndarray | None = None
    if alpha is not None:
        alpha = np.squeeze(alpha)
        if alpha.ndim != 2:
            try:
                alpha = alpha.reshape(alpha.shape[-2], alpha.shape[-1])
            except Exception:
                alpha = None
        if alpha is not None:
            alpha = np.clip(alpha, 0, 255).astype(np.uint8, copy=False)
            weights = alpha.astype(np.float32) / 255.0
            weights = np.clip(weights, 0.0, 1.0)
            mask_zero = weights <= ALPHA_OPACITY_THRESHOLD
            weights = np.where(mask_zero, 0.0, 1.0)
            if data.ndim == 2 and weights.shape == data.shape:
                data = np.where(mask_zero, np.nan, data)
            elif data.ndim == 3 and weights.shape == data.shape[:2]:
                data = np.where(mask_zero[..., None], np.nan, data)

    return data, weights, alpha


def _resolve_global_mosaic_path(output_dir: str | Path, candidate: str | os.PathLike | None) -> str | None:
    """Return an absolute filesystem path for *candidate* under *output_dir*."""

    candidate_path = expand_to_path(candidate)
    if candidate_path is None:
        return None
    base_dir = expand_to_path(output_dir) or Path(".")
    if not isinstance(base_dir, Path):
        base_dir = Path(".")
    if not candidate_path.is_absolute():
        candidate_path = base_dir / candidate_path
    try:
        return str(candidate_path.resolve(strict=False))
    except Exception:
        return str(candidate_path)


def _load_global_wcs_descriptor_safe(fits_path: str, json_path: str | None = None):
    """Best-effort wrapper around zemosaic_utils.load_global_wcs_descriptor."""

    if (
        not fits_path
        or not (ZEMOSAIC_UTILS_AVAILABLE and zemosaic_utils)
        or not hasattr(zemosaic_utils, "load_global_wcs_descriptor")
    ):
        return None
    fits_path_obj = Path(fits_path).expanduser()
    if not fits_path_obj.exists():
        return None
    json_path_obj = Path(json_path).expanduser() if json_path else None
    try:
        return zemosaic_utils.load_global_wcs_descriptor(
            str(fits_path_obj),
            str(json_path_obj) if json_path_obj else None,
            logger_override=logger,
        )
    except Exception as exc:
        logger.warning("Global WCS: unable to load descriptor (%s): %s", fits_path_obj, exc)
        return None


def _prepare_global_wcs_plan(
    output_dir: str,
    worker_config: dict | None,
    filter_overrides: dict | None,
) -> dict[str, Any]:
    """Assemble a best-effort plan for Mosaic-First execution."""

    plan: dict[str, Any] = {
        "enabled": False,
        "fits_path": None,
        "json_path": None,
        "descriptor": None,
        "meta": None,
        "wcs": None,
        "width": None,
        "height": None,
        "mode": "classic",
        "coadd_method": "kappa_sigma",
        "coadd_k": 2.0,
        "winsor_limits": (0.05, 0.05),
    }

    if isinstance(worker_config, dict):
        plan["coadd_method"] = (
            str(worker_config.get("global_coadd_method", "kappa_sigma") or "kappa_sigma")
            .strip()
            .lower()
        )
        try:
            plan["coadd_k"] = float(worker_config.get("global_coadd_k", 2.0) or 2.0)
        except Exception:
            plan["coadd_k"] = 2.0
    if not plan["coadd_method"]:
        plan["coadd_method"] = "kappa_sigma"

    winsor_limits = (0.05, 0.05)
    winsor_cfg = worker_config.get("stacking_winsor_limits") if isinstance(worker_config, dict) else None
    if isinstance(winsor_cfg, str):
        try:
            parts = [float(x.strip()) for x in winsor_cfg.split(",") if x.strip()]
            if len(parts) >= 2:
                winsor_limits = (parts[0], parts[1])
        except Exception:
            winsor_limits = (0.05, 0.05)
    elif isinstance(winsor_cfg, (list, tuple)) and len(winsor_cfg) >= 2:
        try:
            winsor_limits = (float(winsor_cfg[0]), float(winsor_cfg[1]))
        except Exception:
            winsor_limits = (0.05, 0.05)
    plan["winsor_limits"] = winsor_limits

    overrides = filter_overrides if isinstance(filter_overrides, dict) else {}
    override_mode = overrides.get("mode")
    if isinstance(override_mode, str) and override_mode.strip():
        plan["mode"] = override_mode.strip().lower()

    candidates: list[tuple[str, str | None, dict | None, str | None]] = []
    override_path = _resolve_global_mosaic_path(output_dir, overrides.get("global_wcs_path"))
    override_json = _resolve_global_mosaic_path(output_dir, overrides.get("global_wcs_json"))
    if override_path:
        candidates.append(
            (
                override_path,
                override_json,
                overrides.get("global_wcs_meta") if isinstance(overrides.get("global_wcs_meta"), dict) else None,
                plan["mode"],
            )
        )

    if isinstance(worker_config, dict):
        default_output = worker_config.get("global_wcs_output_path", "global_mosaic_wcs.fits")
    else:
        default_output = "global_mosaic_wcs.fits"
    resolved_default = _resolve_global_mosaic_path(output_dir, default_output)
    if resolved_default:
        candidates.append((resolved_default, None, None, "seestar"))

    for candidate_path, candidate_json, meta_payload, mode_hint in candidates:
        descriptor = _load_global_wcs_descriptor_safe(candidate_path, candidate_json)
        if not descriptor:
            continue
        plan["enabled"] = True
        plan["fits_path"] = candidate_path
        plan["json_path"] = candidate_json
        plan["descriptor"] = descriptor
        plan["wcs"] = descriptor.get("wcs")
        plan["width"] = descriptor.get("width")
        plan["height"] = descriptor.get("height")
        plan["meta"] = _coerce_to_builtin(meta_payload if isinstance(meta_payload, dict) else descriptor.get("metadata") or {})
        mode_final = (mode_hint or plan.get("mode") or "seestar").strip().lower()
        if mode_final not in ("seestar", "classic"):
            mode_final = "seestar"
        plan["mode"] = mode_final
        break

    return plan


def _gather_wcs_items_from_groups(seestar_groups: list[list[dict]] | None) -> list[dict[str, Any]]:
    """Collect unique entries with usable WCS information from grouped raws."""

    if not seestar_groups:
        return []
    entries: list[dict[str, Any]] = []
    seen_paths: set[str] = set()

    def _coerce_shape(entry: dict) -> tuple[int, int] | None:
        shape_candidate = entry.get("shape") or entry.get("phase0_shape") or entry.get("phase1_shape")
        if isinstance(shape_candidate, (list, tuple)) and len(shape_candidate) >= 2:
            try:
                h = int(shape_candidate[0])
                w = int(shape_candidate[1])
                if h > 0 and w > 0:
                    return h, w
            except Exception:
                return None
        height_key = entry.get("height") or entry.get("img_height")
        width_key = entry.get("width") or entry.get("img_width")
        if height_key and width_key:
            try:
                h = int(height_key)
                w = int(width_key)
                if h > 0 and w > 0:
                    return h, w
            except Exception:
                return None
        return None

    for group in seestar_groups or []:
        if not isinstance(group, (list, tuple)):
            continue
        for entry in group:
            if not isinstance(entry, dict):
                continue
            path_val = entry.get("path_raw") or entry.get("path")
            norm_path = None
            if path_val:
                try:
                    path_obj = Path(str(path_val)).expanduser()
                    try:
                        resolved = path_obj.resolve(strict=False)
                    except Exception:
                        resolved = path_obj
                    norm_candidate = str(resolved)
                    if os.name == "nt":
                        norm_candidate = norm_candidate.lower()
                    norm_path = norm_candidate
                except Exception:
                    norm_path = None
            if norm_path and norm_path in seen_paths:
                continue
            wcs_obj = (
                entry.get("wcs")
                or entry.get("phase1_wcs")
                or entry.get("phase0_wcs")
            )
            if wcs_obj is None:
                header = entry.get("header") or entry.get("phase0_header")
                if header is not None and ASTROPY_AVAILABLE and WCS:
                    try:
                        wcs_obj = WCS(header)
                    except Exception:
                        wcs_obj = None
            if wcs_obj is None or not getattr(wcs_obj, "is_celestial", False):
                continue
            shape_hw = _coerce_shape(entry)
            if shape_hw is None:
                continue
            entries.append(
                {
                    "wcs": wcs_obj,
                    "shape": shape_hw,
                    "path": path_val,
                }
            )
            if norm_path:
                seen_paths.add(norm_path)
    return entries


def _runtime_build_global_wcs_plan(
    *,
    seestar_groups: list[list[dict]] | None,
    global_plan: dict[str, Any],
    worker_config: dict | None,
    output_dir: str | None,
    pcb: callable | None = None,
) -> bool:
    """Attempt to compute a global WCS descriptor on the fly when none is available."""

    if global_plan.get("enabled"):
        return True
    if not seestar_groups:
        return False
    if not (
        ZEMOSAIC_UTILS_AVAILABLE
        and zemosaic_utils
        and hasattr(zemosaic_utils, "compute_global_wcs_descriptor")
        and hasattr(zemosaic_utils, "write_global_wcs_files")
        and hasattr(zemosaic_utils, "resolve_global_wcs_output_paths")
    ):
        return False
    output_dir = output_dir or ""
    if not output_dir:
        if pcb:
            pcb("sds_warn_runtime_wcs_no_output_dir", prog=None, lvl="WARN")
        return False
    entries = _gather_wcs_items_from_groups(seestar_groups)
    if not entries:
        if pcb:
            pcb("sds_warn_runtime_wcs_failed", prog=None, lvl="WARN", error="no usable WCS entries")
        return False
    pixel_scale_mode = str(
        (worker_config or {}).get("global_wcs_pixelscale_mode", "median") or "median"
    )
    orientation_mode = str(
        (worker_config or {}).get("global_wcs_orientation", "north_up") or "north_up"
    )
    try:
        padding_percent = float((worker_config or {}).get("global_wcs_padding_percent", 2.0) or 0.0)
    except Exception:
        padding_percent = 2.0
    res_override_raw = (worker_config or {}).get("global_wcs_res_override")
    resolution_override = None
    parse_res = getattr(zemosaic_utils, "parse_global_wcs_resolution_override", None)
    if callable(parse_res):
        try:
            resolution_override = parse_res(res_override_raw)
        except Exception:
            resolution_override = None
    try:
        descriptor = zemosaic_utils.compute_global_wcs_descriptor(
            entries,
            pixel_scale_mode=pixel_scale_mode,
            orientation_mode=orientation_mode,
            padding_percent=padding_percent,
            resolution_override=resolution_override,
            logger_override=logger,
        )
    except Exception as exc:
        if pcb:
            pcb("sds_warn_runtime_wcs_failed", prog=None, lvl="WARN", error=str(exc))
        return False
    try:
        fits_path, json_path = zemosaic_utils.resolve_global_wcs_output_paths(
            output_dir, (worker_config or {}).get("global_wcs_output_path", "global_mosaic_wcs.fits")
        )
    except Exception:
        output_dir_path = Path(output_dir)
        fits_path = str(output_dir_path / "global_mosaic_wcs.fits")
        json_path = f"{fits_path}.json"
    try:
        zemosaic_utils.write_global_wcs_files(descriptor, fits_path, json_path, logger_override=logger)
    except Exception as exc:
        if pcb:
            pcb("sds_warn_runtime_wcs_failed", prog=None, lvl="WARN", error=str(exc))
        return False
    descriptor_loaded = _load_global_wcs_descriptor_safe(fits_path, json_path) or descriptor
    global_plan.update(
        {
            "enabled": True,
            "fits_path": fits_path,
            "json_path": json_path,
            "descriptor": descriptor_loaded,
            "wcs": descriptor_loaded.get("wcs"),
            "width": descriptor_loaded.get("width"),
            "height": descriptor_loaded.get("height"),
            "meta": _coerce_to_builtin(descriptor_loaded.get("metadata") or {}),
            "mode": "seestar",
        }
    )
    if pcb:
        pcb(
            "sds_info_runtime_wcs_built",
            prog=None,
            lvl="INFO_DETAIL",
            entries=len(entries),
        )
    return True


def _zequality_accept_override(metrics: dict[str, float], threshold: float) -> bool:
    """Replicates ZeQualityMT's lenient acceptance rules."""
    if (
        metrics.get("CC", 0.0) >= 0.70
        and metrics.get("CER", 0.0) < 0.35
        and metrics.get("ED", 0.0) < 1.2
    ):
        return True
    if (
        metrics.get("CC", 0.0) >= 0.78
        and metrics.get("SC", 0.0) < 0.28
        and metrics.get("NBR", 0.0) < 0.10
    ):
        return True
    if (
        metrics.get("TRL", 0.0) > 0.30
        and metrics.get("CER", 0.0) < 0.40
        and metrics.get("SC", 0.0) < 0.40
    ):
        return True
    return False


def _prepare_quality_gate_array(arr: np.ndarray) -> np.ndarray:
    """Ensure ZeQualityMT receives an ``H x W x C`` float32 array with <=3 channels."""
    if arr is None:
        raise ValueError("Master tile array is None")
    data = np.asarray(arr, dtype=np.float32)
    if data.ndim == 2:
        data = data[..., np.newaxis]
    if data.ndim != 3:
        raise ValueError(f"Unsupported master tile dimensionality for quality gate: {data.shape}")
    if data.shape[0] in (1, 3) and data.shape[-1] not in (1, 3):
        data = np.moveaxis(data, 0, -1)
    if data.shape[-1] == 1:
        data = np.repeat(data, 3, axis=-1)
    elif data.shape[-1] > 3:
        data = data[..., :3]
    return np.ascontiguousarray(data, dtype=np.float32)


def _prepare_quality_gate_alpha_mask(mask: np.ndarray | None, target_hw: tuple[int, int]) -> np.ndarray | None:
    """Normalize a float alpha mask for quality gate usage."""

    if mask is None:
        return None
    try:
        arr = np.asarray(mask)
    except Exception:
        return None
    if arr.ndim == 3 and arr.shape[-1] == 1:
        arr = arr[..., 0]
    arr = np.squeeze(arr)
    if arr.ndim != 2:
        return None

    arr = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)
    if arr.dtype.kind in {"i", "u"}:
        arr = arr.astype(np.float32, copy=False)
        max_val = float(np.nanmax(arr)) if arr.size else 0.0
        if max_val > 1.0:
            arr *= (1.0 / 255.0)
    elif arr.dtype.kind not in {"f"}:
        arr = arr.astype(np.float32, copy=False)
    arr = np.clip(arr, 0.0, 1.0)

    if arr.shape != target_hw:
        try:
            import cv2  # type: ignore

            arr = cv2.resize(arr, (target_hw[1], target_hw[0]), interpolation=cv2.INTER_LINEAR)
        except Exception:
            return None

    return np.ascontiguousarray(arr, dtype=np.float32)


def _evaluate_quality_gate_metrics(
    tile_id: int,
    arr: np.ndarray,
    *,
    enabled: bool,
    threshold: float,
    edge_band: int,
    k_sigma: float,
    erode_px: int,
    pcb: Optional[Callable] = None,
    alpha_mask: np.ndarray | None = None,
    alpha_soft_threshold: float = QUALITY_GATE_ALPHA_SOFT_THRESHOLD,
) -> Optional[dict[str, Any]]:
    """Compute ZeQualityMT metrics for a stacked tile if enabled and available."""
    if not enabled:
        return None
    if _zq_quality_metrics is None:
        global _ZEQUALITY_WARNING_EMITTED
        emit = False
        with _ZEQUALITY_WARNING_LOCK:
            if not _ZEQUALITY_WARNING_EMITTED:
                _ZEQUALITY_WARNING_EMITTED = True
                emit = True
        if emit:
            msg = "module_missing"
            if pcb:
                pcb("mt_quality_gate_unavailable", prog=None, lvl="WARN", tile_id=int(tile_id), error=msg)
            logger.warning("[ZeQualityMT] quality gate unavailable: zequalityMT module missing")
        return None
    try:
        arr_for_metrics = _prepare_quality_gate_array(arr)
        mask_for_metrics = None
        if alpha_mask is not None:
            mask_for_metrics = _prepare_quality_gate_alpha_mask(alpha_mask, arr_for_metrics.shape[:2])
        if mask_for_metrics is not None:
            thresh = float(np.clip(alpha_soft_threshold, 0.0, 1.0))
            if thresh > 0.0:
                invalid = mask_for_metrics < thresh
                if np.any(invalid):
                    arr_masked = arr_for_metrics.copy()
                    arr_masked[invalid] = np.nan
                    arr_for_metrics = arr_masked
        metrics = _zq_quality_metrics(
            arr_for_metrics,
            edge_band=max(8, int(edge_band)),
            k_sigma=max(0.0, float(k_sigma)),
            erode_px=max(0, int(erode_px)),
        )
        score = float(metrics.get("score", float("nan")))
        accepted = (score <= float(threshold)) or _zequality_accept_override(metrics, float(threshold))
        return {
            "metrics": metrics,
            "score": score,
            "accepted": bool(accepted),
        }
    except Exception as exc:
        if pcb:
            pcb("mt_quality_gate_unavailable", prog=None, lvl="WARN", tile_id=int(tile_id), error=str(exc))
        logger.warning("[ZeQualityMT] quality gate failed for tile %s: %s", tile_id, exc)
        return None


def _move_quality_reject_file(src_path: str) -> tuple[str, bool]:
    """Move a rejected master tile alongside its siblings for review."""
    try:
        src = Path(src_path)
        base_dir = src.parent if src.parent != Path("") else Path(".")
        rej_dir = base_dir / "rejected_by_quality"
        rej_dir.mkdir(parents=True, exist_ok=True)
        candidate = rej_dir / src.name
        if candidate.exists():
            base = candidate.with_suffix("")
            ext = candidate.suffix
            idx = 1
            while (candidate := base.with_name(f"{base.name}_{idx}{ext}")).exists():
                idx += 1
        shutil.move(str(src), str(candidate))
        return str(candidate), True
    except Exception as exc:
        logger.warning("Failed to move rejected master tile %s: %s", src_path, exc)
        return src_path, False


@dataclass
class _TileAffineSource:
    """Container for intertile photometric calibration inputs."""

    path: str | None
    wcs: Any
    data: np.ndarray | None = None


def _apply_preview_quality_crop(
    tile_array: "np.ndarray",
    crop_settings: dict | None,
) -> "np.ndarray":
    """Apply quality-crop heuristics to candidate previews when available."""

    if tile_array is None or not crop_settings:
        return tile_array
    if not crop_settings.get("enabled", False) or not ANCHOR_AUTOCROP_AVAILABLE:
        return tile_array

    arr = np.asarray(tile_array, dtype=np.float32)
    if arr.ndim == 2:
        arr = arr[..., np.newaxis]
    if arr.ndim != 3 or arr.shape[-1] == 0:
        return tile_array

    h, w, c = arr.shape
    if h < 8 or w < 8:
        return tile_array

    if c == 1:
        rgb_view = np.repeat(arr, 3, axis=-1)
    elif c >= 3:
        rgb_view = arr[..., :3]
    else:
        pad_count = 3 - c
        rgb_view = np.concatenate([arr, np.repeat(arr[..., -1:], pad_count, axis=-1)], axis=-1)

    try:
        band_px = max(4, int(crop_settings.get("band_px", 32)))
    except Exception:
        band_px = 32
    try:
        margin_px = max(0, int(crop_settings.get("margin_px", 8)))
    except Exception:
        margin_px = 8
    try:
        k_sigma = float(crop_settings.get("k_sigma", 2.0))
        if not math.isfinite(k_sigma):
            raise ValueError
    except Exception:
        k_sigma = 2.0
    k_sigma = max(0.1, min(k_sigma, 10.0))

    try:
        lum2d = rgb_view.mean(axis=-1)
        R = rgb_view[..., 0]
        G = rgb_view[..., 1]
        B = rgb_view[..., 2]
        y0, x0, y1, x1 = _anchor_detect_autocrop(
            lum2d,
            R,
            G,
            B,
            band_px=band_px,
            k_sigma=k_sigma,
            margin_px=margin_px,
        )
    except Exception:
        return tile_array

    if not (0 <= y0 < y1 <= h and 0 <= x0 < x1 <= w):
        return tile_array

    crop_area = (y1 - y0) * (x1 - x0)
    full_area = h * w if h > 0 and w > 0 else 0
    if crop_area <= 0 or (full_area > 0 and (crop_area / full_area) >= 0.97):
        return tile_array

    cropped = arr[y0:y1, x0:x1, ...]
    return np.ascontiguousarray(cropped, dtype=np.float32)


def _score_anchor_candidate(
    stats: dict,
    group_median: float,
    deviation_clip: float | None = None,
) -> float:
    """Compute a quality score for an anchor candidate (lower is better)."""

    try:
        median_val = float(stats.get("median", 0.0))
    except Exception:
        median_val = 0.0
    try:
        span_val = float(stats.get("span"))
    except Exception:
        try:
            span_val = float(stats.get("high", 0.0) - stats.get("low", 0.0))
        except Exception:
            span_val = 0.0
    try:
        robust_sigma = float(stats.get("robust_sigma", 0.0))
    except Exception:
        robust_sigma = 0.0

    if not math.isfinite(span_val):
        span_val = 0.0
    if not math.isfinite(robust_sigma):
        robust_sigma = 0.0

    deviation = abs(median_val - group_median)
    if deviation_clip is not None and deviation_clip > 0:
        deviation = min(deviation, float(deviation_clip))

    return float(deviation + 0.7 * max(span_val, 0.0) + 0.3 * max(robust_sigma, 0.0))


def _sanitize_affine_corrections(
    raw_corrections: Any,
    total_tiles: int,
) -> tuple[list[tuple[float, float]] | None, bool]:
    """Normalize gain/offset pairs and detect non-trivial entries."""

    if raw_corrections is None:
        return None, False

    sanitized: list[tuple[float, float]] = []
    nontrivial = False

    for idx in range(max(total_tiles, 0)):
        if isinstance(raw_corrections, dict):
            candidate = raw_corrections.get(idx, (1.0, 0.0))
        else:
            try:
                candidate = raw_corrections[idx]
            except (IndexError, TypeError):
                candidate = (1.0, 0.0)

        try:
            gain_val = float(candidate[0])
        except Exception:
            gain_val = 1.0
        try:
            offset_val = float(candidate[1])
        except Exception:
            offset_val = 0.0

        if not np.isfinite(gain_val):
            gain_val = 1.0
        if not np.isfinite(offset_val):
            offset_val = 0.0

        if not nontrivial and (abs(gain_val - 1.0) > 1e-6 or abs(offset_val) > 1e-6):
            nontrivial = True

        sanitized.append((gain_val, offset_val))

    if not sanitized:
        return None, False

    if not nontrivial:
        return None, False

    return sanitized, True


def _select_affine_log_indices(
    affine_list: list[tuple[float, float]] | None,
) -> set[int]:
    """Pick representative tile indices (1-based) for logging."""

    if not affine_list:
        return set()

    nontrivial_indices = [
        idx + 1
        for idx, (gain_val, offset_val) in enumerate(affine_list)
        if abs(gain_val - 1.0) > 1e-6 or abs(offset_val) > 1e-6
    ]
    if not nontrivial_indices:
        return set()

    sample_span = max(1, len(nontrivial_indices) // 4)
    selected = set(nontrivial_indices[::sample_span])
    selected.add(nontrivial_indices[0])
    selected.add(nontrivial_indices[-1])
    return selected


def _build_affine_lookup_for_tiles(
    tiles: list[dict[str, Any]],
    affine_list: list[tuple[float, float]] | None,
) -> tuple[dict[str, tuple[float, float]] | None, str | None]:
    """Map sanitized affine corrections to ``tile_id`` entries."""

    if not tiles or not affine_list:
        return None, None

    expected = len(tiles)
    if len(affine_list) != expected:
        return None, f"expected={expected}, got={len(affine_list)}"

    lookup: dict[str, tuple[float, float]] = {}
    missing: list[str] = []
    for tile_entry, affine in zip(tiles, affine_list):
        tile_id = tile_entry.get("tile_id") if isinstance(tile_entry, dict) else None
        if not tile_id:
            missing.append(f"idx{len(lookup)}")
            continue
        if tile_id in lookup:
            missing.append(f"dup:{tile_id}")
            continue
        try:
            gain_val = float(affine[0])
        except Exception:
            gain_val = 1.0
        try:
            offset_val = float(affine[1])
        except Exception:
            offset_val = 0.0
        lookup[str(tile_id)] = (gain_val, offset_val)

    if missing:
        preview = ", ".join(missing[:5])
        detail = f"missing_ids={preview}"
        return None, detail

    return lookup or None, None


def _compose_global_anchor_shift(
    affine_list: list[tuple[float, float]] | None,
    total_tiles: int,
    anchor_shift: tuple[float, float] | None,
) -> tuple[list[tuple[float, float]] | None, bool]:
    """Apply a global anchor shift to per-tile affine corrections."""

    if not anchor_shift:
        return affine_list, False
    try:
        gain_shift = float(anchor_shift[0])
    except Exception:
        gain_shift = 1.0
    try:
        offset_shift = float(anchor_shift[1])
    except Exception:
        offset_shift = 0.0
    if not np.isfinite(gain_shift):
        gain_shift = 1.0
    if not np.isfinite(offset_shift):
        offset_shift = 0.0
    if abs(gain_shift - 1.0) < 1e-6 and abs(offset_shift) < 1e-6:
        return affine_list, False

    total_tiles = max(0, int(total_tiles))
    if affine_list is None:
        if total_tiles <= 0:
            return None, False
        composed = [(gain_shift, offset_shift)] * total_tiles
    else:
        composed: list[tuple[float, float]] = []
        for idx in range(total_tiles):
            if idx < len(affine_list):
                raw = affine_list[idx]
            else:
                raw = (1.0, 0.0)
            try:
                g = float(raw[0])
            except Exception:
                g = 1.0
            try:
                o = float(raw[1])
            except Exception:
                o = 0.0
            if not np.isfinite(g):
                g = 1.0
            if not np.isfinite(o):
                o = 0.0
            new_gain = g * gain_shift
            new_offset = o * gain_shift + offset_shift
            composed.append((float(new_gain), float(new_offset)))
        composed.extend([(gain_shift, offset_shift)] * max(0, total_tiles - len(composed)))
    result = composed
    nontrivial = any(abs(g - 1.0) > 1e-6 or abs(o) > 1e-6 for g, o in result) if result else False
    return result, nontrivial


@dataclass
class _InterMasterTile:
    index: int
    path: str
    wcs: Any
    shape_hw: tuple[int, int]
    sky_corners: Any | None = None
    bbox_bounds: tuple[float, float, float, float] | None = None


def _phase45_resolve_tile_shape(path: str | None, tile_wcs: Any) -> tuple[int, int] | None:
    if path is None or not _path_exists(path):
        return None
    if tile_wcs is not None and getattr(tile_wcs, "pixel_shape", None):
        try:
            px_shape = tile_wcs.pixel_shape
            if px_shape and len(px_shape) >= 2:
                h = int(px_shape[1])
                w = int(px_shape[0])
                if h > 0 and w > 0:
                    return (h, w)
        except Exception:
            pass
    try:
        with fits.open(path, memmap=True, do_not_scale_image_data=True) as hdul:
            data_shape = hdul[0].shape if hdul and hdul[0] is not None else None
            if not data_shape:
                return None
            if len(data_shape) == 2:
                return (int(data_shape[0]), int(data_shape[1]))
            if len(data_shape) == 3:
                return (int(data_shape[0]), int(data_shape[1]))
    except Exception:
        return None
    return None


def _phase45_tile_corners(tile: _InterMasterTile) -> Any | None:
    if tile.sky_corners is not None:
        return tile.sky_corners
    if not (tile.wcs and tile.wcs.is_celestial and tile.shape_hw):
        return None
    try:
        h, w = tile.shape_hw
        # Use pixel-edge coordinates (-0.5 .. width-0.5) so the projected polygon
        # matches the actual coverage footprint instead of pixel centres.
        xs = np.asarray(
            [-0.5, float(w) - 0.5, float(w) - 0.5, -0.5],
            dtype=np.float64,
        )
        ys = np.asarray(
            [-0.5, -0.5, float(h) - 0.5, float(h) - 0.5],
            dtype=np.float64,
        )
        sky = tile.wcs.pixel_to_world(xs, ys)
        tile.sky_corners = sky
        return sky
    except Exception:
        return None


def _phase45_tile_bbox(tile: _InterMasterTile) -> tuple[float, float, float, float] | None:
    if tile.bbox_bounds is not None:
        return tile.bbox_bounds
    corners = _phase45_tile_corners(tile)
    if corners is None:
        return None
    try:
        celestial = corners.icrs
    except Exception:
        celestial = corners
    ra_vals: np.ndarray | None = None
    dec_vals: np.ndarray | None = None
    for attr_name in (("ra", "dec"), ("lon", "lat")):
        ra_attr = getattr(celestial, attr_name[0], None)
        dec_attr = getattr(celestial, attr_name[1], None)
        if ra_attr is None or dec_attr is None:
            continue
        try:
            ra_vals = np.asarray(ra_attr.deg, dtype=np.float64).ravel()
            dec_vals = np.asarray(dec_attr.deg, dtype=np.float64).ravel()
            break
        except Exception:
            continue
    if ra_vals is None or dec_vals is None:
        try:
            ra_vals = np.asarray(celestial.spherical.lon.deg, dtype=np.float64).ravel()
            dec_vals = np.asarray(celestial.spherical.lat.deg, dtype=np.float64).ravel()
        except Exception:
            return None
    mask = np.isfinite(ra_vals) & np.isfinite(dec_vals)
    if not np.any(mask):
        return None
    ra_vals = ra_vals[mask]
    dec_vals = dec_vals[mask]
    bbox = (
        float(np.min(ra_vals)),
        float(np.max(ra_vals)),
        float(np.min(dec_vals)),
        float(np.max(dec_vals)),
    )
    tile.bbox_bounds = bbox
    return bbox


def _phase45_group_bbox(members: list[_InterMasterTile]) -> dict[str, float] | None:
    ra_min: float | None = None
    ra_max: float | None = None
    dec_min: float | None = None
    dec_max: float | None = None
    for tile in members:
        bbox = _phase45_tile_bbox(tile)
        if not bbox:
            continue
        t_ra_min, t_ra_max, t_dec_min, t_dec_max = bbox
        ra_min = t_ra_min if ra_min is None else min(ra_min, t_ra_min)
        ra_max = t_ra_max if ra_max is None else max(ra_max, t_ra_max)
        dec_min = t_dec_min if dec_min is None else min(dec_min, t_dec_min)
        dec_max = t_dec_max if dec_max is None else max(dec_max, t_dec_max)
    if None in (ra_min, ra_max, dec_min, dec_max):
        return None
    return {
        "ra_min": float(ra_min),
        "ra_max": float(ra_max),
        "dec_min": float(dec_min),
        "dec_max": float(dec_max),
    }


def _phase45_project_polygon(corners: Any, reference: Any) -> np.ndarray | None:
    if corners is None or reference is None:
        return None
    try:
        offset_frame = reference.skyoffset_frame()
        projected = corners.transform_to(offset_frame)
        x = np.asarray(projected.lon.arcsec, dtype=np.float64)
        y = np.asarray(projected.lat.arcsec, dtype=np.float64)
        return np.stack([x, y], axis=1)
    except Exception:
        return None


def _phase45_polygon_area(poly: np.ndarray | None) -> float:
    if poly is None or len(poly) < 3:
        return 0.0
    x = poly[:, 0]
    y = poly[:, 1]
    return float(abs(np.dot(x, np.roll(y, -1)) - np.dot(y, np.roll(x, -1))) * 0.5)


def _phase45_clip_polygon(subject: np.ndarray, clipper: np.ndarray) -> np.ndarray:
    if subject.size == 0 or clipper.size == 0:
        return np.empty((0, 2), dtype=np.float64)

    def _edge_intersection(p1, p2, q1, q2):
        x1, y1 = p1
        x2, y2 = p2
        x3, y3 = q1
        x4, y4 = q2
        denom = (x1 - x2) * (y3 - y4) - (y1 - y2) * (x3 - x4)
        if abs(denom) < 1e-9:
            return p2
        num_x = (x1 * y2 - y1 * x2) * (x3 - x4) - (x1 - x2) * (x3 * y4 - y3 * x4)
        num_y = (x1 * y2 - y1 * x2) * (y3 - y4) - (y1 - y2) * (x3 * y4 - y3 * x4)
        return np.array([num_x / denom, num_y / denom], dtype=np.float64)

    def _inside(point, edge_start, edge_end):
        return (edge_end[0] - edge_start[0]) * (point[1] - edge_start[1]) - (edge_end[1] - edge_start[1]) * (point[0] - edge_start[0]) >= 0

    output = subject
    clip_points = clipper
    for i in range(len(clip_points)):
        next_output = []
        a = clip_points[i]
        b = clip_points[(i + 1) % len(clip_points)]
        if not len(output):
            break
        prev = output[-1]
        for curr in output:
            if _inside(curr, a, b):
                if not _inside(prev, a, b):
                    next_output.append(_edge_intersection(prev, curr, a, b))
                next_output.append(curr)
            elif _inside(prev, a, b):
                next_output.append(_edge_intersection(prev, curr, a, b))
            prev = curr
        output = np.asarray(next_output, dtype=np.float64)
        if output.size == 0:
            break
    return output if output.size else np.empty((0, 2), dtype=np.float64)


def _phase45_overlap_fraction(tile_a: _InterMasterTile, tile_b: _InterMasterTile) -> float:
    if not (ASTROPY_AVAILABLE and SkyCoord and tile_a and tile_b):
        return 0.0
    sky_a = _phase45_tile_corners(tile_a)
    sky_b = _phase45_tile_corners(tile_b)
    if sky_a is None or sky_b is None:
        return 0.0
    try:
        ra_all = np.concatenate([sky_a.ra.deg, sky_b.ra.deg])
        dec_all = np.concatenate([sky_a.dec.deg, sky_b.dec.deg])
        if ra_all.size == 0 or dec_all.size == 0:
            return 0.0
        center = SkyCoord(ra=np.nanmean(ra_all), dec=np.nanmean(dec_all), unit="deg")
    except Exception:
        return 0.0

    poly_a = _phase45_project_polygon(sky_a, center)
    poly_b = _phase45_project_polygon(sky_b, center)
    if poly_a is None or poly_b is None or poly_a.size == 0 or poly_b.size == 0:
        return 0.0
    area_a = _phase45_polygon_area(poly_a)
    area_b = _phase45_polygon_area(poly_b)
    if area_a <= 0.0 or area_b <= 0.0:
        return 0.0
    intersection = _phase45_clip_polygon(poly_a, poly_b)
    inter_area = _phase45_polygon_area(intersection)
    if inter_area <= 0.0:
        return 0.0
    return float(inter_area / max(1e-9, min(area_a, area_b)))


def _phase45_compute_cutout_wcs(final_wcs: Any, final_shape_hw: tuple[int, int] | None, tiles: list[_InterMasterTile]) -> tuple[Any | None, tuple[int, int] | None]:
    if not (final_wcs and final_shape_hw and tiles):
        return None, None
    try:
        wcs_copy = final_wcs.deepcopy() if hasattr(final_wcs, "deepcopy") else copy.deepcopy(final_wcs)
    except Exception:
        wcs_copy = copy.deepcopy(final_wcs)
    if wcs_copy is None:
        return None, None
    height, width = int(final_shape_hw[0]), int(final_shape_hw[1])
    edge_min_x = float("inf")
    edge_min_y = float("inf")
    edge_max_x = float("-inf")
    edge_max_y = float("-inf")
    valid = False
    for tile in tiles:
        sky = _phase45_tile_corners(tile)
        if sky is None:
            continue
        try:
            px, py = wcs_copy.world_to_pixel(sky)
            if px is None or py is None:
                continue
            px = np.asarray(px, dtype=np.float64)
            py = np.asarray(py, dtype=np.float64)
        except Exception:
            continue
        if px.size == 0 or py.size == 0:
            continue
        edge_min_x = min(edge_min_x, float(np.nanmin(px)))
        edge_min_y = min(edge_min_y, float(np.nanmin(py)))
        edge_max_x = max(edge_max_x, float(np.nanmax(px)))
        edge_max_y = max(edge_max_y, float(np.nanmax(py)))
        valid = True
    if not valid:
        return None, None
    # Convert edge coordinates (relative to pixel boundaries) into integer pixel
    # indices that map to the same mosaic grid as the final WCS.
    start_x = max(0, int(math.floor(edge_min_x + 0.5)))
    start_y = max(0, int(math.floor(edge_min_y + 0.5)))
    stop_x = min(width, int(math.ceil(edge_max_x - 0.5)) + 1)
    stop_y = min(height, int(math.ceil(edge_max_y - 0.5)) + 1)
    if stop_x <= start_x:
        stop_x = min(width, start_x + 1)
    if stop_y <= start_y:
        stop_y = min(height, start_y + 1)
    local_w = max(1, int(stop_x - start_x))
    local_h = max(1, int(stop_y - start_y))
    try:
        if hasattr(wcs_copy, "wcs") and hasattr(wcs_copy.wcs, "crpix"):
            wcs_copy.wcs.crpix[0] -= start_x
            wcs_copy.wcs.crpix[1] -= start_y
        if hasattr(wcs_copy, "array_shape"):
            wcs_copy.array_shape = (local_h, local_w)
    except Exception:
        pass
    return wcs_copy, (local_h, local_w)


def _phase45_allocate_stack_storage(
    count: int,
    shape_hw: tuple[int, int],
    channels: int,
    policy: str,
    temp_dir: str | None,
) -> tuple[np.ndarray, str | None]:
    dtype = np.float32
    h, w = shape_hw
    policy_norm = str(policy or "auto").lower()
    use_memmap = False
    if policy_norm == "always":
        use_memmap = True
    elif policy_norm == "auto":
        estimated_bytes = count * h * w * channels * np.dtype(dtype).itemsize
        use_memmap = estimated_bytes > (256 * 1024 * 1024)
    if not use_memmap:
        return np.full((count, h, w, channels), np.nan, dtype=dtype), None
    directory_path = Path(temp_dir) if temp_dir else get_runtime_temp_dir()
    try:
        directory_path.mkdir(parents=True, exist_ok=True)
    except Exception:
        directory_path = Path(
            tempfile.mkdtemp(prefix="phase45_stack_", dir=str(get_runtime_temp_dir()))
        )
    file_path = directory_path / f"phase45_stack_{uuid.uuid4().hex}.dat"
    storage = np.memmap(str(file_path), dtype=dtype, mode="w+", shape=(count, h, w, channels))
    storage[:] = np.nan
    return storage, str(file_path)


def _phase45_cleanup_storage(storage: np.ndarray | None, memmap_path: str | None) -> None:
    try:
        if isinstance(storage, np.memmap):
            storage.flush()
    except Exception:
        pass
    if memmap_path:
        path_obj = Path(memmap_path)
        if path_obj.exists():
            try:
                path_obj.unlink()
            except Exception:
                pass


def _run_phase4_5_inter_master_merge(
    master_tiles: list[tuple[str | None, Any]],
    final_output_wcs: Any,
    final_output_shape_hw: tuple[int, int] | None,
    temp_storage_dir: str | None,
    output_folder: str,
    cache_retention_mode: str,
    inter_cfg: dict,
    stack_cfg: dict,
    progress_callback: Callable | None,
    pcb: Callable[..., None],
) -> list[tuple[str | None, Any]]:
    enable = bool(inter_cfg.get("enable", False))
    if not enable:
        return master_tiles
    if len(master_tiles) < 2:
        return master_tiles
    if not (ASTROPY_AVAILABLE and REPROJECT_AVAILABLE and ZEMOSAIC_UTILS_AVAILABLE and ZEMOSAIC_ALIGN_STACK_AVAILABLE and reproject_interp):
        pcb("p45_finished", prog=None, lvl="INFO_DETAIL", tiles_in=len(master_tiles), tiles_out=len(master_tiles))
        return master_tiles

    threshold = float(inter_cfg.get("overlap_threshold", 0.60))
    threshold = max(0.0, min(1.0, threshold))
    min_group = max(2, int(inter_cfg.get("min_group_size", 2)))
    max_group = max(min_group, int(inter_cfg.get("max_group", 64)))
    memmap_policy = str(inter_cfg.get("memmap_policy", "auto")).lower()
    local_scale = str(inter_cfg.get("local_scale", "native")).lower()
    if local_scale not in {"final", "native"}:
        local_scale = "native"

    photometry_intragroup = bool(inter_cfg.get("photometry_intragroup", True))
    photometry_intersuper = bool(inter_cfg.get("photometry_intersuper", True))  # réservé pour plus tard
    try:
        photometry_clip_sigma = float(inter_cfg.get("photometry_clip_sigma", 3.0))
    except Exception:
        photometry_clip_sigma = 3.0
    if not math.isfinite(photometry_clip_sigma):
        photometry_clip_sigma = 3.0
    photometry_clip_sigma = max(0.1, photometry_clip_sigma)

    pcb("p45_start", prog=None, lvl="INFO")

    def _phase45_gui_emit(key=None, *, level: str = "DEBUG", **kwargs) -> bool:
        """Emit a GUI log safely, falling back silently if the GUI is detached."""
        if not pcb:
            return False
        try:
            pcb(key, prog=None, lvl=level, **kwargs)
            return True
        except Exception:
            return False

    def _phase45_gui_message(message: str, level: str = "DEBUG") -> bool:
        """Send a free-form heartbeat message to the GUI log."""
        return _phase45_gui_emit(message, level=level)

    logger.debug(
        "[P4.5] Starting Inter-Master merge: threshold=%.2f, min_group=%d, max_group=%d, "
        "memmap=%s, local_scale=%s, master_tiles=%d",
        threshold,
        min_group,
        max_group,
        memmap_policy,
        local_scale,
        len(master_tiles),
    )
    if not _phase45_gui_emit(
        "p45_group_info",
        level="DEBUG",
        group=0,
        total=len(master_tiles),
        members=len(master_tiles),
    ):
        _phase45_gui_message(
            f"Phase 4.5: preparing overlap graph (tiles={len(master_tiles)})"
        )

    micro_align_available = hasattr(zemosaic_align_stack, "micro_align_stack")
    photometry_estimator_available = hasattr(zemosaic_align_stack, "estimate_affine_photometry")
    photometry_apply_available = hasattr(zemosaic_align_stack, "apply_affine_photometry")

    tiles: list[_InterMasterTile] = []
    group_super_counts: dict[int, int] = {}
    for idx, (path, wcs_obj) in enumerate(master_tiles):
        shape_hw = _phase45_resolve_tile_shape(path, wcs_obj)
        if not shape_hw:
            continue
        tiles.append(_InterMasterTile(index=idx, path=path, wcs=wcs_obj, shape_hw=shape_hw))

    logger.debug(
        "[P4.5] Candidate tiles retained for overlap graph: %d/%d",
        len(tiles),
        len(master_tiles),
    )
    _phase45_gui_message(f"Phase 4.5: building overlap graph with {len(tiles)} tiles")

    if len(tiles) < min_group:
        pcb("p45_finished", prog=None, lvl="INFO_DETAIL", tiles_in=len(master_tiles), tiles_out=len(master_tiles))
        return master_tiles

    tile_map = {tile.index: tile for tile in tiles}
    adjacency: dict[int, set[int]] = {tile.index: set() for tile in tiles}
    total_pairs = 0
    for i, tile_a in enumerate(tiles):
        for tile_b in tiles[i + 1 :]:
            overlap = _phase45_overlap_fraction(tile_a, tile_b)
            if overlap >= threshold:
                adjacency[tile_a.index].add(tile_b.index)
                adjacency[tile_b.index].add(tile_a.index)
                total_pairs += 1
        if i and i % 10 == 0:
            msg = (
                f"Phase 4.5: overlap scan {i}/{len(tiles)} (edges>={threshold:.2f}: {total_pairs})"
            )
            logger.debug("[P4.5] %s", msg)
            _phase45_gui_message(msg)
    logger.debug(
        "[P4.5] Overlap graph: %d tiles, %d edges >= %.2f",
        len(tiles),
        total_pairs,
        threshold,
    )
    _phase45_gui_message(
        f"Phase 4.5: overlap graph ready ({total_pairs} edges >= {threshold:.2f})"
    )
    visited: set[int] = set()
    groups: list[list[int]] = []
    total_components = 0
    valid_components = 0
    for tile in tiles:
        if tile.index in visited:
            continue
        stack = [tile.index]
        component: list[int] = []
        while stack:
            node = stack.pop()
            if node in visited:
                continue
            visited.add(node)
            component.append(node)
            stack.extend(adjacency.get(node, ()))
        total_components += 1
        if len(component) >= min_group:
            groups.append(sorted(component))
            valid_components += 1

    logger.debug(
        "[P4.5] Connected components: total=%d, meeting_min_group=%d (min_group=%d)",
        total_components,
        valid_components,
        min_group,
    )
    if not groups:
        _phase45_gui_message("Phase 4.5: no groups reached the minimum overlap")

    if not groups:
        pcb("p45_finished", prog=None, lvl="INFO_DETAIL", tiles_in=len(master_tiles), tiles_out=len(master_tiles))
        return master_tiles

    _phase45_gui_message(
        f"Phase 4.5: overlap grouping ready ({len(groups)} groups >= {min_group})"
    )
    group_layout_payload: list[dict[str, Any]] = []
    for gid, component in enumerate(groups, start=1):
        member_tiles = [tile_map[idx] for idx in component if idx in tile_map]
        if not member_tiles:
            continue
        repr_idx = min(tile.index for tile in member_tiles)
        bbox = _phase45_group_bbox(member_tiles)
        group_layout_payload.append(
            {
                "group_id": gid,
                "members": [tile.index for tile in member_tiles],
                "repr": repr_idx,
                "bbox": bbox,
            }
        )
    if group_layout_payload:
        logger.debug("[P4.5] Emitting groups layout payload (%d groups)", len(group_layout_payload))
        _phase45_gui_emit(
            "p45_groups_layout",
            level="DEBUG",
            groups=group_layout_payload,
            total_groups=len(groups),
        )

    total_chunks = sum(max(1, math.ceil(len(group) / max_group)) for group in groups)
    logger.debug(
        "[P4.5] Planned processing batches: groups=%d, chunks=%d, max_group=%d",
        len(groups),
        total_chunks,
        max_group,
    )
    _phase45_gui_message(
        f"Phase 4.5: processing {len(groups)} groups / {total_chunks} chunks"
    )
    processed_chunks = 0
    replacements: dict[int, tuple[str | None, Any]] = {}
    consumed_indices: set[int] = set()
    cleanup_paths: list[str] = []

    for group_id, group_indices in enumerate(groups, start=1):
        members = [tile_map[idx] for idx in group_indices if idx in tile_map]
        if not members:
            continue
        group_chunks = max(1, math.ceil(len(members) / max_group))
        member_ids = [tile.index for tile in members]
        group_completed_chunks = 0
        if not _phase45_gui_emit(
            "p45_group_info",
            level="DEBUG",
            group=group_id,
            total=len(groups),
            members=len(members),
            chunks=group_chunks,
        ):
            _phase45_gui_message(
                f"Phase 4.5: group {group_id} queued ({len(members)} tiles)"
            )
        logger.debug(
            "[P4.5][G%03d] Preparing group: members=%d, chunks=%d, ids=%s",
            group_id,
            len(members),
            group_chunks,
            member_ids,
        )
        for chunk_start in range(0, len(members), max_group):
            chunk_tiles = members[chunk_start : chunk_start + max_group]
            processed_chunks += 1
            if progress_callback:
                try:
                    progress_callback("phase4_5", processed_chunks, total_chunks)
                except Exception:
                    pass
            chunk_idx = min(group_chunks, (chunk_start // max_group) + 1)
            _phase45_gui_emit(
                "p45_group_started",
                level="DEBUG",
                group_id=group_id,
                chunk=chunk_idx,
                chunks=group_chunks,
                size=len(chunk_tiles),
            )
            _phase45_gui_emit(
                "p45_group_progress",
                level="ETA_LEVEL",
                group_id=group_id,
                chunk=chunk_idx,
                done=group_completed_chunks,
                total=group_chunks,
                size=len(chunk_tiles),
            )
            if not _phase45_gui_emit(
                "p45_group_info",
                level="DEBUG",
                group=group_id,
                total=len(groups),
                members=len(chunk_tiles),
                chunk=chunk_idx,
                chunks=group_chunks,
                processed=processed_chunks,
                total_chunks=total_chunks,
            ):
                _phase45_gui_message(
                    f"Phase 4.5: processing chunk {chunk_idx}/{group_chunks} "
                    f"(group {group_id}, global {processed_chunks}/{total_chunks})"
                )
            logger.debug(
                "[P4.5][G%03d] Chunk %d/%d (%d tiles) - global chunk %d/%d",
                group_id,
                chunk_idx,
                group_chunks,
                len(chunk_tiles),
                processed_chunks,
                total_chunks,
            )

            reference_tile = chunk_tiles[0]
            local_wcs_source = "native"
            local_wcs = reference_tile.wcs
            local_shape = reference_tile.shape_hw
            if local_scale == "final" and final_output_wcs and final_output_shape_hw:
                cutout_wcs, cutout_shape = _phase45_compute_cutout_wcs(
                    final_output_wcs, final_output_shape_hw, chunk_tiles
                )
                if cutout_wcs is not None and cutout_shape is not None:
                    local_wcs = cutout_wcs
                    local_shape = cutout_shape
                    local_wcs_source = "cutout"

            if not (local_wcs and local_shape):
                continue
            logger.debug(
                "[P4.5][G%03d][cutout] Local frame resolved: source=%s, scale=%s, shape=%s, ref_tile=%d",
                group_id,
                local_wcs_source,
                local_scale,
                tuple(local_shape),
                reference_tile.index,
            )
            _phase45_gui_message(
                f"Phase 4.5: group {group_id} local frame {local_shape[0]}x{local_shape[1]}"
            )

            chunk_affine_corrections: list[tuple[float, float] | None] = [None] * len(chunk_tiles)
            chunk_photometry_done = False

            try:
                kappa_val = float(stack_cfg.get("kappa_low", 3.0))
            except Exception:
                kappa_val = 3.0
            limits_val = stack_cfg.get("winsor_limits", (0.05, 0.05))
            try:
                limits_val = (
                    float(limits_val[0]),
                    float(limits_val[1]),
                )
            except Exception:
                limits_val = (0.05, 0.05)
            try:
                max_pass_val = int(stack_cfg.get("winsor_max_frames_per_pass", 0))
            except Exception:
                max_pass_val = 0
            try:
                worker_limit_val = int(stack_cfg.get("winsor_worker_limit", 1))
            except Exception:
                worker_limit_val = 1
            current_parallel_plan = getattr(zconfig, "parallel_plan", worker_config_cache.get("parallel_plan"))
            stack_kwargs = {
                "kappa": kappa_val,
                "winsor_limits": limits_val,
                "winsor_max_frames_per_pass": max_pass_val,
                "winsor_max_workers": max(1, worker_limit_val),
            }
            super_arr = None
            # --- Phase 4.5 (améliorée) : 2.1 → 3.1 à l’intérieur de 4.5 ---
            # Réutiliser les options choisies dans le GUI :
            #   normalize_method, weight_method, reject_algo, final_combine,
            #   kappa/winsor/workers déjà présents dans stack_cfg/stack_kwargs.
            try:
                norm_method = str(
                    stack_cfg.get(
                        "stacking_normalize_method",
                        stack_cfg.get(
                            "normalize_method",
                            stack_cfg.get("stack_norm_method", "none"),
                        ),
                    )
                ).lower()
            except Exception:
                norm_method = "none"
            try:
                weight_method = str(stack_cfg.get("weight_method", stack_cfg.get("stack_weight_method", "none"))).lower()
            except Exception:
                weight_method = "none"
            try:
                reject_algo = str(stack_cfg.get("reject_algo", stack_cfg.get("stack_reject_algo", "winsorized_sigma_clip"))).lower()
            except Exception:
                reject_algo = "winsorized_sigma_clip"
            try:
                final_combine = str(stack_cfg.get("final_combine", stack_cfg.get("stack_final_combine", "mean"))).lower()
            except Exception:
                final_combine = "mean"

            normalize_method = norm_method
            do_chunk_photometry = (
                photometry_intragroup
                and normalize_method not in ("", "none")
                and ZEMOSAIC_ALIGN_STACK_AVAILABLE
                and photometry_estimator_available
                and photometry_apply_available
            )
            if do_chunk_photometry:
                try:
                    preview_size = int(stack_cfg.get("intertile_preview_size", 512))
                except Exception:
                    preview_size = 512
                try:
                    overlap_min = float(stack_cfg.get("intertile_overlap_min", 0.05))
                except Exception:
                    overlap_min = 0.05
                sky_percent_cfg = stack_cfg.get("intertile_sky_percentile") or (30.0, 70.0)
                if not (isinstance(sky_percent_cfg, (list, tuple)) and len(sky_percent_cfg) >= 2):
                    sky_percent_cfg = (30.0, 70.0)
                try:
                    sky_low = float(sky_percent_cfg[0])
                except Exception:
                    sky_low = 30.0
                try:
                    sky_high = float(sky_percent_cfg[1])
                except Exception:
                    sky_high = 70.0
                if sky_low > sky_high:
                    sky_low, sky_high = sky_high, sky_low
                try:
                    robust_clip = float(stack_cfg.get("intertile_robust_clip_sigma", 2.5))
                except Exception:
                    robust_clip = 2.5
                robust_clip = max(0.1, robust_clip)
                sources: list[_TileAffineSource] = []
                valid_indices: list[int] = []
                for idx_tile, tile in enumerate(chunk_tiles):
                    path = tile.path
                    if not path:
                        continue
                    try:
                        arr, _, _ = load_image_with_optional_alpha(path, tile_label=_safe_basename(path))
                        arr = np.asarray(arr, dtype=np.float32, copy=False)
                        sources.append(_TileAffineSource(path=path, wcs=tile.wcs, data=arr))
                        valid_indices.append(idx_tile)
                    except Exception as exc:
                        logger.warning("[P4.5] Failed to read tile data for photometry (%s): %s", path, exc)
                if len(sources) >= 2:
                    pcb(
                        "p45_photometry_start",
                        prog=None,
                        lvl="INFO_DETAIL",
                        group=group_id,
                        chunk=chunk_idx,
                        tiles=len(sources),
                        method=normalize_method,
                    )
                    logger.info(
                        "[P4.5][photometry] Intra-group normalization start (group=%03d chunk=%d, method=%s, tiles=%d)",
                        group_id,
                        chunk_idx,
                        normalize_method,
                        len(sources),
                    )
                    try:
                        affine_raw = zemosaic_align_stack.estimate_affine_photometry(
                            sources,
                            preview_size=preview_size,
                            overlap_min=overlap_min,
                            sky_percentile=(sky_low, sky_high),
                            robust_clip_sigma=min(robust_clip, float(photometry_clip_sigma)),
                        )
                        corrections, nontrivial = _sanitize_affine_corrections(
                            affine_raw,
                            len(sources),
                        )
                        if corrections and nontrivial:
                            for corr_idx, corr in enumerate(corrections):
                                if 0 <= corr_idx < len(valid_indices):
                                    chunk_affine_corrections[valid_indices[corr_idx]] = corr
                            pcb(
                                "p45_photometry_applied",
                                prog=None,
                                lvl="INFO_DETAIL",
                                group=group_id,
                                chunk=chunk_idx,
                            )
                            logger.info(
                                "[P4.5][photometry] Applied affine corrections (group=%03d chunk=%d, corrected=%d)",
                                group_id,
                                chunk_idx,
                                len([c for c in corrections if c is not None]),
                            )
                            chunk_photometry_done = True
                        else:
                            logger.debug("[P4.5][photometry] No affine corrections (identity).")
                    except Exception as exc:
                        logger.warning("[P4.5][photometry] Error while estimating/applying: %s", exc, exc_info=True)
                else:
                    logger.info("[P4.5][photometry] Skipped (insufficient valid tiles).")
            elif photometry_intragroup and normalize_method not in ("", "none"):
                logger.warning(
                    "[P4.5][photometry] Skip intra-group normalization (API unavailable)."
                )

            storage = None
            memmap_path = None
            success = True
            frames: list[np.ndarray] = []
            frame_weights: list[np.ndarray | None] = []
            channels = 3
            preloaded: dict[int, np.ndarray] = {}
            preloaded_weights: dict[int, np.ndarray | None] = {}
            try:
                first_arr, first_weights, _ = load_image_with_optional_alpha(
                    chunk_tiles[0].path,
                    tile_label=_safe_basename(chunk_tiles[0].path),
                )
                channels = int(first_arr.shape[-1]) if first_arr.ndim == 3 else 1
                if channels <= 0:
                    channels = 1
                if channels not in (1, 3):
                    channels = min(3, max(1, channels))
                    if first_arr.shape[-1] != channels:
                        first_arr = first_arr[..., :channels]
                preloaded[chunk_tiles[0].index] = first_arr.astype(np.float32, copy=False)
                preloaded_weights[chunk_tiles[0].index] = (
                    np.asarray(first_weights, dtype=np.float32) if first_weights is not None else None
                )
            except Exception:
                success = False
                channels = 3
            if not success:
                logger.debug("[P4.5][G%03d] Failed to preload reference tile for chunk %d/%d", group_id, chunk_idx, group_chunks)
                continue
            try:
                storage, memmap_path = _phase45_allocate_stack_storage(len(chunk_tiles), local_shape, channels, memmap_policy, temp_storage_dir)
                allocation_mode = "memmap" if memmap_path else "ram"
                logger.debug(
                    "[P4.5][G%03d] Allocated %s stack storage for chunk %d/%d: "
                    "tiles=%d, shape=%s, channels=%d",
                    group_id,
                    allocation_mode,
                    chunk_idx,
                    group_chunks,
                    len(chunk_tiles),
                    local_shape,
                    channels,
                )
                _phase45_gui_message(
                    f"Phase 4.5: group {group_id} chunk {chunk_idx}/{group_chunks} reprojection ({allocation_mode})"
                )
                reproject_kwargs = {"shape_out": (local_shape[0], local_shape[1])}
                if _REPROJECT_INTERP_SUPPORTS_FILL_VALUE:
                    reproject_kwargs["fill_value"] = np.nan
                if _REPROJECT_INTERP_SUPPORTS_MATCH_BG:
                    reproject_kwargs["match_background"] = True
                bytes_per_pixel = np.dtype(np.float32).itemsize

                def _estimate_reprojection_workers() -> int:
                    """Derive a bounded worker count for intra-group reprojection."""

                    max_workers = min(len(chunk_tiles), max(1, os.cpu_count() or 1))
                    plan = current_parallel_plan
                    if plan is not None:
                        try:
                            cpu_hint = int(getattr(plan, "cpu_workers", 0) or 0)
                        except Exception:
                            cpu_hint = 0
                        if cpu_hint > 0:
                            max_workers = min(max_workers, cpu_hint)
                        try:
                            chunk_bytes_hint = int(getattr(plan, "max_chunk_bytes", 0) or 0)
                        except Exception:
                            chunk_bytes_hint = 0
                        if (
                            chunk_bytes_hint > 0
                            and local_shape
                            and local_shape[0] > 0
                            and local_shape[1] > 0
                        ):
                            approx_tile_bytes = (
                                int(local_shape[0])
                                * int(local_shape[1])
                                * int(max(1, channels))
                                * bytes_per_pixel
                            )
                            approx_tile_bytes += int(local_shape[0]) * int(local_shape[1]) * bytes_per_pixel
                            if approx_tile_bytes > 0:
                                mem_cap = max(1, chunk_bytes_hint // max(approx_tile_bytes, 1))
                                max_workers = max(1, min(max_workers, mem_cap))
                    return max(1, min(max_workers, len(chunk_tiles)))

                def _reproject_chunk_tile(
                    idx_tile: int,
                    tile_obj,
                    preload_arr: np.ndarray | None,
                    preload_weight: np.ndarray | None,
                ) -> tuple[int, np.ndarray, np.ndarray | None]:
                    arr = preload_arr
                    weight_map = preload_weight
                    if arr is None:
                        arr, weight_map, _ = load_image_with_optional_alpha(
                            tile_obj.path,
                            tile_label=_safe_basename(tile_obj.path),
                        )
                    if weight_map is not None:
                        weight_map = np.asarray(weight_map, dtype=np.float32, copy=False)
                    if arr.ndim == 2:
                        arr = arr[..., np.newaxis]
                    if arr.shape[-1] != channels:
                        if channels == 3 and arr.shape[-1] == 1:
                            arr = np.repeat(arr, 3, axis=-1)
                        elif channels == 1:
                            arr = arr[..., :1]
                        else:
                            arr = np.repeat(arr[..., :1], channels, axis=-1)
                    arr = np.asarray(arr, dtype=np.float32, copy=False)
                    corr = chunk_affine_corrections[idx_tile]
                    if corr:
                        try:
                            gain = float(corr[0])
                        except Exception:
                            gain = 1.0
                        try:
                            offset = float(corr[1])
                        except Exception:
                            offset = 0.0
                        arr *= gain
                        arr += offset
                    reproj = np.full((local_shape[0], local_shape[1], channels), np.nan, dtype=np.float32)
                    weight_local = None
                    if weight_map is not None:
                        try:
                            weight_plane, weight_fp = reproject_interp(
                                (weight_map, tile_obj.wcs),
                                local_wcs,
                                **reproject_kwargs,
                            )
                            weight_plane = np.asarray(weight_plane, dtype=np.float32)
                            if weight_fp is not None:
                                mask_fp = np.asarray(weight_fp) <= 0.0
                                if mask_fp.shape == weight_plane.shape:
                                    weight_plane[mask_fp] = 0.0
                            weight_local = np.clip(np.nan_to_num(weight_plane, nan=0.0), 0.0, 1.0)
                        except Exception:
                            weight_local = None
                    for ch in range(channels):
                        plane = arr[..., ch]
                        reproj_plane, footprint = reproject_interp((plane, tile_obj.wcs), local_wcs, **reproject_kwargs)
                        if reproj_plane is None:
                            raise RuntimeError("reproject_interp returned None")
                        reproj_plane = np.asarray(reproj_plane, dtype=np.float32)
                        if footprint is not None:
                            mask = np.asarray(footprint) <= 0.0
                            if mask.shape == reproj_plane.shape:
                                reproj_plane[mask] = np.nan
                        reproj[..., ch] = reproj_plane
                    if weight_local is not None:
                        mask_zero = weight_local <= 0.0
                        if mask_zero.shape == reproj.shape[:2]:
                            reproj = np.where(mask_zero[..., None], np.nan, reproj)
                    return idx_tile, reproj, weight_local

                reproject_workers = _estimate_reprojection_workers()
                reproject_results: list[tuple[int, np.ndarray, np.ndarray | None] | None] = [None] * len(chunk_tiles)
                reproject_errors: list[str] = []

                if reproject_workers > 1:
                    with ThreadPoolExecutor(
                        max_workers=reproject_workers,
                        thread_name_prefix="ZeMosaic_P45_",
                    ) as executor:
                        futures = {}
                        for idx_tile, tile in enumerate(chunk_tiles):
                            preload_arr = preloaded.pop(tile.index, None)
                            preload_weight = preloaded_weights.pop(tile.index, None)
                            futures[executor.submit(_reproject_chunk_tile, idx_tile, tile, preload_arr, preload_weight)] = idx_tile
                        for future in as_completed(futures):
                            idx_tile = futures[future]
                            try:
                                reproject_results[idx_tile] = future.result()
                            except Exception as exc:
                                reproject_errors.append(str(exc))
                else:
                    for idx_tile, tile in enumerate(chunk_tiles):
                        preload_arr = preloaded.pop(tile.index, None)
                        preload_weight = preloaded_weights.pop(tile.index, None)
                        try:
                            reproject_results[idx_tile] = _reproject_chunk_tile(
                                idx_tile,
                                tile,
                                preload_arr,
                                preload_weight,
                            )
                        except Exception as exc:
                            reproject_errors.append(str(exc))
                            break

                if reproject_errors or not all(reproject_results):
                    success = False
                else:
                    for idx_tile, reproj, weight_local in reproject_results:  # type: ignore[misc]
                        storage[idx_tile, :, :, :] = reproj
                        frames.append(storage[idx_tile])
                        frame_weights.append(weight_local)
                if not success or not frames:
                    success = False
            except Exception:
                success = False

            if not success:
                logger.debug(
                    "[P4.5][G%03d] Chunk %d/%d aborted during reprojection",
                    group_id,
                    chunk_idx,
                    group_chunks,
                )
                _phase45_cleanup_storage(storage, memmap_path)
                continue

            alpha_weights_present = any(w is not None for w in frame_weights)
            # 4.5.a — micro-alignement résiduel (noop si indisponible)
            try:
                if micro_align_available and not alpha_weights_present:
                    logger.debug(
                        "[P4.5][G%03d] Micro-align start: method=phase, frames=%d",
                        group_id,
                        len(frames),
                    )
                    _phase45_gui_message(
                        f"Phase 4.5: group {group_id} micro-align ({len(frames)} frames)"
                    )
                    frames = zemosaic_align_stack.micro_align_stack(
                        frames, method="phase", max_shift_px=8
                    )
                    logger.debug("[P4.5][G%03d] Micro-align done", group_id)
                    _phase45_gui_message(
                        f"Phase 4.5: group {group_id} micro-align done"
                    )
                elif micro_align_available and alpha_weights_present:
                    logger.debug("[P4.5][G%03d] Micro-align skipped (alpha weights attached)", group_id)
            except Exception as exc:
                logger.debug("[P4.5][G%03d] Micro-align skipped/failed: %s", group_id, exc)

            # 4.5.b — normalisation photométrique intra-groupe (gain/offset)
            if (
                not chunk_photometry_done
                and photometry_estimator_available
                and photometry_apply_available
                and photometry_intragroup
                and norm_method not in ("", "none")
                and len(frames) >= 2
            ):
                try:
                    _phase45_gui_emit(
                        "p45_group_photometry_start",
                        level="INFO_DETAIL",
                        group_id=group_id,
                        chunk=chunk_idx,
                        size=len(chunk_tiles),
                        clip=photometry_clip_sigma,
                    )
                    logger.debug(
                        "[P4.5][G%03d] Intra-group photometry start (chunk=%d, tiles=%d, clip=%.2f)",
                        group_id,
                        chunk_idx,
                        len(chunk_tiles),
                        photometry_clip_sigma,
                    )
                    chunk_sources: list[_TileAffineSource] = []
                    for idx_tile, tile in enumerate(chunk_tiles):
                        if idx_tile >= len(frames):
                            break
                        frame_view = np.array(frames[idx_tile], dtype=np.float32, copy=False)
                        chunk_sources.append(
                            _TileAffineSource(
                                path=tile.path,
                                wcs=local_wcs,
                                data=frame_view,
                            )
                        )
                    if len(chunk_sources) >= 2:
                        affine_estimates = zemosaic_align_stack.estimate_affine_photometry(
                            chunk_sources,
                            robust=True,
                            clip_sigma=photometry_clip_sigma,
                            match_background=True,
                        )
                        corrections, nontrivial = _sanitize_affine_corrections(
                            affine_estimates,
                            len(chunk_sources),
                        )
                        if corrections and nontrivial:
                            frames = zemosaic_align_stack.apply_affine_photometry(frames, corrections)
                            log_indices = _select_affine_log_indices(corrections)
                            sample_indices = [
                                chunk_tiles[idx - 1].index
                                for idx in sorted(log_indices)
                                if 0 < idx <= len(chunk_tiles)
                            ]
                            sample_payload = ", ".join(str(val) for val in sample_indices) if sample_indices else "-"
                            _phase45_gui_emit(
                                "p45_group_photometry_applied",
                                level="INFO_DETAIL",
                                group_id=group_id,
                                chunk=chunk_idx,
                                applied=len(corrections),
                                samples=sample_payload,
                            )
                            logger.debug(
                                "[P4.5][G%03d] Intra-group photometry applied (chunk=%d, samples=%s)",
                                group_id,
                                chunk_idx,
                                sample_payload,
                            )
                        else:
                            logger.debug(
                                "[P4.5][G%03d] Intra-group photometry produced no corrections (chunk=%d)",
                                group_id,
                                chunk_idx,
                            )
                    else:
                        logger.debug(
                            "[P4.5][G%03d] Intra-group photometry skipped (insufficient tiles: %d)",
                            group_id,
                            len(chunk_sources),
                        )
                except Exception as exc:
                    logger.debug(
                        "[P4.5][G%03d] Intra-group photometry skipped due to error: %s",
                        group_id,
                        exc,
                    )

            # Héritage: respecter les réglages legacy si la nouvelle photométrie est désactivée.
            if (
                not photometry_intragroup
                and photometry_estimator_available
                and photometry_apply_available
                and norm_method != "none"
                and len(frames) >= 2
            ):
                try:
                    logger.debug(
                        "[P4.5][G%03d] Legacy inter-tile photometric normalization (method=%s, weight=%s)",
                        group_id,
                        norm_method,
                        weight_method,
                    )
                    affine_corr = zemosaic_align_stack.estimate_affine_photometry(
                        frames, method=norm_method, weight_method=weight_method
                    )
                    frames = zemosaic_align_stack.apply_affine_photometry(frames, affine_corr)
                    logger.debug("[P4.5][G%03d] Legacy inter-tile normalization applied", group_id)
                    _phase45_gui_message(
                        f"Phase 4.5: group {group_id} inter-tile photometric normalization"
                    )
                except Exception as exc:
                    logger.warning("[P4.5][G%03d] Legacy inter-tile normalization failed: %s", group_id, exc)

            logger.debug(
                "[P4.5][G%03d] Photometry: method=%s, frames=%d, chan=%d",
                group_id,
                norm_method,
                len(frames),
                channels,
            )
            try:
                clip_sigma_norm = float(photometry_clip_sigma)
            except Exception:
                clip_sigma_norm = 3.0
            if not math.isfinite(clip_sigma_norm):
                clip_sigma_norm = 3.0
            clip_sigma_norm = max(0.1, min(clip_sigma_norm, 10.0))
            sky_percent_cfg = stack_cfg.get("intertile_sky_percentile") or inter_cfg.get("intertile_sky_percentile")
            if not (
                isinstance(sky_percent_cfg, (list, tuple))
                and len(sky_percent_cfg) >= 2
            ):
                sky_low, sky_high = 30.0, 70.0
            else:
                try:
                    sky_low = float(sky_percent_cfg[0])
                except Exception:
                    sky_low = 30.0
                try:
                    sky_high = float(sky_percent_cfg[1])
                except Exception:
                    sky_high = 70.0
                if not math.isfinite(sky_low):
                    sky_low = 30.0
                if not math.isfinite(sky_high):
                    sky_high = 70.0
            if sky_low > sky_high:
                sky_low, sky_high = sky_high, sky_low
            sky_low = max(0.0, min(100.0, sky_low))
            sky_high = max(0.0, min(100.0, sky_high))
            if sky_high - sky_low < 1e-3:
                sky_low, sky_high = 30.0, 70.0

            if norm_method in ("linear_fit", "sky_mean") and len(frames) >= 2:
                try:
                    ref_arr = np.asarray(frames[0], dtype=np.float32, copy=False)
                    if ref_arr.ndim == 2:
                        ref_arr = ref_arr[..., np.newaxis]
                    ref_channels = ref_arr.shape[-1]
                    total_pixels = max(1, ref_arr.shape[0] * ref_arr.shape[1])
                    min_overlap_required = max(5000, int(math.ceil(total_pixels * 0.01)))
                    for frame_idx in range(1, len(frames)):
                        src_arr = frames[frame_idx]
                        if src_arr is None:
                            continue
                        src_arr = np.asarray(src_arr, dtype=np.float32, copy=False)
                        if src_arr.ndim == 2:
                            src_arr = src_arr[..., np.newaxis]
                        if src_arr.shape[-1] != ref_channels:
                            continue
                        if norm_method == "linear_fit":
                            for ch in range(ref_channels):
                                ref_chan = ref_arr[..., ch]
                                src_chan = src_arr[..., ch]
                                common_mask = np.isfinite(ref_chan) & np.isfinite(src_chan)
                                common_pixels = int(common_mask.sum())
                                if common_pixels < min_overlap_required:
                                    continue
                                x = src_chan[common_mask]
                                y = ref_chan[common_mask]
                                if clip_sigma_norm > 0.0 and x.size and y.size:
                                    clip_mask = np.ones(x.shape, dtype=bool)
                                    if x.size > 4:
                                        x_med = float(np.nanmedian(x))
                                        x_std = float(np.nanstd(x))
                                        if math.isfinite(x_std) and x_std > 0.0:
                                            clip_mask &= np.abs(x - x_med) <= (clip_sigma_norm * x_std)
                                    if y.size > 4:
                                        y_med = float(np.nanmedian(y))
                                        y_std = float(np.nanstd(y))
                                        if math.isfinite(y_std) and y_std > 0.0:
                                            clip_mask &= np.abs(y - y_med) <= (clip_sigma_norm * y_std)
                                    if not np.any(clip_mask):
                                        continue
                                    x = x[clip_mask]
                                    y = y[clip_mask]
                                    if x.size < max(1000, min_overlap_required // 2):
                                        continue
                                if x.size < 2:
                                    continue
                                xm = float(np.mean(x))
                                ym = float(np.mean(y))
                                xv = x - xm
                                yv = y - ym
                                denom = float(np.dot(xv, xv))
                                if denom <= 0.0 or not math.isfinite(denom):
                                    continue
                                slope = float(np.dot(xv, yv) / denom)
                                intercept = float(ym - slope * xm)
                                if not (math.isfinite(slope) and math.isfinite(intercept)):
                                    continue
                                slope = float(np.clip(slope, 0.25, 4.0))
                                src_valid = np.isfinite(src_chan)
                                if not np.any(src_valid):
                                    continue
                                src_values = src_chan[src_valid]
                                src_values *= slope
                                src_values += intercept
                                src_chan[src_valid] = src_values
                        else:  # sky_mean
                            for ch in range(ref_channels):
                                ref_chan = ref_arr[..., ch]
                                src_chan = src_arr[..., ch]
                                ref_mask = np.isfinite(ref_chan)
                                src_mask = np.isfinite(src_chan)
                                if not (np.any(ref_mask) and np.any(src_mask)):
                                    continue
                                ref_vals = ref_chan[ref_mask]
                                src_vals = src_chan[src_mask]
                                ref_range = np.nanpercentile(ref_vals, [sky_low, sky_high])
                                src_range = np.nanpercentile(src_vals, [sky_low, sky_high])
                                ref_sel = (ref_vals >= ref_range[0]) & (ref_vals <= ref_range[1])
                                src_sel = (src_vals >= src_range[0]) & (src_vals <= src_range[1])
                                ref_clip = ref_vals[ref_sel] if np.any(ref_sel) else ref_vals
                                src_clip = src_vals[src_sel] if np.any(src_sel) else src_vals
                                if ref_clip.size == 0 or src_clip.size == 0:
                                    continue
                                bg_ref = float(np.nanmedian(ref_clip))
                                bg_src = float(np.nanmedian(src_clip))
                                if not (math.isfinite(bg_ref) and math.isfinite(bg_src)):
                                    continue
                                delta = bg_ref - bg_src
                                src_chan[src_mask] = src_chan[src_mask] + delta
                except Exception:
                    logger.debug(
                        "[P4.5][G%03d] Chunk photometric normalization skipped (error)",
                        group_id,
                        exc_info=True,
                    )

            # 4.5.c — empilement selon les réglages GUI
            logger.debug(
                "[P4.5][G%03d] Stack params: reject=%s, combine=%s, kappa=%.2f, winsor_limits=%s, workers=%d, weight=%s",
                group_id,
                reject_algo,
                final_combine,
                kappa_val,
                limits_val,
                stack_kwargs["winsor_max_workers"],
                weight_method,
            )
            _phase45_gui_message(
                f"Phase 4.5: group {group_id} stacking ({reject_algo}/{final_combine})"
            )
            alpha_out = None
            weights_ready = any(isinstance(w, np.ndarray) for w in frame_weights)
            super_arr = None
            if weights_ready:
                try:
                    frames_np = np.stack(frames, axis=0).astype(np.float32, copy=False)
                    reference_shape = frames_np.shape[1:3]
                    weight_stack_list: list[np.ndarray] = []
                    for wmap in frame_weights:
                        if isinstance(wmap, np.ndarray) and wmap.shape == reference_shape:
                            weight_stack_list.append(wmap.astype(np.float32, copy=False))
                        else:
                            weight_stack_list.append(np.ones(reference_shape, dtype=np.float32))
                    weight_stack = np.stack(weight_stack_list, axis=0)
                    weight_stack = np.clip(np.nan_to_num(weight_stack, nan=0.0), 0.0, 1.0)
                    weight_expanded = weight_stack[..., None]
                    num = np.nansum(frames_np * weight_expanded, axis=0)
                    den = np.nansum(weight_expanded, axis=0)
                    super_arr = np.where(den > 0, num / den, np.nan)
                    alpha_out = (np.nanmax(weight_stack, axis=0) * 255.0).astype(np.uint8)
                    logger.debug("[P4.5][G%03d] Applied alpha-weighted stacking", group_id)
                except Exception as exc:
                    alpha_out = None
                    super_arr = None
                    logger.debug("[P4.5][G%03d] Alpha-weighted stack failed: %s", group_id, exc)
            if super_arr is None:
                try:
                    if reject_algo in ("winsor", "winsorized_sigma_clip"):
                        result = zemosaic_align_stack.stack_winsorized_sigma_clip(
                            frames,
                            weight_method=weight_method,
                            zconfig=None,
                            parallel_plan=current_parallel_plan,
                            **stack_kwargs,
                        )
                        super_arr = result[0] if isinstance(result, (tuple, list)) else result
                    elif reject_algo == "kappa_sigma":
                        stack_result = None
                        if hasattr(zemosaic_align_stack, "stack_kappa_sigma"):
                            stack_result = zemosaic_align_stack.stack_kappa_sigma(
                                frames,
                                kappa=float(stack_cfg.get("kappa_low", 3.0)),
                                combine=final_combine,
                                weight_method=weight_method,
                            )
                        elif hasattr(zemosaic_align_stack, "stack_kappa_sigma_clip"):
                            stack_result = zemosaic_align_stack.stack_kappa_sigma_clip(
                                frames,
                                weight_method=weight_method,
                                zconfig=None,
                                sigma_low=float(stack_cfg.get("kappa_low", 3.0)),
                                sigma_high=float(stack_cfg.get("kappa_high", stack_cfg.get("kappa_low", 3.0))),
                                parallel_plan=current_parallel_plan,
                            )
                        if stack_result is not None:
                            super_arr = stack_result[0] if isinstance(stack_result, (tuple, list)) else stack_result
                    elif reject_algo == "linear_fit_clip" and hasattr(zemosaic_align_stack, "stack_linear_fit_clip"):
                        result = zemosaic_align_stack.stack_linear_fit_clip(
                            frames,
                            weight_method=weight_method,
                            zconfig=None,
                            sigma=float(stack_cfg.get("kappa_high", stack_cfg.get("kappa_low", 3.0))),
                            parallel_plan=current_parallel_plan,
                        )
                        super_arr = result[0] if isinstance(result, (tuple, list)) else result

                    if super_arr is None:
                        stack_np = np.stack(frames, axis=0).astype(np.float32, copy=False)
                        super_arr = (
                            np.nanmedian(stack_np, axis=0).astype(np.float32)
                            if final_combine == "median"
                            else np.nanmean(stack_np, axis=0).astype(np.float32)
                        )
                except Exception as exc:
                    logger.debug("[P4.5][G%03d] Stack failed: %s", group_id, exc)
                    _phase45_cleanup_storage(storage, memmap_path)
                    continue

            _phase45_cleanup_storage(storage, memmap_path)

            if super_arr is None:
                logger.debug(
                    "[P4.5][G%03d] Stack yielded no data; skipping chunk %d/%d",
                    group_id,
                    chunk_idx,
                    group_chunks,
                )
                continue

            try:
                super_arr = _ensure_hwc_master_tile(
                    super_arr,
                    tile_label=f"p45_group_{group_id:03d}_chunk_{chunk_idx}",
                )
            except Exception as exc:
                logger.debug(
                    "[P4.5][G%03d] Super array normalization failed: %s",
                    group_id,
                    exc,
                )
                continue
            if channels == 1 and super_arr.shape[-1] != 1:
                super_arr = super_arr[..., :1]
            elif channels == 3:
                if super_arr.shape[-1] == 1:
                    super_arr = np.repeat(super_arr, 3, axis=-1)
                elif super_arr.shape[-1] > 3:
                    super_arr = super_arr[..., :3]
            super_arr = np.asarray(super_arr, dtype=np.float32, order="C")
            np.nan_to_num(super_arr, copy=False)
            pipeline_cfg = {
                "quality_crop_enabled": stack_cfg.get("quality_crop_enabled"),
                "quality_crop_band_px": stack_cfg.get("quality_crop_band_px"),
                "quality_crop_k_sigma": stack_cfg.get("quality_crop_k_sigma"),
                "quality_crop_margin_px": stack_cfg.get("quality_crop_margin_px"),
                "quality_crop_min_run": stack_cfg.get("quality_crop_min_run"),
                "altaz_cleanup_enabled": stack_cfg.get("altaz_cleanup_enabled"),
                "altaz_margin_percent": stack_cfg.get("altaz_margin_percent"),
                "altaz_decay": stack_cfg.get("altaz_decay"),
                "altaz_nanize": stack_cfg.get("altaz_nanize"),
            }
            super_arr, pipeline_alpha_mask = _apply_lecropper_pipeline(super_arr, pipeline_cfg)
            if super_arr is None:
                logger.debug("[P4.5][G%03d] Lecropper pipeline returned empty array", group_id)
                continue
            pipeline_alpha_u8 = _normalize_alpha_mask(
                pipeline_alpha_mask,
                target_hw=super_arr.shape[:2],
                opacity_threshold=ALPHA_OPACITY_THRESHOLD,
            )
            if pipeline_alpha_u8 is not None:
                alpha_out = _combine_alpha_masks(alpha_out, pipeline_alpha_u8)
            arr_shape = tuple(super_arr.shape)
            logger.debug(
                "[P4.5][G%03d] Super array ready: shape=%s, dtype=%s",
                group_id,
                arr_shape,
                super_arr.dtype,
            )
            _phase45_gui_message(
                f"Phase 4.5: group {group_id} super-tile shape {arr_shape}"
            )
            member_indices = sorted(tile.index for tile in chunk_tiles)
            representative_idx = member_indices[0]
            consumed_indices.update(tile.index for tile in chunk_tiles)

            member_signature = ",".join(str(idx) for idx in member_indices)
            digest = hashlib.sha1(member_signature.encode("utf-8")).hexdigest()[:8]
            super_tile_id = f"super:{representative_idx:04d}_{len(member_indices):02d}_{digest}"

            target_dir_path = Path(temp_storage_dir or output_folder).expanduser()
            target_dir_path.mkdir(parents=True, exist_ok=True)
            super_filename = f"super_tile_{representative_idx:03d}_{uuid.uuid4().hex[:8]}.fits"
            super_path = target_dir_path / super_filename
            logger.debug("[P4.5][G%03d] Saving super tile to %s", group_id, super_path)
            _phase45_gui_message(
                f"Phase 4.5: group {group_id} writing {super_filename} ({arr_shape})"
            )

            header = fits.Header()
            try:
                header.update(local_wcs.to_header(relax=True))
            except Exception:
                pass
            try:
                header["ZMT_WCSLV"] = (
                    "local" if local_wcs_source == "cutout" else "native",
                    "Phase 4.5 super-tile WCS frame",
                )
            except Exception:
                pass
            try:
                header["ZMT_TYPE"] = ("Super Tile", "Inter-Master merged tile")
                header["ZMT_SUPR"] = (len(chunk_tiles), "Tiles merged in Phase 4.5")
            except Exception:
                pass
            try:
                header["ZMT_SUPID"] = (super_tile_id, "Phase 4.5 super tile id")
            except Exception:
                pass
            try:
                header.add_history(f"Inter-Master merge ({len(chunk_tiles)} tiles)")
            except Exception:
                header["HISTORY"] = f"Inter-Master merge ({len(chunk_tiles)} tiles)"

            try:
                header["ALPHAEXT"] = (
                    1 if alpha_out is not None else 0,
                    "Alpha mask ext present",
                )
            except Exception:
                pass

            super_path_str = str(super_path)
            try:
                zemosaic_utils.save_fits_image(
                    image_data=super_arr,
                    output_path=super_path_str,
                    header=header,
                    overwrite=True,
                    save_as_float=True,
                    axis_order="HWC" if super_arr.ndim == 3 else None,
                    alpha_mask=alpha_out,
                )
            except Exception:
                try:
                    hdus = [fits.PrimaryHDU(super_arr.astype(np.float32), header=header)]
                    if alpha_out is not None:
                        alpha_hdu = fits.ImageHDU(alpha_out, name="ALPHA")
                        alpha_hdu.header["ALPHADSC"] = ("1=opaque(in), 0=transparent(out)", "")
                        hdus.append(alpha_hdu)
                    fits.HDUList(hdus).writeto(super_path_str, overwrite=True)
                except Exception:
                    continue

            _register_master_tile_identity(super_path_str, super_tile_id)

            replacements[representative_idx] = (super_path_str, local_wcs)
            cleanup_paths.extend(tile.path for tile in chunk_tiles if tile.path)
            group_super_counts[group_id] = group_super_counts.get(group_id, 0) + 1

            snr_gain = math.sqrt(len(chunk_tiles))
            pcb(
                "p45_group_result",
                prog=None,
                lvl="INFO_DETAIL",
                size=len(chunk_tiles),
                out=_safe_basename(super_path_str),
                snr=f"{snr_gain:.2f}",
                group_id=group_id,
                chunk=chunk_idx,
            )
            logger.debug(
                "[P4.5][G%03d] Super tile saved (%d tiles, SNR x%.2f): %s shape=%s",
                group_id,
                len(chunk_tiles),
                snr_gain,
                super_path,
                arr_shape,
            )
            _phase45_gui_message(
                f"Phase 4.5: group {group_id} super tile saved "
                f"(chunk {chunk_idx}/{group_chunks}, SNR x{snr_gain:.2f})"
            )
            group_completed_chunks = max(group_completed_chunks, chunk_idx)
            _phase45_gui_emit(
                "p45_group_progress",
                level="ETA_LEVEL",
                group_id=group_id,
                chunk=chunk_idx,
                done=group_completed_chunks,
                total=group_chunks,
                size=len(chunk_tiles),
            )

    total_super_tiles = sum(group_super_counts.values())
    if total_super_tiles:
        groups_with_super = len(group_super_counts)
        logger.info(
            "phase4_5: injected %d super-tiles from %d groups",
            total_super_tiles,
            groups_with_super,
        )
        _phase45_gui_message(
            f"phase4_5: injected {total_super_tiles} super-tiles from {groups_with_super} groups",
            level="INFO",
        )

    if cache_retention_mode != "keep":
        removed_count = 0
        for path in cleanup_paths:
            if not path or not _path_exists(path):
                continue
            try:
                os.remove(path)
                removed_count += 1
            except Exception:
                pass
        if removed_count:
            logger.debug("[P4.5] Removed %d original master tiles post-merge", removed_count)
            _phase45_gui_message(f"Phase 4.5: cleanup removed {removed_count} original tiles")

    # --- [P4.5] Inter-super photometric normalization (gain-only) ---
    try:
        candidate_super_tiles = [
            (tidx, entry)
            for tidx, entry in sorted(replacements.items())
            if entry and entry[0] and _path_exists(entry[0])
        ]
        if photometry_intersuper and len(candidate_super_tiles) >= 2:
            pcb("p45_norm_start", lvl="INFO_DETAIL", tiles=len(candidate_super_tiles))
            end_payload: dict[str, Any] = {"applied": 0}
            try:
                stats: list[tuple[int, str, np.ndarray, int]] = []
                for tidx, (tpath, _twcs) in candidate_super_tiles:
                    try:
                        with fits.open(
                            tpath,
                            memmap=True,
                            do_not_scale_image_data=True,
                        ) as hdul:
                            raw = hdul[0].data
                            if raw is None:
                                continue
                            arr = _ensure_hwc_master_tile(raw, _safe_basename(tpath))
                    except Exception as exc:
                        logger.debug("[P4.5] Norm stats failed for %s: %s", tpath, exc)
                        continue
                    channels = arr.shape[-1]
                    med = np.full(channels, np.nan, dtype=np.float32)
                    weight = 0
                    for ch in range(channels):
                        plane = arr[..., ch]
                        valid = np.isfinite(plane)
                        valid_count = int(valid.sum())
                        if not valid_count:
                            continue
                        vals = plane[valid].astype(np.float32, copy=False)
                        median_val = float(np.nanmedian(vals))
                        if valid_count > 32 and photometry_clip_sigma > 0:
                            try:
                                q1, q3 = np.nanpercentile(vals, [25.0, 75.0])
                                iqr = float(q3 - q1)
                                if iqr > 0:
                                    sigma_est = iqr / 1.349
                                    clip_width = sigma_est * photometry_clip_sigma
                                    if clip_width > 0:
                                        low = median_val - clip_width
                                        high = median_val + clip_width
                                        clip_mask = (vals >= low) & (vals <= high)
                                        if clip_mask.any():
                                            median_val = float(np.nanmedian(vals[clip_mask]))
                            except Exception:
                                pass
                        med[ch] = median_val
                        weight += valid_count
                    if weight == 0 or not np.all(np.isfinite(med)):
                        continue
                    stats.append((int(tidx), tpath, med, weight))

                valid_stats = [entry for entry in stats if entry[3] > 0 and np.all(np.isfinite(entry[2]))]
                if len(valid_stats) >= 2:
                    meds = np.vstack([entry[2] for entry in valid_stats])
                    weights = np.asarray([float(entry[3]) for entry in valid_stats], dtype=np.float64)
                    total_weight = float(weights.sum())
                    ref_mode = "median"
                    dominant_idx = int(np.argmax(weights))
                    ref_tile = int(valid_stats[dominant_idx][0])
                    ref = np.zeros(meds.shape[1], dtype=np.float32)
                    if total_weight <= 0:
                        ref_mode = "tile"
                        ref = meds[dominant_idx]
                    else:
                        for ch in range(meds.shape[1]):
                            col = meds[:, ch]
                            order = np.argsort(col)
                            sorted_vals = col[order]
                            sorted_weights = weights[order]
                            cutoff = total_weight * 0.5
                            cumsum = np.cumsum(sorted_weights)
                            idx = int(np.searchsorted(cumsum, cutoff, side="left"))
                            idx = min(idx, len(sorted_vals) - 1)
                            ref[ch] = float(sorted_vals[idx])
                    if not np.all(np.isfinite(ref)):
                        ref_mode = "tile"
                        ref = meds[dominant_idx]
                    ref = np.asarray(ref, dtype=np.float32, order="C")

                    gain_clip_cfg = (
                        inter_cfg.get("two_pass_cov_gain_clip")
                        or stack_cfg.get("two_pass_cov_gain_clip")
                    )
                    gmin, gmax = 0.85, 1.18
                    if isinstance(gain_clip_cfg, (list, tuple)) and len(gain_clip_cfg) >= 2:
                        try:
                            gmin = float(gain_clip_cfg[0])
                            gmax = float(gain_clip_cfg[1])
                        except Exception:
                            gmin, gmax = 0.85, 1.18
                    if not math.isfinite(gmin) or not math.isfinite(gmax):
                        gmin, gmax = 0.85, 1.18
                    if gmin > gmax:
                        gmin, gmax = gmax, gmin

                    preview_gains = []
                    for med in meds:
                        gains_preview = np.ones_like(ref, dtype=np.float32)
                        valid_mask = (
                            np.isfinite(med)
                            & np.isfinite(ref)
                            & (med != 0)
                            & (ref != 0)
                        )
                        same_sign = np.signbit(med) == np.signbit(ref)
                        valid_mask &= same_sign
                        gains_preview[valid_mask] = ref[valid_mask] / med[valid_mask]
                        preview_gains.append(np.clip(gains_preview, gmin, gmax))
                    preview_arr = np.vstack(preview_gains)
                    gain_summary = ",".join(
                        f"ch{idx}:{float(np.nanmedian(preview_arr[:, idx])):.3f}"
                        for idx in range(preview_arr.shape[1])
                    )
                    pcb(
                        "p45_norm_stats",
                        lvl="DEBUG",
                        ref="median" if ref_mode == "median" else f"tile_{ref_tile}",
                        target=[float(val) for val in ref.tolist()],
                        gains_summary=gain_summary,
                    )

                    applied_tiles = 0
                    ref_descriptor = "weighted" if ref_mode == "median" else f"tile:{ref_tile}"
                    for tidx, tpath, med, _ in valid_stats:
                        gains = np.ones_like(ref, dtype=np.float32)
                        valid_mask = (
                            np.isfinite(med)
                            & np.isfinite(ref)
                            & (med != 0)
                            & (ref != 0)
                        )
                        same_sign = np.signbit(med) == np.signbit(ref)
                        valid_mask &= same_sign
                        gains[valid_mask] = ref[valid_mask] / med[valid_mask]
                        gains = np.clip(gains, gmin, gmax)
                        if np.all(np.abs(gains - 1.0) <= 1e-6):
                            continue
                        if not tpath or not _path_exists(tpath):
                            continue
                        try:
                            with fits.open(
                                tpath,
                                mode="update",
                                memmap=False,
                                do_not_scale_image_data=True,
                            ) as hdul:
                                raw = hdul[0].data
                                if raw is None:
                                    continue
                                arr = _ensure_hwc_master_tile(raw, _safe_basename(tpath))
                                arr_corr = np.asarray(arr, dtype=np.float32, order="C")
                                for ch in range(min(arr_corr.shape[-1], gains.size)):
                                    arr_corr[..., ch] *= float(gains[ch])
                                if raw.ndim == 2:
                                    arr_store = arr_corr[..., 0]
                                elif (
                                    raw.ndim == 3
                                    and raw.shape[0] in (1, 3)
                                    and raw.shape[-1] not in (1, 3)
                                ):
                                    arr_store = np.moveaxis(arr_corr, -1, 0)
                                elif raw.ndim == 3 and raw.shape[-1] in (1, 3):
                                    arr_store = arr_corr[..., : raw.shape[-1]]
                                else:
                                    arr_store = arr_corr
                                arr_store = np.asarray(arr_store, dtype=np.float32, order="C")
                                if hdul[0].data.shape == arr_store.shape:
                                    hdul[0].data[...] = arr_store
                                else:
                                    hdul[0].data = arr_store
                                header = hdul[0].header
                                try:
                                    header["ZM45NORM"] = (True, "Phase 4.5 inter-super normalization")
                                except Exception:
                                    pass
                                history_line = (
                                    "P4.5 intersuper norm gain="
                                    + ",".join(f"{val:.6f}" for val in gains.tolist())
                                    + ", offset=0.0, ref="
                                    + ref_descriptor
                                )
                                try:
                                    header.add_history(history_line)
                                except Exception:
                                    try:
                                        header["HISTORY"] = history_line
                                    except Exception:
                                        pass
                                hdul.flush()
                            applied_tiles += 1
                            pcb("p45_norm_apply", lvl="DEBUG", tile=int(tidx), gains=gains.tolist())
                        except Exception as exc:
                            logger.debug("[P4.5] Norm apply failed for %s: %s", tpath, exc)
                    end_payload["applied"] = applied_tiles
                else:
                    logger.debug(
                        "[P4.5] Inter-super normalization skipped: insufficient stats (%d)",
                        len(valid_stats),
                    )
                    end_payload["reason"] = "insufficient_stats"
            finally:
                pcb("p45_norm_end", lvl="INFO_DETAIL", **end_payload)
    except Exception as exc:
        logger.debug("[P4.5] Inter-super normalization skipped due to error: %s", exc)

    # ---- Inter-supertiles photometric normalization (optional) ----
    if (
        photometry_estimator_available
        and photometry_apply_available
        and photometry_intersuper
    ):
        try:
            calib_entries: list[_TileAffineSource] = []
            for idx, original in enumerate(master_tiles):
                entry: tuple[str | None, Any] | None = None
                if idx in replacements:
                    entry = replacements[idx]
                elif idx not in consumed_indices:
                    entry = original
                if not entry:
                    continue
                path, wcs_obj = entry
                if not path or wcs_obj is None:
                    continue
                calib_entries.append(_TileAffineSource(path=path, wcs=wcs_obj))
            if len(calib_entries) >= 2:
                _phase45_gui_emit(
                    "p45_global_photometry_start",
                    level="INFO_DETAIL",
                    size=len(calib_entries),
                    clip=photometry_clip_sigma,
                )
                logger.debug(
                    "[P4.5] Global photometry start: entries=%d, clip=%.2f",
                    len(calib_entries),
                    photometry_clip_sigma,
                )
                global_affine = zemosaic_align_stack.estimate_affine_photometry(
                    calib_entries,
                    robust=True,
                    clip_sigma=photometry_clip_sigma,
                    match_background=True,
                )
                corrections, nontrivial = _sanitize_affine_corrections(
                    global_affine,
                    len(calib_entries),
                )
                if corrections and nontrivial:
                    nontrivial_count = sum(
                        1
                        for gain_val, offset_val in corrections
                        if abs(gain_val - 1.0) > 1e-6 or abs(offset_val) > 1e-6
                    )
                    for entry_idx, src in enumerate(calib_entries):
                        path = src.path
                        if not path or not _path_exists(path):
                            continue
                        gain_val, offset_val = corrections[entry_idx]
                        if abs(gain_val - 1.0) <= 1e-6 and abs(offset_val) <= 1e-6:
                            continue
                        try:
                            with fits.open(
                                path,
                                mode="update",
                                memmap=True,
                                do_not_scale_image_data=True,
                            ) as hdul:
                                raw_data = hdul[0].data
                                if raw_data is None:
                                    continue
                                arr_hwc = _ensure_hwc_master_tile(
                                    raw_data,
                                    _safe_basename(path),
                                )
                                arr_corr = (
                                    arr_hwc * np.float32(gain_val) + np.float32(offset_val)
                                ).astype(np.float32, copy=False)
                                if raw_data.ndim == 2:
                                    arr_store = arr_corr[..., 0]
                                elif (
                                    raw_data.ndim == 3
                                    and raw_data.shape[0] in (1, 3)
                                    and raw_data.shape[-1] not in (1, 3)
                                ):
                                    arr_store = np.moveaxis(arr_corr, -1, 0)
                                elif raw_data.ndim == 3 and raw_data.shape[-1] in (1, 3):
                                    arr_store = arr_corr[..., : raw_data.shape[-1]]
                                else:
                                    arr_store = arr_corr
                                arr_store = np.asarray(arr_store, dtype=np.float32, order="C")
                                if hdul[0].data.shape == arr_store.shape:
                                    hdul[0].data[...] = arr_store
                                else:
                                    hdul[0].data = arr_store
                                header = hdul[0].header
                                try:
                                    header.add_history("Phase 4.5 inter-super photometric match applied")
                                except Exception:
                                    try:
                                        header["HISTORY"] = "Phase 4.5 inter-super photometric match applied"
                                    except Exception:
                                        pass
                                hdul.flush()
                        except Exception as exc:
                            logger.debug(
                                "[P4.5] Global photometry apply failed for %s: %s",
                                _safe_basename(path),
                                exc,
                            )
                    _phase45_gui_emit(
                        "p45_global_photometry_done",
                        level="INFO_DETAIL",
                        nontrivial=nontrivial_count,
                        size=len(corrections),
                    )
                    logger.debug(
                        "[P4.5] Global photometry applied to %d/%d entries",
                        nontrivial_count,
                        len(corrections),
                    )
                else:
                    _phase45_gui_emit(
                        "p45_global_photometry_done",
                        level="INFO_DETAIL",
                        nontrivial=0,
                        size=len(calib_entries),
                    )
                    logger.debug("[P4.5] Global photometry produced no corrections")
            else:
                logger.debug(
                    "[P4.5] Global photometry skipped (insufficient entries: %d)",
                    len(calib_entries),
                )
        except Exception:
            logger.debug("[P4.5] Inter-super photometry skipped (error).", exc_info=True)

    new_master_tiles: list[tuple[str | None, Any]] = []
    for idx, original in enumerate(master_tiles):
        if idx in replacements:
            new_master_tiles.append(replacements[idx])
        elif idx not in consumed_indices:
            new_master_tiles.append(original)

    logger.debug(
        "[P4.5] Finished Inter-Master merge: tiles_in=%d, tiles_out=%d, replacements=%d",
        len(master_tiles),
        len(new_master_tiles),
        len(replacements),
    )
    pcb(
        "p45_finished",
        prog=None,
        lvl="INFO",
        tiles_in=len(master_tiles),
        tiles_out=len(new_master_tiles),
    )
    return new_master_tiles if new_master_tiles else master_tiles


def _select_quality_anchor(
    ordered_ids: list[int],
    distances: dict[int, float],
    seestar_groups: list[list[dict]],
    anchor_settings: dict,
    center_settings: dict,
    quality_crop_settings: dict | None,
    progress_callback: Callable | None = None,
) -> int | None:
    """Pick the best-quality anchor among the most central tiles."""

    if not ordered_ids or not seestar_groups:
        return None
    if not (
        ZEMOSAIC_UTILS_AVAILABLE
        and hasattr(zemosaic_utils, "create_downscaled_luminance_preview")
        and hasattr(zemosaic_utils, "compute_sky_statistics")
    ):
        return None

    try:
        probe_limit = int(anchor_settings.get("probe_limit", 12))
    except Exception:
        probe_limit = 12
    probe_limit = max(1, probe_limit)

    span_cfg = anchor_settings.get("span_range", (0.02, 6.0))
    if not (isinstance(span_cfg, (list, tuple)) and len(span_cfg) >= 2):
        span_cfg = (0.02, 6.0)
    try:
        span_low = float(span_cfg[0])
    except Exception:
        span_low = 0.02
    try:
        span_high = float(span_cfg[1])
    except Exception:
        span_high = 6.0
    if span_low > span_high:
        span_low, span_high = span_high, span_low

    try:
        median_clip_sigma = float(anchor_settings.get("median_clip_sigma", 2.5))
    except Exception:
        median_clip_sigma = 2.5

    try:
        preview_size = int(center_settings.get("preview_size", 256))
    except Exception:
        preview_size = 256
    sky_percent = center_settings.get("sky_percentile", (25.0, 60.0))
    if not (isinstance(sky_percent, (list, tuple)) and len(sky_percent) >= 2):
        sky_percent = (25.0, 60.0)
    try:
        sky_low = float(sky_percent[0])
    except Exception:
        sky_low = 25.0
    try:
        sky_high = float(sky_percent[1])
    except Exception:
        sky_high = 60.0

    id_to_group_index = {
        ordered_ids[idx]: idx
        for idx in range(min(len(ordered_ids), len(seestar_groups)))
    }
    candidate_ids: list[int] = []
    for tid in ordered_ids:
        if tid in id_to_group_index:
            candidate_ids.append(int(tid))
        if len(candidate_ids) >= probe_limit:
            break
    if not candidate_ids:
        return None

    _log_and_callback(
        "center_anchor_probe_start",
        lvl="INFO",
        callback=progress_callback,
        probe_count=len(candidate_ids),
    )

    candidate_entries: list[dict[str, Any]] = []
    medians: list[float] = []

    for tile_id in candidate_ids:
        stats: dict[str, float] | None = None
        group_idx = id_to_group_index.get(tile_id)
        preview_arr = None
        tile_array = None
        loaded_tile_ref = None
        if group_idx is not None and 0 <= group_idx < len(seestar_groups):
            group_info = seestar_groups[group_idx]
        else:
            group_info = None
        try:
            reference_entry = group_info[0] if group_info else None
            if reference_entry is None:
                raise ValueError("empty_group")
            wcs_obj = reference_entry.get("wcs")
            cache_path = reference_entry.get("path_preprocessed_cache")
            if cache_path and _path_exists(cache_path):
                try:
                    tile_array = np.load(cache_path, mmap_mode="r")
                except Exception:
                    tile_array = None
            if tile_array is None and reference_entry.get("preprocessed_data") is not None:
                tile_array = reference_entry.get("preprocessed_data")
            if tile_array is None and reference_entry.get("img_data_processed") is not None:
                tile_array = reference_entry.get("img_data_processed")
            if tile_array is None:
                raw_path = reference_entry.get("path_raw") or reference_entry.get("path")
                if (
                    raw_path
                    and _path_exists(raw_path)
                    and hasattr(zemosaic_utils, "load_and_validate_fits")
                ):
                    try:
                        res = zemosaic_utils.load_and_validate_fits(
                            raw_path,
                            normalize_to_float32=False,
                            attempt_fix_nonfinite=True,
                            progress_callback=None,
                        )
                        tile_array = res[0] if isinstance(res, (tuple, list)) and res else res
                    except Exception:
                        tile_array = None
            if tile_array is None:
                raise ValueError("no_cache")
            loaded_tile_ref = tile_array
            tile_array = _ensure_hwc_master_tile(tile_array, f"anchor_probe#{tile_id}")
            tile_array = _apply_preview_quality_crop(tile_array, quality_crop_settings)
            tile_array = np.asarray(tile_array, dtype=np.float32, order="C")
            preview_arr, _ = zemosaic_utils.create_downscaled_luminance_preview(
                tile_array,
                wcs_obj,
                preview_size,
            )
            stats = zemosaic_utils.compute_sky_statistics(preview_arr, sky_low, sky_high)
            if stats is not None and preview_arr is not None:
                valid = np.asarray(preview_arr, dtype=np.float64)
                valid = valid[np.isfinite(valid)]
                if valid.size > 0:
                    base_median = float(np.median(valid))
                    stats.setdefault("median", base_median)
                    span_val = float(stats.get("high", base_median) - stats.get("low", base_median))
                    stats["span"] = span_val
                    mad = float(np.median(np.abs(valid - base_median)))
                    stats["robust_sigma"] = float(1.4826 * mad) if mad > 0 else 0.0
                    medians.append(float(stats.get("median", base_median)))
                else:
                    stats.setdefault("median", 0.0)
                    stats["span"] = float(stats.get("high", 0.0) - stats.get("low", 0.0))
                    stats["robust_sigma"] = 0.0
            else:
                stats = None
        except Exception as exc:
            logger.debug("Anchor candidate %s preview failed: %s", tile_id, exc, exc_info=True)
            stats = None
        finally:
            if preview_arr is not None:
                del preview_arr
            if loaded_tile_ref is not None and isinstance(loaded_tile_ref, np.memmap):
                mmap_obj = getattr(loaded_tile_ref, "_mmap", None)
                if mmap_obj is not None:
                    try:
                        mmap_obj.close()
                    except Exception:
                        pass
            tile_array = None

        candidate_entries.append(
            {
                "tile_id": tile_id,
                "stats": stats,
                "distance": float(distances.get(tile_id, float("nan"))),
            }
        )

    if not medians:
        return None

    medians_arr = np.asarray(medians, dtype=np.float64)
    medians_arr = medians_arr[np.isfinite(medians_arr)]
    if medians_arr.size == 0:
        return None
    group_median = float(np.median(medians_arr))
    mad = float(np.median(np.abs(medians_arr - group_median)))
    deviation_clip = None
    if mad > 0 and math.isfinite(mad):
        deviation_clip = float(max(0.0, median_clip_sigma) * 1.4826 * mad)

    best_entry: dict[str, Any] | None = None
    best_score = float("inf")
    best_entry_soft: dict[str, Any] | None = None
    best_score_soft = float("inf")

    for entry in candidate_entries:
        stats = entry.get("stats")
        accepted = True
        score = float("inf")
        median_val = float("nan")
        span_val = float("nan")
        raw_score = float("inf")
        if not stats:
            accepted = False
        else:
            try:
                median_val = float(stats.get("median", float("nan")))
            except Exception:
                median_val = float("nan")
            try:
                span_val = float(stats.get("span"))
            except Exception:
                try:
                    span_val = float(stats.get("high", 0.0) - stats.get("low", 0.0))
                except Exception:
                    span_val = float("nan")
            try:
                robust_sigma_val = float(stats.get("robust_sigma", 0.0))
            except Exception:
                robust_sigma_val = 0.0
            try:
                raw_score = _score_anchor_candidate(stats, group_median, deviation_clip)
            except Exception:
                raw_score = float("inf")
            if not (math.isfinite(median_val) and math.isfinite(span_val)):
                accepted = False
            if accepted and not math.isfinite(robust_sigma_val):
                stats["robust_sigma"] = 0.0
                robust_sigma_val = 0.0
            if accepted:
                if span_val < span_low or span_val > span_high:
                    accepted = False
            if accepted and deviation_clip is not None and deviation_clip > 0:
                if abs(median_val - group_median) > deviation_clip:
                    accepted = False
            if accepted:
                score = raw_score
                if score < best_score:
                    best_score = score
                    best_entry = entry
        if math.isfinite(raw_score) and raw_score < best_score_soft:
            best_score_soft = raw_score
            best_entry_soft = entry
        entry["accepted"] = accepted
        entry["score"] = raw_score
        _log_and_callback(
            "center_anchor_probe_candidate",
            lvl="DEBUG_DETAIL",
            callback=progress_callback,
            tile=int(entry["tile_id"]),
            dist_deg=entry.get("distance"),
            median=median_val,
            span=span_val,
            score=score,
            accepted="accepted" if accepted else "rejected",
        )

    if best_entry is None:
        if best_entry_soft is not None:
            best_entry = best_entry_soft
            best_score = best_score_soft
            _log_and_callback(
                "center_anchor_soft_fallback",
                lvl="WARN",
                callback=progress_callback,
            )
        else:
            return None

    _log_and_callback(
        "center_anchor_selected",
        lvl="INFO",
        callback=progress_callback,
        tile=int(best_entry["tile_id"]),
        dist_deg=best_entry.get("distance"),
        score=best_score,
    )

    return int(best_entry["tile_id"])


def _compute_intertile_affine_corrections_from_sources(
    sources: list[_TileAffineSource],
    final_output_wcs,
    final_output_shape_hw: tuple[int, int],
    preview_size: int,
    min_overlap_fraction: float,
    sky_percentile: tuple[float, float] | list[float],
    robust_clip_sigma: float,
    use_auto_intertile: bool,
    logger_obj=None,
    progress_callback: Callable | None = None,
    intertile_global_recenter: bool = False,
    intertile_recenter_clip: tuple[float, float] | list[float] | None = None,
) -> tuple[list[tuple[float, float]] | None, bool, str, str | None]:
    """Common implementation for intertile gain/offset computation.

    Returns
    -------
    (list[(gain, offset)] | None, bool, str, str | None)
        Sanitized affine list (or ``None``), flag indicating whether non-trivial
        corrections were detected, status string (``\"ok\"``, ``\"skipped\"``,
        ``\"preview_failed\"``, ``\"compute_failed\"``) and optional error message.
    """

    total_tiles = len(sources)
    if total_tiles < 2:
        return None, False, "skipped", None

    if not (
        ZEMOSAIC_UTILS_AVAILABLE
        and hasattr(zemosaic_utils, "compute_intertile_affine_calibration")
    ):
        return None, False, "skipped", None

    tile_pairs: list[tuple[np.ndarray, Any]] = []
    preview_arrays: list[np.ndarray | None] = []

    for idx, src in enumerate(sources, 1):
        try:
            tile_arr: np.ndarray
            label = _safe_basename(src.path)
            if src.data is not None:
                tile_arr = _ensure_hwc_master_tile(src.data, label)
            else:
                if not src.path:
                    raise ValueError("Tile data missing and no path provided.")
                with fits.open(src.path, memmap=False) as hdul:
                    tile_arr = _ensure_hwc_master_tile(hdul[0].data, label)
            tile_arr = np.asarray(tile_arr, dtype=np.float32, order="C")
            tile_pairs.append((tile_arr, src.wcs))
            preview_entry = None
            if intertile_global_recenter:
                try:
                    preview_entry, _ = zemosaic_utils.create_downscaled_luminance_preview(
                        tile_arr,
                        src.wcs,
                        preview_size,
                    )
                    if preview_entry is not None:
                        preview_entry = np.asarray(preview_entry, dtype=np.float32)
                except Exception:
                    preview_entry = None
            preview_arrays.append(preview_entry)
        except Exception as exc:
            if logger_obj:
                logger_obj.warning(
                    "Intertile data load failed for %s: %s",
                    src.path or f"tile#{idx}",
                    exc,
                )
                logger_obj.debug("Traceback (intertile data load):", exc_info=True)
            tile_pairs.clear()
            preview_arrays.clear()
            return None, False, "preview_failed", str(exc)
        finally:
            if progress_callback:
                try:
                    progress_callback("phase5_intertile", idx, total_tiles)
                except Exception:
                    pass

    # [ETA Bridge] Wrapper pour intercepter "[Intertile] Using: ... pairs=K"
    def _intertile_progress_bridge(message_or_stage, current, level, **kwargs):
        try:
            # Seed ETA dès que l'on connaît le nombre de paires et la taille de preview
            if isinstance(message_or_stage, str) and message_or_stage.startswith("[Intertile] Using:"):
                # Ex: "[Intertile] Using: preview=512, min_overlap=0.0500, sky=(30.0,70.0), clip=2.50, pairs=353"
                try:
                    parts = message_or_stage.split("Using:", 1)[1]
                    # petite extraction clé=valeur robuste
                    kv = {}
                    for p in parts.split(","):
                        if "=" in p:
                            k, v = p.split("=", 1)
                            kv[k.strip()] = v.strip().strip(",")
                    preview = int(kv.get("preview", "512"))
                    pairs = int(kv.get("pairs", "0"))
                    # Heuristique : coût ~ C * pairs * (preview/512)^2
                    # GPU/CPU : facteur 1.0 (GPU) / 3.0 (CPU) basique (peut être affiné)
                    gpu_factor = 1.0 if bool(getattr(zconfig, "use_gpu_phase5", False)) else 3.0
                    base_cost_per_pair = 0.020  # 20 ms/pair @ preview=512 sur GPU (à ajuster)
                    seconds = gpu_factor * base_cost_per_pair * pairs * (preview / 512.0) ** 2
                    h, rem = divmod(int(seconds), 3600)
                    m, s = divmod(rem, 60)
                    # Pousser une ETA initiale vers le GUI (il se recadrera ensuite avec les ticks "pairs")
                    _log_and_callback(f"ETA_UPDATE:{h:02d}:{m:02d}:{s:02d}", None, "ETA_LEVEL", callback=progress_callback)
                except Exception:
                    pass
        except Exception:
            pass
        # Relais vers le callback GUI d'origine pour l'affichage + ETA fine (via on_worker_progress)
        if progress_callback:
            try:
                progress_callback(message_or_stage, current, level, **kwargs)
            except Exception:
                pass

    try:
        corrections = zemosaic_utils.compute_intertile_affine_calibration(

            tile_pairs,
            final_output_wcs,
            final_output_shape_hw,
            preview_size=preview_size,
            min_overlap_fraction=min_overlap_fraction,
            sky_percentile=sky_percentile,
            robust_clip_sigma=robust_clip_sigma,
            use_auto_intertile=use_auto_intertile,
            logger=logger_obj,
            progress_callback=_intertile_progress_bridge,
        )
    except Exception as exc:
        if logger_obj:
            logger_obj.warning(
                "Intertile photometric calibration failed: %s",
                exc,
            )
            logger_obj.debug("Traceback (intertile failure):", exc_info=True)
        return None, False, "compute_failed", str(exc)
    finally:
        tile_pairs.clear()

    sanitized, nontrivial = _sanitize_affine_corrections(corrections, total_tiles)

    if sanitized and intertile_global_recenter and preview_arrays:
        clip_cfg = intertile_recenter_clip if isinstance(intertile_recenter_clip, (list, tuple)) else None
        if not clip_cfg or len(clip_cfg) < 2:
            clip_cfg = (0.85, 1.18)
        try:
            clip_low = float(clip_cfg[0])
        except Exception:
            clip_low = 0.85
        try:
            clip_high = float(clip_cfg[1])
        except Exception:
            clip_high = 1.18
        if clip_low > clip_high:
            clip_low, clip_high = clip_high, clip_low

        try:
            medians_before: list[float] = []
            for preview_entry, affine in zip(preview_arrays, sanitized):
                if preview_entry is None:
                    medians_before.append(float("nan"))
                    continue
                gain_val, offset_val = affine
                try:
                    corrected = preview_entry * float(gain_val) + float(offset_val)
                    med_val = float(np.nanmedian(corrected)) if corrected.size > 0 else float("nan")
                except Exception:
                    med_val = float("nan")
                medians_before.append(med_val)

            finite_medians = [m for m in medians_before if math.isfinite(m) and m > 0]
            if finite_medians:
                target = float(np.median(finite_medians))
                _log_and_callback(
                    "intertile_recenter_applied",
                    lvl="INFO",
                    callback=progress_callback,
                    target=target,
                    clip_low=clip_low,
                    clip_high=clip_high,
                )
                for idx, med_val in enumerate(medians_before):
                    if not (math.isfinite(med_val) and med_val > 0):
                        continue
                    gain_adj = target / med_val if med_val != 0 else 1.0
                    if not math.isfinite(gain_adj) or gain_adj <= 0:
                        continue
                    if gain_adj < clip_low:
                        gain_adj = clip_low
                    elif gain_adj > clip_high:
                        gain_adj = clip_high
                    gain_val, offset_val = sanitized[idx]
                    sanitized[idx] = (float(gain_val) * gain_adj, float(offset_val) * gain_adj)
                    _log_and_callback(
                        "intertile_recenter_adjust",
                        lvl="INFO_DETAIL",
                        callback=progress_callback,
                        tile=int(idx),
                        median_before=med_val,
                        gain_adj=gain_adj,
                    )
        except Exception as exc:
            if logger_obj:
                logger_obj.warning("Intertile global recenter failed: %s", exc)
                logger_obj.debug("Traceback (intertile recenter):", exc_info=True)
        finally:
            preview_arrays.clear()
    elif intertile_global_recenter and preview_arrays:
        preview_arrays.clear()

    return sanitized, nontrivial, "ok", None

def cluster_seestar_stacks_connected(
    all_raw_files_with_info: list,
    stack_threshold_deg: float,
    progress_callback: callable,
    orientation_split_threshold_deg: float = 0.0,
):
    """Order-invariant clustering of Seestar raws using spherical proximity.

    Builds a proximity graph (edges when separation < threshold) and returns
    connected components. Deterministic across runs when input ordering is
    stable (we sort file paths earlier).
    """
    # Deps imported later in module; they will be available at runtime
    try:
        ok_astropy = ASTROPY_AVAILABLE and (SkyCoord is not None) and (u is not None) and (Angle is not None)
    except NameError:
        ok_astropy = False
    if not ok_astropy:
        _log_and_callback("clusterstacks_error_astropy_unavailable", level="ERROR", callback=progress_callback)
        return []
    if not all_raw_files_with_info:
        _log_and_callback("clusterstacks_warn_no_raw_info", level="WARN", callback=progress_callback)
        return []
    _log_and_callback(
        "clusterstacks_info_start",
        num_files=len(all_raw_files_with_info),
        threshold=stack_threshold_deg,
        level="INFO",
        callback=progress_callback,
    )
    panel_centers_sky = []
    panel_data_for_clustering = []
    panel_orientations_deg = []  # orientation of +X pixel axis on sky, in degrees [0,360)
    for info in all_raw_files_with_info:
        wcs_obj = info.get("wcs")
        if not (wcs_obj and getattr(wcs_obj, "is_celestial", False)):
            continue
        try:
            if getattr(wcs_obj, "pixel_shape", None):
                cx = wcs_obj.pixel_shape[0] / 2.0
                cy = wcs_obj.pixel_shape[1] / 2.0
                center_world = wcs_obj.pixel_to_world(cx, cy)
            elif hasattr(wcs_obj, "wcs") and hasattr(wcs_obj.wcs, "crval"):
                center_world = SkyCoord(
                    ra=float(wcs_obj.wcs.crval[0]) * u.deg,
                    dec=float(wcs_obj.wcs.crval[1]) * u.deg,
                    frame="icrs",
                )
            else:
                continue
            panel_centers_sky.append(center_world)
            panel_data_for_clustering.append(info)
            # Optionally compute orientation of X pixel axis using WCS
            if orientation_split_threshold_deg and float(orientation_split_threshold_deg) > 0:
                try:
                    # Use center pixel + one-pixel step in +X to get position angle
                    if getattr(wcs_obj, "pixel_shape", None):
                        cx = wcs_obj.pixel_shape[0] / 2.0
                        cy = wcs_obj.pixel_shape[1] / 2.0
                    else:
                        cx, cy = 0.0, 0.0
                    c0 = wcs_obj.pixel_to_world(cx, cy)
                    c1 = wcs_obj.pixel_to_world(cx + 1.0, cy)
                    pa = c0.position_angle(c1).to(u.deg).value  # east of north
                    ang = float(pa) % 360.0
                    panel_orientations_deg.append(ang)
                except Exception:
                    panel_orientations_deg.append(None)
            else:
                panel_orientations_deg.append(None)
        except Exception:
            continue
    if not panel_centers_sky:
        _log_and_callback("clusterstacks_warn_no_centers", level="WARN", callback=progress_callback)
        return []
    coords = SkyCoord(
        ra=[c.ra for c in panel_centers_sky],
        dec=[c.dec for c in panel_centers_sky],
        frame="icrs",
    )
    max_sep = Angle(float(stack_threshold_deg), unit=u.deg)
    try:
        idx1, idx2, _, _ = coords.search_around_sky(coords, max_sep)
    except Exception:
        idx1, idx2 = np.array([], dtype=int), np.array([], dtype=int)
    n = len(coords)
    parent = list(range(n))
    def find(a):
        while parent[a] != a:
            parent[a] = parent[parent[a]]
            a = parent[a]
        return a
    def union(a, b):
        ra, rb = find(a), find(b)
        if ra != rb:
            parent[rb] = ra
    def _circ_delta_deg(a: float, b: float) -> float:
        d = abs(float(a) - float(b))
        if d > 180.0:
            d = 360.0 - d
        return d

    for a, b in zip(idx1, idx2):
        ia, ib = int(a), int(b)
        if ia == ib:
            continue
        # If orientation-split is enabled, only connect when |Δangle| <= threshold
        if orientation_split_threshold_deg and float(orientation_split_threshold_deg) > 0:
            oa = panel_orientations_deg[ia] if ia < len(panel_orientations_deg) else None
            ob = panel_orientations_deg[ib] if ib < len(panel_orientations_deg) else None
            if oa is None or ob is None:
                # Cannot compare orientations: do not connect
                continue
            if _circ_delta_deg(oa, ob) > float(orientation_split_threshold_deg):
                continue
        union(ia, ib)
    groups_indices = {}
    for i in range(n):
        r = find(i)
        groups_indices.setdefault(r, []).append(i)
    ordered_roots = sorted(groups_indices.keys(), key=lambda r: min(groups_indices[r]))
    groups = []
    for r in ordered_roots:
        members = groups_indices[r]
        members.sort()
        groups.append([panel_data_for_clustering[i] for i in members])
    _log_and_callback("clusterstacks_info_finished", num_groups=len(groups), level="INFO", callback=progress_callback)
    return groups


# --- Phase 3 center-out helpers -------------------------------------------------


@dataclass
class _CenterPreviewEntry:
    tile_id: int
    preview: "np.ndarray | None"
    wcs: object | None
    stats: dict | None
    mode: str
    gain: float
    offset: float


class CenterOutNormalizationContext:
    def __init__(
        self,
        anchor_tile_original_id: int,
        ordered_tile_ids: list[int],
        tile_distances: dict[int, float] | None,
        settings: dict,
        global_center=None,
        logger_instance=None,
    ):
        self.anchor_original_id = int(anchor_tile_original_id)
        self.ordered_tile_ids = list(ordered_tile_ids)
        self._rank_map = {tid: idx for idx, tid in enumerate(self.ordered_tile_ids)}
        self.tile_distances = dict(tile_distances or {})
        self.settings = settings or {}
        self.global_center = global_center
        self.logger = logger_instance or logger
        self._lock = threading.RLock()
        self._entries: dict[int, _CenterPreviewEntry] = {}
        self.anchor_stats: dict | None = None
        self.anchor_ready_event = threading.Event()

    def get_rank(self, tile_id: int) -> int | None:
        return self._rank_map.get(int(tile_id))

    def get_distance(self, tile_id: int) -> float | None:
        return self.tile_distances.get(int(tile_id))

    def wait_for_anchor(self) -> bool:
        if self.anchor_ready_event.is_set():
            return True
        return self.anchor_ready_event.wait(timeout=60.0)

    def register_tile(
        self,
        tile_id: int,
        preview: "np.ndarray | None",
        preview_wcs,
        stats: dict | None,
        gain: float,
        offset: float,
        mode: str,
    ) -> None:
        entry = _CenterPreviewEntry(
            tile_id=int(tile_id),
            preview=None if preview is None else np.asarray(preview, dtype=np.float32, copy=True),
            wcs=preview_wcs,
            stats=stats.copy() if isinstance(stats, dict) else stats,
            mode=str(mode),
            gain=float(gain),
            offset=float(offset),
        )
        with self._lock:
            self._entries[int(tile_id)] = entry
            if int(tile_id) == self.anchor_original_id:
                self.anchor_stats = entry.stats
                self.anchor_ready_event.set()

    def get_processed_tiles(self, exclude_tile_id: int | None = None) -> list[_CenterPreviewEntry]:
        with self._lock:
            items = [
                entry
                for tid, entry in self._entries.items()
                if exclude_tile_id is None or int(tid) != int(exclude_tile_id)
            ]
        items.sort(key=lambda ent: (self.get_rank(ent.tile_id) if self.get_rank(ent.tile_id) is not None else 1_000_000))
        return items

    def get_anchor_stats(self) -> dict | None:
        with self._lock:
            return self.anchor_stats.copy() if isinstance(self.anchor_stats, dict) else self.anchor_stats


def _extract_group_center_skycoord(group_info_list: list[dict]) -> "SkyCoord | None":
    if not group_info_list:
        return None
    if not (ASTROPY_AVAILABLE and SkyCoord and u):
        return None
    for entry in group_info_list:
        wcs_obj = entry.get("wcs")
        if wcs_obj and getattr(wcs_obj, "is_celestial", False):
            try:
                if getattr(wcs_obj, "pixel_shape", None):
                    width = float(wcs_obj.pixel_shape[0])
                    height = float(wcs_obj.pixel_shape[1])
                else:
                    shape = entry.get("preprocessed_shape")
                    if shape and len(shape) >= 2:
                        height = float(shape[0])
                        width = float(shape[1])
                    else:
                        height = width = 0.0
                cx = width / 2.0
                cy = height / 2.0
                center_world = wcs_obj.pixel_to_world(cx, cy)
                if isinstance(center_world, SkyCoord):
                    return center_world
            except Exception:
                pass
            try:
                if hasattr(wcs_obj, "wcs") and wcs_obj.wcs and wcs_obj.wcs.crval is not None:
                    ra = float(wcs_obj.wcs.crval[0])
                    dec = float(wcs_obj.wcs.crval[1])
                    return SkyCoord(ra=ra * u.deg, dec=dec * u.deg, frame="icrs")
            except Exception:
                pass
    for entry in group_info_list:
        header = entry.get("header")
        if header is None:
            continue
        try:
            if hasattr(header, "get"):
                ra_val = header.get("CRVAL1")
                dec_val = header.get("CRVAL2")
            else:
                ra_val = header["CRVAL1"]
                dec_val = header["CRVAL2"]
            if ra_val is None or dec_val is None:
                continue
            return SkyCoord(ra=float(ra_val) * u.deg, dec=float(dec_val) * u.deg, frame="icrs")
        except Exception:
            continue
    return None


def _compute_center_out_order(
    seestar_stack_groups: list[list[dict]],
) -> tuple[list[int], "SkyCoord", dict[int, float]] | None:
    if not seestar_stack_groups:
        return None
    if not (ASTROPY_AVAILABLE and SkyCoord and u):
        return None
    centers = []
    fallback = []
    for idx, group in enumerate(seestar_stack_groups):
        center = _extract_group_center_skycoord(group)
        if center:
            centers.append((idx, center))
        else:
            fallback.append(idx)
    if not centers:
        return None
    arr = np.array([coord.cartesian.xyz.value for _, coord in centers], dtype=np.float64)
    norm = np.linalg.norm(arr, axis=1)
    norm[norm == 0] = 1.0
    arr = arr / norm[:, None]
    vec = arr.mean(axis=0)
    if np.linalg.norm(vec) <= 0:
        return None
    vec_norm = vec / np.linalg.norm(vec)
    spherical = SkyCoord(
        x=vec_norm[0] * u.one,
        y=vec_norm[1] * u.one,
        z=vec_norm[2] * u.one,
        frame="icrs",
        representation_type="cartesian",
    ).spherical
    global_center = SkyCoord(ra=spherical.lon.to(u.deg), dec=spherical.lat.to(u.deg), frame="icrs")
    distances: dict[int, float] = {}
    for idx, coord in centers:
        try:
            distances[idx] = float(coord.separation(global_center).deg)
        except Exception:
            distances[idx] = float("nan")
    ordered = sorted(
        [idx for idx, _ in centers],
        key=lambda tid: (distances.get(tid, float("inf")), tid),
    )
    for idx in fallback:
        if idx not in ordered:
            ordered.append(idx)
    return ordered, global_center, distances


def _compute_master_quality_metrics(
    master_arr: np.ndarray,
    sky_low_pct: float = 25.0,
    sky_high_pct: float = 60.0,
) -> dict[str, float]:
    """Estimate robust quality statistics for a master tile array.

    Parameters
    ----------
    master_arr : np.ndarray
        Master tile data in ``H x W x C`` or ``H x W`` layout.
    sky_low_pct : float, optional
        Lower percentile for sky span estimation.
    sky_high_pct : float, optional
        Upper percentile for sky span estimation.

    Returns
    -------
    dict[str, float]
        Dictionary containing ``median``, ``span``, ``robust_sigma``,
        ``grad_proxy`` and aggregated ``score`` (lower is better).
    """

    if master_arr is None:
        return {
            "median": 0.0,
            "span": 0.0,
            "robust_sigma": 0.0,
            "grad_proxy": 0.0,
            "score": float("inf"),
        }

    lum = master_arr if master_arr.ndim == 2 else np.mean(master_arr, axis=2, dtype=np.float32)
    lum = np.asarray(lum, dtype=np.float32)

    if lum.size == 0:
        return {
            "median": 0.0,
            "span": 0.0,
            "robust_sigma": 0.0,
            "grad_proxy": 0.0,
            "score": float("inf"),
        }

    finite_mask = np.isfinite(lum)
    valid = lum[finite_mask]
    if valid.size == 0:
        median_val = 0.0
        span_val = 0.0
        robust_sigma = 0.0
        lum_clean = np.zeros_like(lum, dtype=np.float32)
    else:
        median_val = float(np.median(valid))
        try:
            lo_val = float(np.percentile(valid, float(sky_low_pct)))
        except Exception:
            lo_val = float(np.percentile(valid, 25.0))
        try:
            hi_val = float(np.percentile(valid, float(sky_high_pct)))
        except Exception:
            hi_val = float(np.percentile(valid, 60.0))
        span_val = max(hi_val - lo_val, 0.0)
        abs_dev = np.abs(valid - median_val)
        robust_sigma = float(1.4826 * np.median(abs_dev)) if abs_dev.size else 0.0
        lum_clean = np.array(lum, copy=True)
        lum_clean[~finite_mask] = median_val

    if not np.isfinite(median_val):
        median_val = 0.0
    if not np.isfinite(span_val):
        span_val = 0.0
    if not np.isfinite(robust_sigma):
        robust_sigma = 0.0

    try:
        from scipy.ndimage import gaussian_filter  # type: ignore

        blurred = gaussian_filter(lum_clean, sigma=3.0, mode="nearest")
    except Exception:
        sigma = 3.0
        radius = max(1, int(round(sigma * 3.0)))
        coords = np.arange(-radius, radius + 1, dtype=np.float32)
        kernel = np.exp(-0.5 * (coords ** 2) / float(sigma ** 2))
        kernel_sum = float(kernel.sum())
        if kernel_sum > 0:
            kernel /= kernel_sum
        blurred = np.apply_along_axis(lambda m: np.convolve(m, kernel, mode="same"), axis=0, arr=lum_clean)
        blurred = np.apply_along_axis(lambda m: np.convolve(m, kernel, mode="same"), axis=1, arr=blurred)

    diff = lum_clean - blurred
    grad_proxy = float(np.nanstd(diff)) if diff.size else 0.0
    if not np.isfinite(grad_proxy):
        grad_proxy = 0.0

    eps = 1e-6
    score = (span_val + eps) * 0.7 + robust_sigma * 0.25 + grad_proxy * 0.05

    return {
        "median": float(median_val),
        "span": float(span_val),
        "robust_sigma": float(robust_sigma),
        "grad_proxy": float(grad_proxy),
        "score": float(score),
    }


def _compute_tile_distance_map(
    master_tiles: dict[int, tuple[str | None, Any]]
) -> dict[int, float]:
    """Estimate angular distances of master tiles from the average sky center."""

    if not master_tiles:
        return {}
    if not (ASTROPY_AVAILABLE and SkyCoord and u):
        return {}

    centers: list[tuple[int, "SkyCoord"]] = []
    for tile_id, (_path, wcs_obj) in master_tiles.items():
        if not (wcs_obj and getattr(wcs_obj, "is_celestial", False)):
            continue
        try:
            if getattr(wcs_obj, "pixel_shape", None):
                width = float(wcs_obj.pixel_shape[0])
                height = float(wcs_obj.pixel_shape[1])
            else:
                width = float(getattr(wcs_obj.wcs, "naxis1", 0))
                height = float(getattr(wcs_obj.wcs, "naxis2", 0))
            if width <= 0 or height <= 0:
                continue
            cx = width / 2.0
            cy = height / 2.0
            center_coord = wcs_obj.pixel_to_world(cx, cy)
            if isinstance(center_coord, SkyCoord):
                centers.append((int(tile_id), center_coord))
        except Exception:
            continue

    if not centers:
        return {}

    arr = np.array([coord.cartesian.xyz.value for _, coord in centers], dtype=np.float64)
    norm = np.linalg.norm(arr, axis=1)
    norm[norm == 0] = 1.0
    arr = arr / norm[:, None]
    vec = arr.mean(axis=0)
    if np.linalg.norm(vec) <= 0:
        return {}
    vec_norm = vec / np.linalg.norm(vec)
    global_center = SkyCoord(
        x=vec_norm[0] * u.one,
        y=vec_norm[1] * u.one,
        z=vec_norm[2] * u.one,
        frame="icrs",
        representation_type="cartesian",
    ).spherical
    center_sc = SkyCoord(ra=global_center.lon.to(u.deg), dec=global_center.lat.to(u.deg), frame="icrs")

    distances: dict[int, float] = {}
    for tile_id, coord in centers:
        try:
            distances[int(tile_id)] = float(coord.separation(center_sc).deg)
        except Exception:
            continue
    return distances


def apply_center_out_normalization_p3(
    tile_array: "np.ndarray",
    tile_wcs,
    tile_id: int,
    context: CenterOutNormalizationContext | None,
    settings: dict | None,
    log_func: Callable | None = None,
) -> tuple["np.ndarray", tuple[float, float] | None, str, dict]:
    if tile_array is None or context is None or not settings or not settings.get("enabled", True):
        return tile_array, None, "disabled", {}

    preview_size = int(settings.get("preview_size", 256))
    sky_percent = settings.get("sky_percentile", (25.0, 60.0))
    if not (isinstance(sky_percent, (tuple, list)) and len(sky_percent) >= 2):
        sky_percent = (25.0, 60.0)
    sky_low, sky_high = float(sky_percent[0]), float(sky_percent[1])
    clip_sigma = float(settings.get("clip_sigma", 2.5))
    min_overlap = float(settings.get("min_overlap_fraction", 0.03))

    preview_raw, preview_wcs = zemosaic_utils.create_downscaled_luminance_preview(
        tile_array,
        tile_wcs,
        preview_size,
    )
    tile_stats_raw = zemosaic_utils.compute_sky_statistics(preview_raw, sky_low, sky_high)

    if int(tile_id) == context.anchor_original_id:
        context.register_tile(tile_id, preview_raw, preview_wcs, tile_stats_raw, 1.0, 0.0, "anchor")
        details = {
            "rank": context.get_rank(tile_id),
            "distance": context.get_distance(tile_id),
            "mode": "anchor",
            "samples": int(preview_raw.size if preview_raw is not None else 0),
        }
        return tile_array, (1.0, 0.0), "anchor", details

    context.wait_for_anchor()
    best_gain_offset: tuple[float, float] | None = None
    best_samples = 0
    best_reference = None

    if preview_raw is not None and preview_wcs is not None and reproject_interp:
        for entry in context.get_processed_tiles(exclude_tile_id=tile_id):
            if entry.preview is None or entry.wcs is None:
                continue
            try:
                reproj_src, footprint = reproject_interp(
                    (preview_raw, preview_wcs),
                    entry.wcs,
                    shape_out=entry.preview.shape,
                )
            except Exception:
                continue
            if reproj_src is None or footprint is None:
                continue
            valid = np.isfinite(reproj_src) & np.isfinite(entry.preview) & (footprint > 0.1)
            if not np.any(valid):
                continue
            overlap_fraction = valid.sum() / max(1, entry.preview.size)
            if overlap_fraction < min_overlap:
                continue
            fit = zemosaic_utils.estimate_sky_affine_to_ref(
                reproj_src[valid],
                entry.preview[valid],
                sky_low,
                sky_high,
                clip_sigma,
            )
            if not fit:
                continue
            gain, offset, samples = fit
            if not np.isfinite(gain) or not np.isfinite(offset):
                continue
            if samples > best_samples:
                best_samples = samples
                best_gain_offset = (gain, offset)
                best_reference = entry.tile_id

    mode = "anchor_fallback"
    if best_gain_offset is None:
        anchor_stats = context.get_anchor_stats()
        gain = 1.0
        offset = 0.0
        if anchor_stats and tile_stats_raw:
            anchor_span = float(anchor_stats.get("high", 0.0)) - float(anchor_stats.get("low", 0.0))
            tile_span = float(tile_stats_raw.get("high", 0.0)) - float(tile_stats_raw.get("low", 0.0))
            if np.isfinite(anchor_span) and np.isfinite(tile_span) and tile_span > 1e-6:
                gain = anchor_span / tile_span
            anchor_med = float(anchor_stats.get("median", 0.0))
            tile_med = float(tile_stats_raw.get("median", 0.0))
            offset = anchor_med - gain * tile_med
        best_gain_offset = (gain, offset)
        best_samples = int(tile_stats_raw.get("median", 0) if tile_stats_raw else 0)
    else:
        mode = "overlap"

    gain, offset = best_gain_offset
    if not np.isfinite(gain) or not np.isfinite(offset):
        return tile_array, None, "invalid", {}

    try:
        np.multiply(tile_array, gain, out=tile_array, casting="unsafe")
        np.add(tile_array, offset, out=tile_array, casting="unsafe")
    except Exception:
        tile_array = tile_array * gain + offset

    preview_corrected = None
    if preview_raw is not None:
        preview_corrected = preview_raw * gain + offset
    corrected_stats = zemosaic_utils.compute_sky_statistics(preview_corrected, sky_low, sky_high)
    context.register_tile(tile_id, preview_corrected, preview_wcs, corrected_stats, gain, offset, mode)

    details = {
        "rank": context.get_rank(tile_id),
        "distance": context.get_distance(tile_id),
        "mode": mode if best_reference is None else f"{mode}:{best_reference}",
        "samples": int(best_samples),
        "reference": best_reference,
        "gain": float(gain),
        "offset": float(offset),
    }
    if log_func:
        try:
            log_func(
                "center_out_debug",
                prog=None,
                lvl="DEBUG_DETAIL",
                tile=int(tile_id),
                rank=details.get("rank"),
                distance=f"{details.get('distance'):.4f}" if isinstance(details.get("distance"), float) else None,
                mode=details.get("mode"),
                gain=f"{gain:.6f}",
                offset=f"{offset:.6f}",
                samples=int(best_samples),
            )
        except Exception:
            pass
    return tile_array, (gain, offset), mode, details


def run_poststack_anchor_review(
    master_tiles: dict[int, tuple[str | None, Any]],
    prestack_anchor_id: int | None,
    cfg: dict | SimpleNamespace | None,
    progress_callback: Callable | None = None,
    tile_distances: dict[int, float] | None = None,
) -> tuple[dict[int, tuple[str | None, Any]], tuple[float, float]]:
    """Evaluate master tiles post-stack and optionally re-anchor globally."""

    anchor_shift = (1.0, 0.0)
    if not master_tiles:
        return master_tiles, anchor_shift

    def _cfg_get(key: str, default: Any) -> Any:
        if isinstance(cfg, dict):
            return cfg.get(key, default)
        if isinstance(cfg, SimpleNamespace) and hasattr(cfg, key):
            return getattr(cfg, key)
        return default

    try:
        probe_limit = int(_cfg_get("probe_limit", _cfg_get("poststack_anchor_probe_limit", 8)))
    except Exception:
        probe_limit = 8
    probe_limit = max(1, probe_limit)

    span_cfg = _cfg_get("span_range", _cfg_get("poststack_anchor_span_range", (0.004, 10.0)))
    if not (isinstance(span_cfg, (list, tuple)) and len(span_cfg) >= 2):
        span_cfg = (0.004, 10.0)
    try:
        span_low = float(span_cfg[0])
    except Exception:
        span_low = 0.004
    try:
        span_high = float(span_cfg[1])
    except Exception:
        span_high = 10.0
    if span_low > span_high:
        span_low, span_high = span_high, span_low

    try:
        median_clip_sigma = float(_cfg_get("median_clip_sigma", _cfg_get("poststack_anchor_median_clip_sigma", 3.5)))
    except Exception:
        median_clip_sigma = 3.5

    try:
        min_improvement = float(_cfg_get("min_improvement", _cfg_get("poststack_anchor_min_improvement", 0.12)))
    except Exception:
        min_improvement = 0.12
    if not math.isfinite(min_improvement) or min_improvement < 0:
        min_improvement = 0.0

    use_overlap_affine = bool(_cfg_get("use_overlap_affine", _cfg_get("poststack_anchor_use_overlap_affine", True)))

    sky_percent_cfg = _cfg_get("sky_percentile", _cfg_get("p3_center_sky_percentile", (25.0, 60.0)))
    if not (isinstance(sky_percent_cfg, (list, tuple)) and len(sky_percent_cfg) >= 2):
        sky_percent_cfg = (25.0, 60.0)
    try:
        sky_low_pct = float(sky_percent_cfg[0])
    except Exception:
        sky_low_pct = 25.0
    try:
        sky_high_pct = float(sky_percent_cfg[1])
    except Exception:
        sky_high_pct = 60.0
    if sky_low_pct > sky_high_pct:
        sky_low_pct, sky_high_pct = sky_high_pct, sky_low_pct

    _log_and_callback("post_anchor_start", lvl="INFO", callback=progress_callback)

    distances_map: dict[int, float] = {}
    if isinstance(tile_distances, dict):
        for key, value in tile_distances.items():
            try:
                distances_map[int(key)] = float(value)
            except Exception:
                continue
    computed_distances = _compute_tile_distance_map(master_tiles)
    for key, value in computed_distances.items():
        if key not in distances_map and math.isfinite(value):
            distances_map[key] = float(value)

    available_ids = list(master_tiles.keys())
    if not available_ids:
        return master_tiles, anchor_shift

    def _distance_key(tile_id: int) -> tuple[float, int]:
        dist = distances_map.get(int(tile_id), float("inf"))
        return (float(dist), int(tile_id))

    ordered_ids = sorted(available_ids, key=_distance_key)
    candidate_ids: list[int] = ordered_ids[:probe_limit]
    if prestack_anchor_id is not None and prestack_anchor_id in master_tiles and prestack_anchor_id not in candidate_ids:
        candidate_ids.append(int(prestack_anchor_id))

    candidates: list[dict[str, Any]] = []
    medians: list[float] = []

    for tile_id in candidate_ids:
        path, wcs_obj = master_tiles.get(int(tile_id), (None, None))
        arr: np.ndarray | None = None
        metrics: dict[str, float] | None = None
        try:
            if path and _path_exists(path):
                with fits.open(path, memmap=False, do_not_scale_image_data=True) as hdul_mt:
                    arr = _ensure_hwc_master_tile(hdul_mt[0].data, _safe_basename(path))
            if arr is not None:
                arr = np.asarray(arr, dtype=np.float32)
                metrics = _compute_master_quality_metrics(arr, sky_low_pct=sky_low_pct, sky_high_pct=sky_high_pct)
                if metrics and math.isfinite(metrics.get("median", float("nan"))):
                    medians.append(float(metrics.get("median", 0.0)))
        except Exception:
            arr = None
            metrics = None

        candidates.append(
            {
                "tile_id": int(tile_id),
                "path": path,
                "wcs": wcs_obj,
                "metrics": metrics,
                "array": arr,
                "distance": distances_map.get(int(tile_id)),
            }
        )

    if not medians:
        for entry in candidates:
            entry.pop("array", None)
        return master_tiles, anchor_shift

    medians_arr = np.asarray(medians, dtype=np.float64)
    medians_arr = medians_arr[np.isfinite(medians_arr)]
    if medians_arr.size == 0:
        for entry in candidates:
            entry.pop("array", None)
        return master_tiles, anchor_shift

    group_median = float(np.median(medians_arr))
    mad = float(np.median(np.abs(medians_arr - group_median)))
    deviation_clip = None
    if mad > 0 and math.isfinite(mad):
        deviation_clip = float(max(0.0, median_clip_sigma) * 1.4826 * mad)

    best_entry: dict[str, Any] | None = None
    best_score = float("inf")
    best_entry_soft: dict[str, Any] | None = None
    best_score_soft = float("inf")

    for entry in candidates:
        metrics = entry.get("metrics") or {}
        median_val = float(metrics.get("median", float("nan")))
        span_val = float(metrics.get("span", float("nan")))
        robust_val = float(metrics.get("robust_sigma", 0.0))
        grad_val = float(metrics.get("grad_proxy", 0.0))
        score_val = float(metrics.get("score", float("inf")))
        accepted = metrics is not None and math.isfinite(median_val) and math.isfinite(span_val)
        if accepted and (span_val < span_low or span_val > span_high):
            accepted = False
        if accepted and deviation_clip is not None and deviation_clip > 0:
            if abs(median_val - group_median) > deviation_clip:
                accepted = False
        if accepted:
            if score_val < best_score:
                best_score = score_val
                best_entry = entry
        if math.isfinite(score_val) and score_val < best_score_soft:
            best_score_soft = score_val
            best_entry_soft = entry

        entry["accepted"] = accepted
        entry["score"] = score_val
        _log_and_callback(
            "post_anchor_candidate",
            lvl="DEBUG",
            callback=progress_callback,
            tile=int(entry.get("tile_id", -1)),
            median=median_val,
            span=span_val,
            robust=robust_val,
            grad=grad_val,
            score=score_val,
            accepted="accepted" if accepted else "rejected",
        )

    if best_entry is None:
        if best_entry_soft is not None:
            best_entry = best_entry_soft
            best_score = best_score_soft
            _log_and_callback("center_anchor_soft_fallback", lvl="WARN", callback=progress_callback)
        else:
            for entry in candidates:
                entry.pop("array", None)
            return master_tiles, anchor_shift

    current_anchor_entry = None
    if prestack_anchor_id is not None:
        for entry in candidates:
            if int(entry.get("tile_id", -1)) == int(prestack_anchor_id):
                current_anchor_entry = entry
                break

    best_score = float(best_score)
    if not math.isfinite(best_score):
        for entry in candidates:
            entry.pop("array", None)
        return master_tiles, anchor_shift

    anchor_score = float("inf")
    if current_anchor_entry is not None:
        anchor_score = float(current_anchor_entry.get("score", float("inf")))

    improvement = 0.0
    if math.isfinite(anchor_score) and anchor_score > 0:
        improvement = (anchor_score - best_score) / anchor_score
    elif math.isfinite(best_score):
        improvement = 1.0
    improvement = max(0.0, improvement)

    selected_tile_id = int(best_entry.get("tile_id", -1))
    MIN_MEDIAN_REL_DELTA = 0.01
    median_delta_ok = True
    if current_anchor_entry and current_anchor_entry.get("metrics") and best_entry.get("metrics"):
        old_med = float(current_anchor_entry["metrics"].get("median", 0.0))
        new_med = float(best_entry["metrics"].get("median", 0.0))
        denom = max(abs(old_med), 1e-6)
        median_delta = abs(new_med - old_med) / denom
        median_delta_ok = median_delta >= MIN_MEDIAN_REL_DELTA
    else:
        median_delta = 0.0

    if (
        selected_tile_id == int(prestack_anchor_id)
        or improvement < min_improvement
        or not median_delta_ok
    ):
        _log_and_callback(
            "post_anchor_keep_old",
            lvl="INFO",
            callback=progress_callback,
            impr=float(improvement),
        )
        for entry in candidates:
            entry.pop("array", None)
        return master_tiles, anchor_shift

    if current_anchor_entry is None or not current_anchor_entry.get("metrics"):
        _log_and_callback(
            "post_anchor_selected",
            lvl="INFO",
            callback=progress_callback,
            tile=selected_tile_id,
            impr=float(improvement),
        )
        for entry in candidates:
            entry.pop("array", None)
        return master_tiles, anchor_shift

    def _compute_overlap_shift(old_entry: dict[str, Any], new_entry: dict[str, Any]) -> tuple[float, float] | None:
        if not use_overlap_affine or reproject_interp is None:
            return None
        arr_old = old_entry.get("array")
        arr_new = new_entry.get("array")
        wcs_old = old_entry.get("wcs")
        wcs_new = new_entry.get("wcs")
        if arr_old is None or arr_new is None or wcs_old is None or wcs_new is None:
            return None
        try:
            old_lum = arr_old if arr_old.ndim == 2 else np.mean(arr_old, axis=2, dtype=np.float32)
            new_lum = arr_new if arr_new.ndim == 2 else np.mean(arr_new, axis=2, dtype=np.float32)
            old_lum = np.asarray(old_lum, dtype=np.float32)
            new_lum = np.asarray(new_lum, dtype=np.float32)
            if old_lum.size == 0 or new_lum.size == 0:
                return None
            old_med = float(np.nanmedian(old_lum[np.isfinite(old_lum)])) if np.any(np.isfinite(old_lum)) else 0.0
            new_med = float(np.nanmedian(new_lum[np.isfinite(new_lum)])) if np.any(np.isfinite(new_lum)) else 0.0
            old_clean = np.nan_to_num(old_lum, nan=old_med, posinf=old_med, neginf=old_med)
            new_clean = np.nan_to_num(new_lum, nan=new_med, posinf=new_med, neginf=new_med)
            reproj_new, footprint = reproject_interp((new_clean, wcs_new), wcs_old, shape_out=old_clean.shape)
            if footprint is None:
                return None
            mask = np.isfinite(reproj_new) & np.isfinite(old_clean) & (footprint > 0.25)
            if mask.sum() < max(256, int(old_clean.size * 0.005)):
                return None
            old_vals = old_clean[mask]
            new_vals = reproj_new[mask]
            if old_vals.size == 0 or new_vals.size == 0:
                return None
            lo_old = float(np.percentile(old_vals, sky_low_pct))
            hi_old = float(np.percentile(old_vals, sky_high_pct))
            lo_new = float(np.percentile(new_vals, sky_low_pct))
            hi_new = float(np.percentile(new_vals, sky_high_pct))
            span_old = max(hi_old - lo_old, 1e-6)
            span_new = max(hi_new - lo_new, 0.0)
            gain_val = span_new / span_old if span_old > 1e-6 else 1.0
            if not math.isfinite(gain_val) or gain_val <= 0:
                gain_val = 1.0
            med_old = float(np.median(old_vals))
            med_new = float(np.median(new_vals))
            offset_val = med_new - gain_val * med_old
            if not math.isfinite(offset_val):
                offset_val = 0.0
            return float(gain_val), float(offset_val)
        except Exception:
            return None

    def _compute_metrics_shift(old_metrics: dict[str, float], new_metrics: dict[str, float]) -> tuple[float, float]:
        span_old = float(old_metrics.get("span", 0.0))
        span_new = float(new_metrics.get("span", 0.0))
        if not math.isfinite(span_old) or span_old <= 1e-6:
            span_old = 1.0
        gain_val = span_new / span_old if span_old > 0 else 1.0
        if not math.isfinite(gain_val) or gain_val <= 0:
            gain_val = 1.0
        median_old = float(old_metrics.get("median", 0.0))
        median_new = float(new_metrics.get("median", 0.0))
        offset_val = median_new - gain_val * median_old
        if not math.isfinite(offset_val):
            offset_val = 0.0
        return float(gain_val), float(offset_val)

    gain_shift, offset_shift = (1.0, 0.0)
    overlap_shift = _compute_overlap_shift(current_anchor_entry, best_entry)
    if overlap_shift is not None:
        gain_shift, offset_shift = overlap_shift
    else:
        gain_shift, offset_shift = _compute_metrics_shift(current_anchor_entry["metrics"], best_entry["metrics"])

    anchor_shift = (float(gain_shift), float(offset_shift))

    _log_and_callback(
        "post_anchor_selected",
        lvl="INFO",
        callback=progress_callback,
        tile=selected_tile_id,
        impr=float(improvement),
    )
    _log_and_callback(
        "post_anchor_shift",
        lvl="INFO",
        callback=progress_callback,
        gain=float(gain_shift),
        offset=float(offset_shift),
    )

    for entry in candidates:
        entry.pop("array", None)

    return master_tiles, anchor_shift


# --- Helpers for RAM budget enforcement during stacking ---
def _extract_hw_from_info(raw_info: dict) -> tuple[int, int]:
    """Return (H, W) dimensions inferred from cached metadata."""

    if not isinstance(raw_info, dict):
        return 0, 0

    shape = raw_info.get("preprocessed_shape")
    if shape:
        try:
            # Accept either (H, W) or (H, W, C)
            h = int(shape[0])
            w = int(shape[1]) if len(shape) >= 2 else 0
            if h > 0 and w > 0:
                return h, w
        except Exception:
            pass

    header_obj = raw_info.get("header")
    if header_obj is not None:
        try:
            # fits.Header exposes .get, dict fallback to __getitem__
            get = header_obj.get if hasattr(header_obj, "get") else header_obj.__getitem__
            w = int(get("NAXIS1", 0)) if hasattr(header_obj, "get") else int(get("NAXIS1"))
            h = int(get("NAXIS2", 0)) if hasattr(header_obj, "get") else int(get("NAXIS2"))
            if h > 0 and w > 0:
                return h, w
        except Exception:
            pass

    wcs_obj = raw_info.get("wcs")
    if wcs_obj is not None and getattr(wcs_obj, "pixel_shape", None):
        try:
            w = int(wcs_obj.pixel_shape[0])
            h = int(wcs_obj.pixel_shape[1]) if len(wcs_obj.pixel_shape) > 1 else 0
            if h > 0 and w > 0:
                return h, w
        except Exception:
            pass

    return 0, 0


def _estimate_group_memory_bytes(group: list[dict]) -> tuple[int, int, int, int]:
    """Estimate total memory footprint (bytes) for a stack group.

    Returns ``(total_bytes, per_frame_bytes, max_h, max_w)``.
    ``per_frame_bytes`` follows the simplified model ``H * W * 4``.
    """

    if not group:
        return 0, 0, 0, 0

    max_h = 0
    max_w = 0
    for info in group:
        h, w = _extract_hw_from_info(info)
        max_h = max(max_h, int(h))
        max_w = max(max_w, int(w))

    if max_h <= 0 or max_w <= 0:
        return 0, 0, max_h, max_w

    per_frame_bytes = int(max_h) * int(max_w) * 4
    total_bytes = per_frame_bytes * len(group)
    return total_bytes, per_frame_bytes, max_h, max_w


def _split_group_temporally(group: list[dict], segment_size: int) -> list[list[dict]]:
    """Split ``group`` into contiguous segments of ``segment_size`` (>=1)."""

    if segment_size <= 0:
        return [group]
    return [group[i:i + segment_size] for i in range(0, len(group), segment_size)]


def _estimate_per_frame_cost_mb(
    header_items: list[dict] | None,
    bytes_per_pixel: int = 4,
    overhead_factor: float = 2.0,
    sample_size: int = 32,
) -> dict:
    """Estimate per-frame memory usage from Phase 0 metadata.

    Returns a dictionary containing ``per_frame_mb``, ``max_height`` and
    ``max_width`` along with the inferred ``channels``.
    """

    if not header_items:
        header_items = []

    try:
        overhead_factor = max(1.0, float(overhead_factor))
    except Exception:
        overhead_factor = 2.0

    max_h = 0
    max_w = 0
    max_channels = 0

    if header_items:
        if sample_size > 0 and len(header_items) > sample_size:
            step = max(1, len(header_items) // sample_size)
            sampled_items = [header_items[i] for i in range(0, len(header_items), step)][:sample_size]
        else:
            sampled_items = list(header_items)
    else:
        sampled_items = []

    for item in sampled_items:
        try:
            shape = item.get("shape") if isinstance(item, dict) else None
            if shape:
                h = int(shape[0]) if len(shape) >= 1 else 0
                w = int(shape[1]) if len(shape) >= 2 else 0
                c = int(shape[2]) if len(shape) >= 3 else 1
            else:
                header = item.get("header") if isinstance(item, dict) else None
                h, w = 0, 0
                c = 1
                if header is not None:
                    getter = header.get if hasattr(header, "get") else header.__getitem__
                    try:
                        w = int(getter("NAXIS1", 0)) if hasattr(header, "get") else int(getter("NAXIS1"))
                        h = int(getter("NAXIS2", 0)) if hasattr(header, "get") else int(getter("NAXIS2"))
                    except Exception:
                        h, w = 0, 0
                    try:
                        if hasattr(header, "get"):
                            naxis = int(header.get("NAXIS", 2))
                        else:
                            naxis = int(header["NAXIS"]) if "NAXIS" in header else 2
                    except Exception:
                        naxis = 2
                    if naxis >= 3:
                        try:
                            if hasattr(header, "get"):
                                c = int(header.get("NAXIS3", 1))
                            else:
                                c = int(header.get("NAXIS3", 1)) if hasattr(header, "get") else int(header["NAXIS3"])
                        except Exception:
                            c = 1
                else:
                    h, w, c = 0, 0, 1
            if isinstance(item, dict):
                if "BAYERPAT" in item.get("header", {}):
                    c = max(1, c)
            max_h = max(max_h, int(h))
            max_w = max(max_w, int(w))
            max_channels = max(max_channels, max(1, int(c)))
        except Exception:
            continue

    if max_h <= 0 or max_w <= 0:
        # Conservative fallback for unknown dimensions (~9MP mono sensor)
        max_h = 3000
        max_w = 3000
    if max_channels <= 0:
        max_channels = 1

    per_frame_bytes = max_h * max_w * max_channels * max(1, int(bytes_per_pixel))
    per_frame_mb = (per_frame_bytes / (1024 * 1024)) * overhead_factor

    return {
        "per_frame_mb": float(per_frame_mb),
        "bytes_per_pixel": int(bytes_per_pixel),
        "overhead_factor": float(overhead_factor),
        "max_height": int(max_h),
        "max_width": int(max_w),
        "channels": int(max_channels),
    }


def _probe_system_resources(
    cache_dir: str | None = None,
    *,
    two_pass_enabled: bool | None = None,
    two_pass_sigma_px: int | None = None,
    two_pass_gain_clip: tuple[float, float] | list[float] | None = None,
) -> dict:
    """Collect RAM, disk and GPU availability information."""

    info: dict = {
        "ram_total_mb": None,
        "ram_available_mb": None,
        "usable_ram_mb": None,
        "disk_total_mb": None,
        "disk_free_mb": None,
        "usable_disk_mb": None,
        "gpu_total_mb": None,
        "gpu_free_mb": None,
        "usable_vram_mb": None,
    }

    try:
        if psutil is not None:
            vm = psutil.virtual_memory()
            info["ram_total_mb"] = vm.total / (1024 * 1024)
            info["ram_available_mb"] = vm.available / (1024 * 1024)
            info["usable_ram_mb"] = min(info["ram_total_mb"], info["ram_available_mb"] * 0.6) if info["ram_available_mb"] else None
    except Exception:
        pass

    tp_enabled = bool(two_pass_enabled) if two_pass_enabled is not None else False
    tp_sigma_px = 50
    if two_pass_sigma_px is not None:
        try:
            tp_sigma_px = int(two_pass_sigma_px)
        except (TypeError, ValueError):
            tp_sigma_px = 50
    gain_clip_tuple: tuple[float, float] = (0.85, 1.18)
    if isinstance(two_pass_gain_clip, (list, tuple)) and len(two_pass_gain_clip) >= 2:
        try:
            low = float(two_pass_gain_clip[0])
            high = float(two_pass_gain_clip[1])
            if low > high:
                low, high = high, low
            gain_clip_tuple = (low, high)
        except (TypeError, ValueError):
            gain_clip_tuple = (0.85, 1.18)
    info["two_pass_enabled"] = tp_enabled
    info["two_pass_sigma_px"] = tp_sigma_px
    info["two_pass_gain_clip"] = gain_clip_tuple

    try:
        target_dir = Path(cache_dir).expanduser() if cache_dir else Path.cwd()
        if not target_dir.is_dir():
            target_dir = Path.cwd()
        du = shutil.disk_usage(str(target_dir))
        disk_total_mb = du.total / (1024 * 1024)
        disk_free_mb = du.free / (1024 * 1024)
        info["disk_total_mb"] = disk_total_mb
        info["disk_free_mb"] = disk_free_mb
        info["usable_disk_mb"] = disk_free_mb * 0.7
    except Exception:
        pass

    # GPU detection via CuPy first, then torch
    try:
        if CUPY_AVAILABLE:
            import cupy  # type: ignore

            try:
                cupy.cuda.Device().use()
                free_bytes, total_bytes = cupy.cuda.runtime.memGetInfo()
                free_mb = free_bytes / (1024 * 1024)
                total_mb = total_bytes / (1024 * 1024)
                info["gpu_total_mb"] = total_mb
                info["gpu_free_mb"] = free_mb
                info["usable_vram_mb"] = free_mb * 0.7
            except Exception:
                pass
        elif importlib.util.find_spec("torch") is not None:
            import torch  # type: ignore

            if torch.cuda.is_available():
                device = torch.cuda.current_device()
                total_mb = torch.cuda.get_device_properties(device).total_memory / (1024 * 1024)
                free_mb = torch.cuda.mem_get_info(device)[0] / (1024 * 1024)
                info["gpu_total_mb"] = total_mb
                info["gpu_free_mb"] = free_mb
                info["usable_vram_mb"] = free_mb * 0.7
    except Exception:
        pass

    return info

def _emit_gpu_info_summary(progress_callback, resource_info: dict) -> None:
    """Log a GPU summary message when VRAM information is available."""

    total_mb = resource_info.get("gpu_total_mb")
    if not total_mb:
        return
    free_mb = resource_info.get("gpu_free_mb")
    device_name = "GPU"
    if CUPY_AVAILABLE:
        try:
            import cupy  # type: ignore

            device = cupy.cuda.Device()
            device_name = getattr(device, "name", device_name) or device_name
        except Exception:
            device_name = "GPU"
    total_text = f"{float(total_mb):.0f}"
    free_text = f"{float(free_mb):.0f}" if free_mb is not None else "n/a"
    try:
        _log_and_callback(
            "gpu_info_summary",
            lvl="INFO",
            callback=progress_callback,
            name=device_name,
            total_mb=total_text,
            free_mb=free_text,
        )
    except Exception:
        pass


def _run_shared_phase45_phase5_pipeline(
    master_tiles_results_list: list[tuple[str | None, Any]],
    *,
    final_output_wcs: Any,
    final_output_shape_hw: tuple[int, int] | None,
    temp_master_tile_storage_dir: str | None,
    output_folder: str,
    cache_retention_mode: str,
    phase45_options: dict[str, Any],
    phase5_options: dict[str, Any],
    final_quality_pipeline_cfg: dict[str, Any],
    start_time_total_run: float | None,
    progress_callback: Callable | None,
    pcb: Callable[..., None],
    zconfig: Any | None = None,
    logger: logging.Logger,
) -> tuple[
    list[tuple[str | None, Any]],
    np.ndarray | None,
    np.ndarray | None,
    np.ndarray | None,
    np.ndarray | None,
    float,
]:
    """Shared helper that runs Phase 4.5 + Phase 5 on a list of tiles."""

    if zconfig is None:
        zconfig = SimpleNamespace()

    master_tiles: list[tuple[str | None, Any]] = list(master_tiles_results_list or [])
    base_progress_phase4_5 = float(phase45_options.get("base_progress") or 0.0)
    progress_weight_phase4_5 = float(phase45_options.get("progress_weight") or 0.0)
    phase45_active_flag = bool(phase45_options.get("enable"))

    if phase45_active_flag:
        pcb("PHASE_UPDATE:4.5", prog=None, lvl="ETA_LEVEL")
        _log_memory_usage(progress_callback, "Début Phase 4.5 (Inter-Master merge)")
        inter_cfg_phase45 = {
            "enable": True,
            "overlap_threshold": float(phase45_options.get("overlap_threshold", 0.60)),
            "min_group_size": int(phase45_options.get("min_group_size", 2)),
            "stack_method": str(phase45_options.get("stack_method", "winsorized_sigma_clip")).lower(),
            "memmap_policy": str(phase45_options.get("memmap_policy", "auto")).lower(),
            "local_scale": str(phase45_options.get("local_scale", "native")).lower(),
            "max_group": int(phase45_options.get("max_group_size", 64)),
        }
        worker_cfg = phase45_options.get("worker_config") or {}
        try:
            photometry_clip_cfg = float(worker_cfg.get("inter_master_photometry_clip_sigma", 3.0))
        except Exception:
            photometry_clip_cfg = 3.0
        if not math.isfinite(photometry_clip_cfg):
            photometry_clip_cfg = 3.0
        inter_cfg_phase45.update(
            {
                "photometry_intragroup": bool(worker_cfg.get("inter_master_photometry_intragroup", True)),
                "photometry_intersuper": bool(worker_cfg.get("inter_master_photometry_intersuper", True)),
                "photometry_clip_sigma": max(0.1, photometry_clip_cfg),
            }
        )
        gain_clip_cfg = worker_cfg.get("two_pass_cov_gain_clip")
        if isinstance(gain_clip_cfg, (list, tuple)) and len(gain_clip_cfg) >= 2:
            try:
                gmin = float(gain_clip_cfg[0])
                gmax = float(gain_clip_cfg[1])
                if math.isfinite(gmin) and math.isfinite(gmax):
                    if gmin > gmax:
                        gmin, gmax = gmax, gmin
                    inter_cfg_phase45["two_pass_cov_gain_clip"] = (gmin, gmax)
            except Exception:
                pass
        stack_cfg_phase45 = dict(phase45_options.get("stack_cfg") or {})
        master_tiles = _run_phase4_5_inter_master_merge(
            master_tiles,
            final_output_wcs,
            final_output_shape_hw,
            temp_master_tile_storage_dir,
            output_folder,
            cache_retention_mode,
            inter_cfg_phase45,
            stack_cfg_phase45,
            progress_callback,
            pcb,
        )
        _log_memory_usage(progress_callback, "Fin Phase 4.5")

    current_global_progress = base_progress_phase4_5 + progress_weight_phase4_5

    base_progress_phase5 = float(phase5_options.get("base_progress") or current_global_progress)
    progress_weight_phase5 = float(phase5_options.get("progress_weight") or 0.0)
    USE_INCREMENTAL_ASSEMBLY = str(phase5_options.get("final_assembly_method") or "").lower() == "incremental"
    apply_master_tile_crop_config = bool(phase5_options.get("apply_master_tile_crop"))
    quality_crop_enabled_config = bool(phase5_options.get("quality_crop_enabled"))
    apply_crop_for_assembly = bool(apply_master_tile_crop_config and not quality_crop_enabled_config)
    master_tile_crop_percent_config = float(phase5_options.get("master_tile_crop_percent") or 0.0)
    intertile_match_flag = bool(phase5_options.get("intertile_match_flag"))
    match_background_flag = bool(phase5_options.get("match_background_flag"))
    feather_parity_flag = bool(phase5_options.get("feather_parity_flag"))
    two_pass_enabled = bool(phase5_options.get("two_pass_enabled"))
    two_pass_sigma_px = int(phase5_options.get("two_pass_sigma_px") or 0)
    gain_clip_tuple = phase5_options.get("two_pass_gain_clip")
    two_pass_coverage_renorm_config = bool(phase5_options.get("two_pass_coverage_renorm"))
    use_gpu_phase5_flag = bool(phase5_options.get("use_gpu_phase5"))
    assembly_process_workers_config = int(phase5_options.get("assembly_process_workers") or 0)
    intertile_preview_size_config = phase5_options.get("intertile_preview_size") or 512
    intertile_overlap_min_config = phase5_options.get("intertile_overlap_min") or 0.05
    intertile_sky_percentile_tuple = phase5_options.get("intertile_sky_percentile") or (30.0, 70.0)
    intertile_robust_clip_sigma_config = phase5_options.get("intertile_robust_clip_sigma") or 2.5
    intertile_global_recenter_config = phase5_options.get("intertile_global_recenter")
    intertile_recenter_clip_tuple = phase5_options.get("intertile_recenter_clip") or (0.85, 1.18)
    use_auto_intertile_config = phase5_options.get("use_auto_intertile")
    coadd_use_memmap_config = bool(phase5_options.get("coadd_use_memmap"))
    coadd_memmap_dir_config = phase5_options.get("coadd_memmap_dir")
    start_time_total = start_time_total_run
    global_anchor_shift = phase5_options.get("global_anchor_shift")
    parallel_plan = phase5_options.get("parallel_plan")
    tile_weighting_enabled_flag = bool(phase5_options.get("tile_weighting_enabled"))
    tile_weight_mode = str(phase5_options.get("tile_weight_mode") or "n_frames")
    worker_config_cache = phase45_options.get("worker_config") or {}
    parallel_caps_option = phase5_options.get("parallel_capabilities")
    telemetry_ctrl = phase5_options.get("telemetry")
    sds_mode_phase5 = bool(phase5_options.get("sds_mode"))
    final_mosaic_rgb_equalize_enabled = bool(
        phase5_options.get(
            "final_mosaic_rgb_equalize_enabled",
            getattr(zconfig, "final_mosaic_rgb_equalize_enabled", False),
        )
    )
    final_mosaic_black_point_equalize_enabled = phase5_options.get(
        "final_mosaic_black_point_equalize_enabled",
        getattr(zconfig, "final_mosaic_black_point_equalize_enabled", True),
    )
    final_mosaic_black_point_equalize_enabled = _coerce_bool_flag(final_mosaic_black_point_equalize_enabled)
    if final_mosaic_black_point_equalize_enabled is None:
        final_mosaic_black_point_equalize_enabled = True
    final_mosaic_black_point_equalize_enabled = bool(final_mosaic_black_point_equalize_enabled)
    try:
        final_mosaic_black_point_percentile = float(
            phase5_options.get(
                "final_mosaic_black_point_percentile",
                getattr(zconfig, "final_mosaic_black_point_percentile", 0.1),
            )
            or 0.1
        )
    except Exception:
        final_mosaic_black_point_percentile = 0.1

    pcb("PHASE_UPDATE:5", prog=None, lvl="ETA_LEVEL")
    _log_memory_usage(
        progress_callback,
        (
            "Début Phase 5 (Méthode: "
            f"{phase5_options.get('final_assembly_method')}, "
            f"Rognage MT Appliqué: {apply_crop_for_assembly}, "
            f"QualityCrop: {quality_crop_enabled_config}, "
            f"%Rognage: {master_tile_crop_percent_config if apply_crop_for_assembly else 'N/A'})"
        ),
    )

    incremental_parity_active = (
        USE_INCREMENTAL_ASSEMBLY
        and intertile_match_flag
        and match_background_flag
        and feather_parity_flag
    )
    if incremental_parity_active and two_pass_enabled:
        two_pass_enabled = False
        pcb("run_info_incremental_two_pass_parity_disabled", prog=None, lvl="INFO_DETAIL")

    valid_master_tiles_for_assembly = []
    for mt_p, mt_w in master_tiles:
        if mt_p and _path_exists(mt_p) and mt_w and mt_w.is_celestial:
            valid_master_tiles_for_assembly.append((mt_p, mt_w))
        else:
            pcb(
                "run_warn_phase5_invalid_tile_skipped_for_assembly",
                prog=None,
                lvl="WARN",
                filename=_safe_basename(mt_p if mt_p else "N/A"),
            )

    final_mosaic_data_HWC = None
    final_mosaic_coverage_HW = None
    final_alpha_map = None
    alpha_final = None
    fallback_two_pass_loader = None
    tiles_total_phase5 = len(valid_master_tiles_for_assembly)

    if not valid_master_tiles_for_assembly:
        pcb(
            "run_error_phase5_no_valid_tiles_for_assembly",
            prog=base_progress_phase5 + progress_weight_phase5,
            lvl="ERROR",
        )
        return master_tiles, None, None, None, None, base_progress_phase5 + progress_weight_phase5

    collected_tiles_for_second_pass: list[tuple[np.ndarray, Any]] | None = (
        [] if two_pass_enabled and not USE_INCREMENTAL_ASSEMBLY else None
    )
    log_key_phase5_failed = ""
    log_key_phase5_finished = ""

    if logger.isEnabledFor(logging.DEBUG):
        try:
            sample_tile_path = valid_master_tiles_for_assembly[0][0]
            if sample_tile_path and _path_exists(sample_tile_path):
                with fits.open(sample_tile_path, memmap=True, do_not_scale_image_data=True) as hdul_sample:
                    sample_data = np.asarray(hdul_sample[0].data)
                _dbg_rgb_stats("P4_pre_merge_rgb", sample_data, logger=logger)
        except Exception:
            pass

    reproject_coadd_available = (
        "assemble_final_mosaic_reproject_coadd" in globals()
        and callable(assemble_final_mosaic_reproject_coadd)
    )
    incremental_available = (
        "assemble_final_mosaic_incremental" in globals()
        and callable(assemble_final_mosaic_incremental)
    )
    parallel_plan_phase5 = parallel_plan

    def _emit_phase5_stats(
        tiles_done: int,
        tiles_total_override: int | None = None,
        force: bool = False,
        stage: str | None = None,
    ) -> None:
        if sds_mode_phase5 or telemetry_ctrl is None:
            return
        try:
            total = tiles_total_override if tiles_total_override is not None else tiles_total_phase5
            ctx = {
                "phase_index": 5,
                "phase_name": "Phase 5: Reproject & Coadd",
                "tiles_done": int(max(0, tiles_done)),
                "tiles_total": int(max(0, total)),
                "use_gpu_phase5": bool(use_gpu_phase5_flag),
            }
            if stage:
                ctx["stage"] = stage
            plan_ref = parallel_plan_phase5
            for attr in (
                "cpu_workers",
                "rows_per_chunk",
                "gpu_rows_per_chunk",
                "max_chunk_bytes",
                "gpu_max_chunk_bytes",
                "use_gpu",
            ):
                try:
                    value = getattr(plan_ref, attr)
                except Exception:
                    value = None
                if value is None and isinstance(plan_ref, dict):
                    value = plan_ref.get(attr)
                if value is not None:
                    ctx[attr] = value
            telemetry_ctrl.emit_stats(ctx, force=force)
        except Exception:
            pass

    if USE_INCREMENTAL_ASSEMBLY:
        if not incremental_available:
            pcb("run_error_phase5_inc_func_missing", prog=None, lvl="CRITICAL")
            return master_tiles, None, None, None, None, base_progress_phase5 + progress_weight_phase5
        pcb("run_info_phase5_started_incremental", prog=base_progress_phase5, lvl="INFO")
        _emit_phase5_stats(0, tiles_total_phase5, force=True, stage="start")
        inc_memmap_dir = temp_master_tile_storage_dir or output_folder
        try:
            if use_gpu_phase5_flag:
                import cupy

                cupy.cuda.Device(0).use()
        except Exception as e_gpu:
            logger.warning("GPU incremental assembly init failed, falling back to CPU: %s", e_gpu)
            use_gpu_phase5_flag = False
        try:
            final_mosaic_data_HWC, final_mosaic_coverage_HW, final_alpha_map = assemble_final_mosaic_incremental(
                master_tile_fits_with_wcs_list=valid_master_tiles_for_assembly,
                final_output_wcs=final_output_wcs,
                final_output_shape_hw=final_output_shape_hw,
                progress_callback=progress_callback,
                n_channels=3,
                apply_crop=apply_crop_for_assembly,
                crop_percent=master_tile_crop_percent_config,
                processing_threads=assembly_process_workers_config,
                memmap_dir=inc_memmap_dir,
                cleanup_memmap=True,
                intertile_photometric_match=intertile_match_flag,
                intertile_preview_size=int(intertile_preview_size_config),
                intertile_overlap_min=float(intertile_overlap_min_config),
                intertile_sky_percentile=intertile_sky_percentile_tuple,
                intertile_robust_clip_sigma=float(intertile_robust_clip_sigma_config),
                intertile_global_recenter=bool(intertile_global_recenter_config),
                intertile_recenter_clip=intertile_recenter_clip_tuple,
                use_auto_intertile=bool(use_auto_intertile_config),
                match_background=match_background_flag,
                feather_parity=feather_parity_flag,
                two_pass_coverage_renorm=two_pass_coverage_renorm_config,
                base_progress_phase5=base_progress_phase5,
                progress_weight_phase5=progress_weight_phase5,
                start_time_total_run=start_time_total,
                global_anchor_shift=global_anchor_shift,
                stats_callback=_emit_phase5_stats,
            )
        except Exception as exc:
            logger.exception("Incremental assembly failed", exc_info=True)
            final_mosaic_data_HWC = None
        log_key_phase5_failed = "run_error_phase5_assembly_failed_incremental"
        log_key_phase5_finished = "run_info_phase5_finished_incremental"
    else:
        if not reproject_coadd_available:
            pcb("run_error_phase5_reproject_coadd_func_missing", prog=None, lvl="CRITICAL")
            return master_tiles, None, None, None, None, base_progress_phase5 + progress_weight_phase5
        pcb("run_info_phase5_started_reproject_coadd", prog=base_progress_phase5, lvl="INFO")

        if PARALLEL_HELPERS_AVAILABLE and not sds_mode_phase5:
            try:
                try:
                    frame_h = int(final_output_shape_hw[0])
                except Exception:
                    frame_h = 0
                try:
                    frame_w = int(final_output_shape_hw[1])
                except Exception:
                    frame_w = 0
                frame_shape_phase5 = (frame_h, frame_w)
                try:
                    n_frames_phase5 = max(1, len(valid_master_tiles_for_assembly))
                except Exception:
                    n_frames_phase5 = 1
                caps_for_phase5 = parallel_caps_option or worker_config_cache.get("parallel_capabilities")
                if caps_for_phase5 is None:
                    try:
                        caps_for_phase5 = detect_parallel_capabilities()
                    except Exception:
                        caps_for_phase5 = None
                parallel_plan_phase5 = auto_tune_parallel_plan(
                    kind="global_reproject",
                    frame_shape=frame_shape_phase5,
                    n_frames=n_frames_phase5,
                    bytes_per_pixel=4,
                    config=worker_config_cache,
                    caps=caps_for_phase5,
                )
                worker_config_cache["parallel_plan_phase5"] = parallel_plan_phase5
                try:
                    setattr(zconfig, "parallel_plan_phase5", parallel_plan_phase5)
                except Exception:
                    pass
            except Exception as exc_phase5_plan:
                logger.warning("Phase5 auto-tune failed, falling back to global plan: %s", exc_phase5_plan)

        _emit_phase5_stats(0, tiles_total_phase5, force=True, stage="start")
        plan_gpu_allowed = bool(getattr(parallel_plan_phase5, "use_gpu", False))
        if not use_gpu_phase5_flag:
            effective_use_gpu = False
        else:
            effective_use_gpu = bool(plan_gpu_allowed)
            if not effective_use_gpu and use_gpu_phase5_flag:
                logger.info("[Phase5] GPU disabled by auto-tune (plan.use_gpu=False)")

        try:
            if effective_use_gpu:
                import cupy

                cupy.cuda.Device(0).use()
            final_mosaic_data_HWC, final_mosaic_coverage_HW, final_alpha_map = assemble_final_mosaic_reproject_coadd(
                master_tile_fits_with_wcs_list=valid_master_tiles_for_assembly,
                final_output_wcs=final_output_wcs,
                final_output_shape_hw=final_output_shape_hw,
                progress_callback=progress_callback,
                n_channels=3,
                match_bg=True,
                apply_crop=apply_crop_for_assembly,
                crop_percent=master_tile_crop_percent_config,
                use_gpu=effective_use_gpu,
                use_memmap=bool(coadd_use_memmap_config),
                memmap_dir=(coadd_memmap_dir_config or output_folder),
                cleanup_memmap=False,
                base_progress_phase5=base_progress_phase5,
                progress_weight_phase5=progress_weight_phase5,
                start_time_total_run=start_time_total,
                intertile_photometric_match=intertile_match_flag,
                intertile_preview_size=int(intertile_preview_size_config),
                intertile_overlap_min=float(intertile_overlap_min_config),
                intertile_sky_percentile=intertile_sky_percentile_tuple,
                intertile_robust_clip_sigma=float(intertile_robust_clip_sigma_config),
                intertile_global_recenter=bool(intertile_global_recenter_config),
                intertile_recenter_clip=intertile_recenter_clip_tuple,
                use_auto_intertile=bool(use_auto_intertile_config),
                collect_tile_data=collected_tiles_for_second_pass,
                global_anchor_shift=global_anchor_shift,
                phase45_enabled=phase45_active_flag,
                parallel_plan=parallel_plan_phase5,
                enable_tile_weighting=tile_weighting_enabled_flag,
                tile_weight_mode=tile_weight_mode,
                stats_callback=_emit_phase5_stats,
            )
        except Exception as exc:
            logger.exception("Reproject+Coadd assembly failed", exc_info=True)
            final_mosaic_data_HWC = None
        log_key_phase5_failed = "run_error_phase5_assembly_failed_reproject_coadd"
        log_key_phase5_finished = "run_info_phase5_finished_reproject_coadd"

    if final_mosaic_data_HWC is None:
        pcb(
            log_key_phase5_failed or "run_error_phase5_assembly_failed_unknown",
            prog=base_progress_phase5 + progress_weight_phase5,
            lvl="ERROR",
        )
        _emit_phase5_stats(0, tiles_total_phase5, force=True, stage="failed")
        return master_tiles, None, None, None, None, base_progress_phase5 + progress_weight_phase5

    current_global_progress = base_progress_phase5 + progress_weight_phase5

    if USE_INCREMENTAL_ASSEMBLY:
        def _load_tiles_for_two_pass_phase5():
            return _load_master_tiles_for_two_pass(
                valid_master_tiles_for_assembly,
                apply_crop=apply_crop_for_assembly,
                crop_percent=master_tile_crop_percent_config,
                logger=logger,
            )

        fallback_two_pass_loader = _load_tiles_for_two_pass_phase5

    enable_final_lecropper = False
    if final_quality_pipeline_cfg:
        enable_final_lecropper = bool(
            final_quality_pipeline_cfg.get("quality_crop_enabled")
            or final_quality_pipeline_cfg.get("altaz_cleanup_enabled")
        )
    enable_final_master_crop = bool(apply_master_tile_crop_config and not quality_crop_enabled_config)

    enable_final_lecropper_for_mosaic = enable_final_lecropper
    enable_final_master_crop_for_mosaic = enable_final_master_crop
    if not sds_mode_phase5:
        enable_final_lecropper_for_mosaic = False
        enable_final_master_crop_for_mosaic = False

    if logger.isEnabledFor(logging.DEBUG):
        _dbg_rgb_stats(
            "P4_post_merge_rgb",
            final_mosaic_data_HWC,
            logger=logger,
        )

    final_mosaic_data_HWC, final_mosaic_coverage_HW, final_alpha_map = _apply_phase5_post_stack_pipeline(
        final_mosaic_data_HWC,
        final_mosaic_coverage_HW,
        final_alpha_map,
        enable_lecropper_pipeline=enable_final_lecropper_for_mosaic,
        pipeline_cfg=final_quality_pipeline_cfg,
        enable_master_tile_crop=enable_final_master_crop_for_mosaic,
        master_tile_crop_percent=master_tile_crop_percent_config,
        two_pass_enabled=bool(two_pass_enabled),
        two_pass_sigma_px=two_pass_sigma_px,
        two_pass_gain_clip=gain_clip_tuple,
        final_output_wcs=final_output_wcs,
        final_output_shape_hw=final_output_shape_hw,
        use_gpu_two_pass=use_gpu_phase5_flag,
        logger=logger,
        collected_tiles=collected_tiles_for_second_pass,
        fallback_two_pass_loader=fallback_two_pass_loader,
        parallel_plan=parallel_plan,
        telemetry_ctrl=None if sds_mode_phase5 else telemetry_ctrl,
    )

    if logger.isEnabledFor(logging.DEBUG):
        _dbg_rgb_stats(
            "P4_post_merge_valid_rgb",
            final_mosaic_data_HWC,
            coverage=final_mosaic_coverage_HW,
            alpha=final_alpha_map,
            logger=logger,
        )

    if collected_tiles_for_second_pass is not None:
        collected_tiles_for_second_pass.clear()

    final_rgb_eq_info = None

# NOTE:
# We temporarily disable final mosaic RGB equalization.
# Master tiles are already RGB-normalized individually, and
# applying the same algorithm again on the assembled mosaic
# has been shown to produce a strong green cast.
#
# if (
#     final_mosaic_rgb_equalize_enabled
#     and final_mosaic_data_HWC is not None
#     and not sds_mode_phase5
# ):
#     try:
#         final_mosaic_data_HWC, final_rgb_eq_info = _apply_final_mosaic_rgb_equalization(
#             final_mosaic_data_HWC,
#             zconfig=zconfig,
#             logger=logger,
#         )
#     except Exception as exc:
#         if logger:
#             logger.warning(
#                 "[RGB-EQ] Unexpected error during final mosaic RGB equalization: %s",
#                 exc,
#             )

    alpha_final = _derive_final_alpha_mask(
        final_alpha_map,
        final_mosaic_data_HWC,
        final_mosaic_coverage_HW,
        logger,
    )

    if logger.isEnabledFor(logging.DEBUG):
        _dbg_rgb_stats(
            "P5_pre_rgb_equalization",
            final_mosaic_data_HWC,
            coverage=final_mosaic_coverage_HW,
            alpha=alpha_final,
            logger=logger,
        )

    if (
        final_mosaic_black_point_equalize_enabled
        and final_mosaic_data_HWC is not None
        and isinstance(final_mosaic_data_HWC, np.ndarray)
        and final_mosaic_data_HWC.ndim == 3
        and final_mosaic_data_HWC.shape[-1] == 3
        and not sds_mode_phase5
    ):
        try:
            final_mosaic_data_HWC, rgb_black_eq_info = equalize_black_point_rgb(
                final_mosaic_data_HWC,
                alpha_mask=alpha_final,
                coverage_hw=final_mosaic_coverage_HW,
                percentile=final_mosaic_black_point_percentile,
                logger=logger,
            )
            if logger and isinstance(rgb_black_eq_info, dict):
                if rgb_black_eq_info.get("applied"):
                    logger.info(
                        "[BlackPoint] Applied percentile=%.3f pedestals=(%.6f, %.6f, %.6f) samples=%d",
                        rgb_black_eq_info.get("percentile", float("nan")),
                        rgb_black_eq_info.get("pedestal_r", 0.0),
                        rgb_black_eq_info.get("pedestal_g", 0.0),
                        rgb_black_eq_info.get("pedestal_b", 0.0),
                        int(rgb_black_eq_info.get("samples", 0)),
                    )
                else:
                    logger.debug(
                        "[BlackPoint] Skipped percentile=%.3f (samples=%d)",
                        rgb_black_eq_info.get("percentile", float("nan")),
                        int(rgb_black_eq_info.get("samples", 0)),
                    )
        except Exception as exc_black:
            if logger:
                logger.warning("[BlackPoint] Final mosaic pedestal equalization failed: %s", exc_black)

    if logger.isEnabledFor(logging.DEBUG):
        _dbg_rgb_stats(
            "P5_post_rgb_equalization",
            final_mosaic_data_HWC,
            coverage=final_mosaic_coverage_HW,
            alpha=alpha_final,
            logger=logger,
        )

    _emit_phase5_stats(tiles_total_phase5, tiles_total_phase5, force=True, stage="end")
    _log_memory_usage(progress_callback, "Fin Phase 5 (Assemblage)")
    pcb(
        log_key_phase5_finished or "run_info_phase5_finished",
        prog=current_global_progress,
        lvl="INFO",
        shape=final_mosaic_data_HWC.shape if final_mosaic_data_HWC is not None else "N/A",
    )

    return (
        master_tiles,
        final_mosaic_data_HWC,
        final_mosaic_coverage_HW,
        final_alpha_map,
        alpha_final,
        current_global_progress,
    )


def _compute_auto_tile_caps(
    resource_info: dict,
    per_frame_info: dict,
    policy_max: int = 0,
    policy_min: int = 8,
    disk_threshold_mb: float = 8192.0,
    user_max_override: int | None = None,
) -> dict:
    """Combine resource probes and per-frame costs into adaptive caps."""

    per_frame_mb = float(per_frame_info.get("per_frame_mb", 0.0) or 0.0)
    usable_ram_mb = float(resource_info.get("usable_ram_mb") or 0.0)
    ram_available_mb = float(resource_info.get("ram_available_mb") or 0.0)

    if user_max_override and user_max_override > 0:
        policy_max = min(policy_max, int(user_max_override))

    frames_by_ram = 0
    if per_frame_mb > 0 and usable_ram_mb > 0:
        frames_by_ram = max(0, int(math.floor(usable_ram_mb / per_frame_mb)))

    cap_candidate = policy_max if policy_max > 0 else frames_by_ram or policy_min
    if frames_by_ram > 0:
        cap_candidate = min(cap_candidate, frames_by_ram)
    cap_candidate = max(policy_min, cap_candidate)

    disk_free_mb = float(resource_info.get("disk_free_mb") or 0.0)
    usable_disk_mb = float(resource_info.get("usable_disk_mb") or 0.0)

    memmap_enabled = False
    memmap_budget_mb = None
    if frames_by_ram < policy_min and disk_free_mb > disk_threshold_mb:
        memmap_enabled = True
        memmap_budget_mb = max(policy_min * per_frame_mb, usable_disk_mb * 0.2 if usable_disk_mb else disk_free_mb * 0.2)

    gpu_hint = None
    usable_vram_mb = float(resource_info.get("usable_vram_mb") or 0.0)
    if per_frame_mb > 0 and usable_vram_mb > 0:
        gpu_hint = max(1, min(cap_candidate, int(math.floor(usable_vram_mb / per_frame_mb))))

    if memmap_enabled and (not user_max_override or user_max_override <= 0):
        cap_candidate = 0  # allow unlimited when memmap streaming is available

    parallel_cap = 1
    if frames_by_ram and cap_candidate > 0:
        parallel_cap = max(1, frames_by_ram // max(1, cap_candidate))
    if memmap_enabled:
        parallel_cap = 1

    return {
        "per_frame_mb": per_frame_mb,
        "frames_by_ram": frames_by_ram,
        "cap": int(cap_candidate),
        "min_cap": int(policy_min),
        "memmap": bool(memmap_enabled),
        "memmap_budget_mb": memmap_budget_mb,
        "gpu_batch_hint": gpu_hint,
        "ram_available_mb": ram_available_mb,
        "parallel_groups": int(parallel_cap),
    }


def _extract_timestamp(info: dict, fallback: float) -> float:
    header = info.get("header") if isinstance(info, dict) else None
    if header is not None:
        for key in ("DATE-OBS", "DATE-AVG", "DATE", "TIME-OBS"):
            try:
                if hasattr(header, "get"):
                    value = header.get(key)
                else:
                    value = header[key] if key in header else None
            except Exception:
                value = None
            if not value:
                continue
            try:
                from astropy.time import Time  # type: ignore

                return float(Time(value, format="isot", scale="utc").unix)
            except Exception:
                try:
                    dt = datetime.fromisoformat(str(value).replace("Z", "+00:00"))
                    return dt.timestamp()
                except Exception:
                    continue
    try:
        idx = info.get("phase0_index")
        if idx is not None:
            return float(idx)
    except Exception:
        pass
    return float(fallback)


def _extract_ra_dec_deg(info: dict) -> tuple[float, float] | None:
    wcs_obj = info.get("wcs") if isinstance(info, dict) else None
    if wcs_obj and getattr(wcs_obj, "is_celestial", False):
        try:
            if getattr(wcs_obj, "pixel_shape", None):
                cx = wcs_obj.pixel_shape[0] / 2.0
                cy = wcs_obj.pixel_shape[1] / 2.0
            else:
                cx = cy = 0.0
            center = wcs_obj.pixel_to_world(cx, cy)
            if hasattr(center, "ra") and hasattr(center.ra, "deg"):
                return float(center.ra.deg), float(center.dec.deg)
        except Exception:
            pass

    if isinstance(info, dict):
        phase0_center = info.get("phase0_center")
        if phase0_center is not None:
            try:
                if hasattr(phase0_center, "ra") and hasattr(phase0_center.ra, "deg"):
                    return float(phase0_center.ra.deg), float(phase0_center.dec.deg)
                if isinstance(phase0_center, (list, tuple)) and len(phase0_center) >= 2:
                    return float(phase0_center[0]), float(phase0_center[1])
            except Exception:
                pass

        header = info.get("header")
        if header is not None:
            try:
                getter = header.get if hasattr(header, "get") else header.__getitem__
                ra = getter("CRVAL1", None)
                dec = getter("CRVAL2", None)
                if ra is not None and dec is not None:
                    return float(ra), float(dec)
            except Exception:
                pass
    return None


def _estimate_frame_fov_deg(info: dict) -> float | None:
    if isinstance(info, dict):
        direct = info.get("phase0_fov_deg") or info.get("estimated_fov_deg")
        if direct:
            try:
                return float(direct)
            except Exception:
                pass
    wcs_obj = info.get("wcs") if isinstance(info, dict) else None
    if wcs_obj and getattr(wcs_obj, "is_celestial", False):
        try:
            if getattr(wcs_obj, "pixel_shape", None):
                width = float(wcs_obj.pixel_shape[0])
                height = float(wcs_obj.pixel_shape[1]) if len(wcs_obj.pixel_shape) > 1 else width
            else:
                height, width = _extract_hw_from_info(info)
            if width and height:
                xs = [0.0, width, 0.0, width]
                ys = [0.0, 0.0, height, height]
                corners = wcs_obj.pixel_to_world(xs, ys)
                if SkyCoord is not None and u is not None:
                    sc = SkyCoord(ra=corners.ra, dec=corners.dec)
                    seps = sc[:, None].separation(sc[None, :]).deg
                    return float(np.nanmax(seps)) if np.size(seps) else None
        except Exception:
            pass

    header = info.get("header") if isinstance(info, dict) else None
    if header is not None:
        try:
            getter = header.get if hasattr(header, "get") else header.__getitem__
            cd1 = abs(float(getter("CDELT1", 0)))
            cd2 = abs(float(getter("CDELT2", 0)))
            h, w = _extract_hw_from_info(info)
            if cd1 and cd2 and h and w:
                return math.hypot(cd1 * w, cd2 * h)
        except Exception:
            pass
    return None


def _unit_vector_from_ra_dec(ra_deg: float, dec_deg: float) -> tuple[float, float, float]:
    ra_rad = math.radians(float(ra_deg))
    dec_rad = math.radians(float(dec_deg))
    x = math.cos(dec_rad) * math.cos(ra_rad)
    y = math.cos(dec_rad) * math.sin(ra_rad)
    z = math.sin(dec_rad)
    return x, y, z


def _compute_max_angular_separation_deg(coords: list[tuple[float, float]]) -> float:
    if not coords or len(coords) < 2:
        return 0.0
    if SkyCoord is not None and u is not None:
        try:
            sc = SkyCoord(ra=[c[0] for c in coords] * u.deg, dec=[c[1] for c in coords] * u.deg)
            seps = sc[:, None].separation(sc[None, :]).deg
            return float(np.nanmax(seps)) if np.size(seps) else 0.0
        except Exception:
            pass
    vectors = np.array([_unit_vector_from_ra_dec(*c) for c in coords], dtype=float)
    max_sep = 0.0
    for i in range(len(vectors)):
        for j in range(i + 1, len(vectors)):
            dot = float(np.dot(vectors[i], vectors[j]))
            dot = min(1.0, max(-1.0, dot))
            sep = math.degrees(math.acos(dot))
            if sep > max_sep:
                max_sep = sep
    return max_sep


def _cluster_unit_vectors(vectors: 'np.ndarray', k: int, max_iter: int = 25) -> list[int]:
    if k <= 1 or vectors.shape[0] <= 1:
        return [0] * vectors.shape[0]
    k = min(k, vectors.shape[0])
    centers = [vectors[0]]
    for _ in range(1, k):
        distances = 1 - np.dot(vectors, np.stack(centers, axis=0).T)
        min_dist = np.min(distances, axis=1)
        idx = int(np.argmax(min_dist))
        centers.append(vectors[idx])
    centers = np.array(centers, dtype=float)

    assignments = np.zeros(vectors.shape[0], dtype=int)
    for _ in range(max_iter):
        distances = 1 - np.dot(vectors, centers.T)
        new_assignments = np.argmin(distances, axis=1)
        if np.array_equal(assignments, new_assignments):
            break
        assignments = new_assignments
        for ci in range(k):
            members = vectors[assignments == ci]
            if members.size == 0:
                # Reinitialize empty cluster to farthest point
                idx = int(np.argmax(np.min(distances, axis=1)))
                centers[ci] = vectors[idx]
            else:
                center = members.mean(axis=0)
                norm = np.linalg.norm(center)
                if norm > 0:
                    centers[ci] = center / norm
    return assignments.tolist()


def _sort_group_chronologically(group: list[dict]) -> list[dict]:
    ordered = []
    for idx, info in enumerate(group):
        ts = _extract_timestamp(info, idx)
        ordered.append((ts, idx, info))
    ordered.sort(key=lambda x: (x[0], x[1]))
    return [item[2] for item in ordered]


def _chunk_sequence(seq: list[dict], size: int) -> list[list[dict]]:
    if size <= 0:
        return [seq]
    return [seq[i:i + size] for i in range(0, len(seq), size)]


def make_overlapping_batches(indices: list[int], cap: int, overlap_frac: float) -> list[list[int]]:
    n = len(indices)
    if n <= cap:
        return [indices]
    step = max(1, int(cap * (1.0 - overlap_frac)))
    batches: list[list[int]] = []
    start = 0
    while start < n:
        end = min(n, start + cap)
        batch = indices[start:end]
        if len(batch) > 1:
            batches.append(batch)
        if end == n:
            break
        start += step
    return batches


def _sort_group_for_overlap(group: list[dict]) -> list[dict]:
    if not group:
        return []

    def _safe_number(value: Any) -> float:
        try:
            number = float(value)
        except Exception:
            return math.inf
        return number if math.isfinite(number) else math.inf

    ordered = sorted(
        ((idx, info) for idx, info in enumerate(group)),
        key=lambda item: (_safe_number(item[1].get("RA")), _safe_number(item[1].get("DEC")), item[0]),
    )
    return [info for _idx, info in ordered]


def _auto_split_single_group(
    group: list[dict],
    cap: int,
    min_cap: int,
    spatial_fraction: float = 0.25,
) -> tuple[list[list[dict]], dict]:
    n = len(group)
    detail = {
        "original_size": n,
        "segment_sizes": [n],
        "spatial_split": False,
        "reason": "within_cap" if n <= cap else "ram_cap",
        "dispersion_deg": None,
        "fov_deg": None,
    }

    if n <= max(cap, min_cap):
        return [group], detail

    centers = []
    indices = []
    for idx, info in enumerate(group):
        coord = _extract_ra_dec_deg(info)
        if coord:
            centers.append(coord)
            indices.append(idx)

    fov_deg = _estimate_frame_fov_deg(group[0]) if group else None
    dispersion_deg = _compute_max_angular_separation_deg(centers) if centers else 0.0
    detail["dispersion_deg"] = dispersion_deg
    detail["fov_deg"] = fov_deg

    base_clusters: list[list[tuple[int, dict]]] = []
    if (
        centers
        and fov_deg
        and dispersion_deg > float(fov_deg) * float(max(0.0, spatial_fraction))
    ):
        k = max(1, math.ceil(n / max(1, cap)))
        vectors = np.array([_unit_vector_from_ra_dec(*c) for c in centers], dtype=float)
        assignments = _cluster_unit_vectors(vectors, k)
        cluster_map: dict[int, list[tuple[int, dict]]] = {i: [] for i in range(k)}
        for pos, assignment in zip(indices, assignments):
            cluster_map.setdefault(int(assignment), []).append((pos, group[pos]))
        remaining_indices = [i for i in range(n) if i not in indices]
        for idx in remaining_indices:
            target = min(cluster_map.keys(), key=lambda key: (len(cluster_map[key]), key))
            cluster_map[target].append((idx, group[idx]))
        base_clusters = [sorted(items, key=lambda x: x[0]) for items in cluster_map.values() if items]
        if len(base_clusters) > 1:
            detail["spatial_split"] = True
            detail["reason"] = "dispersion"
    if not base_clusters:
        base_clusters = [list(enumerate(group))]

    output_groups: list[list[dict]] = []
    for cluster in base_clusters:
        ordered = _sort_group_chronologically([info for _idx, info in cluster])
        output_groups.extend(_chunk_sequence(ordered, max(min_cap, cap)))

    detail["segment_sizes"] = [len(sub) for sub in output_groups]
    return output_groups, detail


def _auto_split_groups(
    groups: list[list[dict]],
    cap: int,
    min_cap: int,
    progress_callback: Callable | None = None,
    spatial_fraction: float = 0.25,
) -> list[list[dict]]:
    if cap <= 0 or not groups:
        return groups
    new_groups: list[list[dict]] = []
    for idx, group in enumerate(groups, start=1):
        subgroups, detail = _auto_split_single_group(group, cap, min_cap, spatial_fraction)
        new_groups.extend(subgroups)
        if progress_callback:
            try:
                sizes_str = ",".join(str(len(sg)) for sg in subgroups)
                msg = (
                    f"AutoSplit: group #{idx} N={len(group)} -> {len(subgroups)} subgroups "
                    f"[{sizes_str}] (chrono; spatial split={'yes' if detail['spatial_split'] else 'no'}; "
                    f"reason={detail['reason']})"
                )
                _log_and_callback(msg, prog=None, lvl="INFO_DETAIL", callback=progress_callback)
            except Exception:
                pass
    return new_groups


def _group_center_deg(group):
    """Renvoie le centre RA/DEC moyen d'un groupe."""

    ras, decs = [], []
    for info in group:
        ra, dec = info.get("RA"), info.get("DEC")
        if ra is not None and dec is not None:
            ras.append(float(ra))
            decs.append(float(dec))
    if not ras:
        return None
    return (sum(ras) / len(ras), sum(decs) / len(decs))


def _angular_sep_deg(a, b):
    """Distance angulaire simple en degrés (approximation suffisante)."""

    if not a or not b:
        return 9999
    dra = abs(a[0] - b[0])
    ddec = abs(a[1] - b[1])
    return (dra**2 + ddec**2) ** 0.5


def _merge_small_groups(groups, min_size, cap):
    """
    Fusionne les petits groupes (<min_size) avec le plus proche voisin
    si le total reste <= cap (avec marge 10%).
    """

    merged_flags = [False] * len(groups)
    centers = [_group_center_deg(g) for g in groups]

    for i, gi in enumerate(groups):
        if merged_flags[i] or len(gi) >= min_size:
            continue

        best_j, best_d = None, 1e9
        for j, gj in enumerate(groups):
            if i == j or merged_flags[j]:
                continue
            d = _angular_sep_deg(centers[i], centers[j])
            if d < best_d:
                best_d, best_j = d, j

        if best_j is not None and len(groups[best_j]) + len(gi) <= int(cap * 1.1):
            groups[best_j].extend(gi)
            merged_flags[i] = True
            print(
                f"[AutoMerge] Group {i} ({len(gi)} imgs) merged into {best_j} (now {len(groups[best_j])})"
            )

    return [g for k, g in enumerate(groups) if not merged_flags[k]]


def _attempt_recluster_for_budget(
    group: list[dict],
    budget_bytes: int,
    base_threshold_deg: float,
    orientation_split_threshold_deg: float,
    cluster_func: Callable[..., list] = cluster_seestar_stacks_connected,
    max_attempts: int = 6,
) -> tuple[list[list[dict]], float, int] | None:
    """Try to relax clustering threshold until all subgroups fit the RAM budget."""

    if not group or len(group) <= 1:
        return None
    try:
        current_thr = float(base_threshold_deg)
    except Exception:
        return None
    if current_thr <= 0:
        return None

    for attempt in range(1, max_attempts + 1):
        current_thr = max(current_thr * 0.7, 1e-5)
        try:
            reclustered = cluster_func(
                group,
                float(current_thr),
                None,
                orientation_split_threshold_deg=orientation_split_threshold_deg,
            )
        except Exception:
            return None

        if not reclustered or len(reclustered) <= 1:
            continue

        fits_budget = True
        for sub in reclustered:
            total_bytes, _, _, _ = _estimate_group_memory_bytes(sub)
            if budget_bytes > 0 and total_bytes > budget_bytes:
                fits_budget = False
                break
        if fits_budget:
            return reclustered, float(current_thr), attempt

    return None


def _apply_ram_budget_to_groups(
    groups: list[list[dict]],
    budget_bytes: int,
    base_threshold_deg: float,
    orientation_split_threshold_deg: float,
    cluster_func: Callable[..., list] = cluster_seestar_stacks_connected,
) -> tuple[list[list[dict]], list[dict]]:
    """Ensure each stack group fits in the RAM budget by splitting or re-clustering."""

    if budget_bytes is None or budget_bytes <= 0:
        return groups, []

    final_groups: list[list[dict]] = []
    adjustments: list[dict] = []
    queue: list[tuple[int, list[dict]]] = [(idx + 1, grp) for idx, grp in enumerate(groups)]

    while queue:
        group_index, group = queue.pop(0)
        total_bytes, per_frame_bytes, _, _ = _estimate_group_memory_bytes(group)

        if total_bytes <= 0 or total_bytes <= budget_bytes:
            final_groups.append(group)
            continue

        if len(group) == 1:
            # Nothing else can be done; log and proceed.
            adjustments.append(
                {
                    "method": "single_over_budget",
                    "group_index": group_index,
                    "original_frames": len(group),
                    "estimated_mb": total_bytes / (1024 ** 2),
                    "budget_mb": budget_bytes / (1024 ** 2),
                }
            )
            final_groups.append(group)
            continue

        recluster_result = _attempt_recluster_for_budget(
            group,
            budget_bytes,
            base_threshold_deg,
            orientation_split_threshold_deg,
            cluster_func=cluster_func,
        )
        if recluster_result:
            reclustered_groups, new_threshold, attempts = recluster_result
            adjustments.append(
                {
                    "method": "recluster",
                    "group_index": group_index,
                    "original_frames": len(group),
                    "num_subgroups": len(reclustered_groups),
                    "new_threshold_deg": new_threshold,
                    "attempts": attempts,
                    "estimated_mb": total_bytes / (1024 ** 2),
                    "budget_mb": budget_bytes / (1024 ** 2),
                }
            )
            queue = [(group_index, sub) for sub in reclustered_groups] + queue
            continue

        if per_frame_bytes <= 0:
            # Unable to infer size; keep original group.
            final_groups.append(group)
            continue

        max_frames = max(1, int(budget_bytes // per_frame_bytes))
        if max_frames >= len(group):
            final_groups.append(group)
            continue

        segmented = _split_group_temporally(group, max_frames)
        still_over = any(_estimate_group_memory_bytes(seg)[0] > budget_bytes for seg in segmented)
        adjustments.append(
            {
                "method": "split",
                "group_index": group_index,
                "original_frames": len(group),
                "num_subgroups": len(segmented),
                "segment_size": max_frames,
                "estimated_mb": total_bytes / (1024 ** 2),
                "budget_mb": budget_bytes / (1024 ** 2),
                "still_over_budget": still_over,
            }
        )
        queue = [(group_index, seg) for seg in segmented] + queue

    return final_groups, adjustments


# --- Configuration du Logging ---
try:
    log_file_path = ensure_user_config_dir() / "zemosaic_worker.log"
except Exception:
    log_file_path = Path("zemosaic_worker.log")

logger = logging.getLogger("ZeMosaicWorker")
if not logger.handlers:
    logger.setLevel(logging.DEBUG)
    fh = logging.FileHandler(str(log_file_path), mode='w', encoding='utf-8')
    fh.setLevel(logging.DEBUG)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(module)s.%(funcName)s:%(lineno)d - %(message)s')
    fh.setFormatter(formatter)
    logger.addHandler(fh)
logger.info("Logging pour ZeMosaicWorker initialisé. Logs écrits dans: %s", log_file_path)


def _configure_worker_logging(logging_level_config: str | None, *, source_hint: str | None = None) -> None:
    """Apply logging level from config or environment and emit a confirmation line."""

    level_map = {
        "ERROR": logging.ERROR,
        "WARN": logging.WARNING,
        "WARNING": logging.WARNING,
        "INFO": logging.INFO,
        "DEBUG": logging.DEBUG,
    }

    env_level = os.environ.get("ZEMOSAIC_LOG_LEVEL")
    level_value = logging_level_config or env_level or "INFO"
    level_name = str(level_value).upper()
    lvl = level_map.get(level_name, logging.INFO)

    logger.setLevel(lvl)
    for handler in logger.handlers:
        try:
            handler.setLevel(lvl)
        except Exception:
            pass

    src = source_hint or ("env" if env_level and not logging_level_config else "qt_gui_config" if logging_level_config else "default")
    try:
        logger.info("[LOGCFG] effective_level=%s source=%s", logging.getLevelName(lvl), src)
    except Exception:
        pass

# --- Alignment Warning Tracking ---
# These warnings come from zemosaic_align_stack when an image fails to align.
# We count them here so a summary can be written at the end of a run.
ALIGN_WARNING_SUMMARY = {
    "aligngroup_warn_max_iter_error": "astroalign MaxIterError",
    "aligngroup_warn_shape_mismatch_after_align": "shape mismatch after align",
    "aligngroup_warn_register_returned_none": "astroalign returned None",
    "aligngroup_warn_value_error": "value error during align",
}
ALIGN_WARNING_COUNTS = {key: 0 for key in ALIGN_WARNING_SUMMARY}

# --- Third-Party Library Imports ---
import zarr
from packaging.version import Version

try:
    from zarr.storage import LRUStoreCache
    if Version(zarr.__version__).major >= 3:
        # In zarr>=3 LRUStoreCache was removed. Use a no-op wrapper
        raise ImportError
except Exception:  # pragma: no cover - fallback for zarr>=3 or missing cache
    class LRUStoreCache:
        """Simple pass-through wrapper used when LRUStoreCache is unavailable."""

        def __init__(self, store, max_size=None):
            self.store = store

        def __getattr__(self, name):
            return getattr(self.store, name)

try:
    # Prefer storage module first (zarr < 3)
    from zarr.storage import DirectoryStore
except Exception:
    try:  # pragma: no cover - zarr >= 3 uses LocalStore
        from zarr.storage import LocalStore as DirectoryStore
    except Exception:
        try:
            from zarr.storage import FsspecStore
            import fsspec

            def DirectoryStore(path):
                return FsspecStore(fsspec.filesystem("file").get_mapper(path))
        except Exception:  # pragma: no cover - ultimate fallback
            DirectoryStore = None

# now LRUStoreCache and DirectoryStore are defined


# --- Astropy (critique) ---
ASTROPY_AVAILABLE = False
WCS, SkyCoord, Angle, fits, u = None, None, None, None, None
try:
    from astropy.io import fits as actual_fits
    from astropy.wcs import WCS as actual_WCS
    from astropy.wcs import wcs as wcs_module
    from astropy.coordinates import SkyCoord as actual_SkyCoord, Angle as actual_Angle
    from astropy import units as actual_u
    fits, WCS, SkyCoord, Angle, u = actual_fits, actual_WCS, actual_SkyCoord, actual_Angle, actual_u
    ASTROPY_AVAILABLE = True
    logger.info("Bibliothèque Astropy importée.")
except ImportError as e_astro_imp: logger.critical(f"Astropy non trouvée: {e_astro_imp}.")
except Exception as e_astro_other_imp: logger.critical(f"Erreur import Astropy: {e_astro_other_imp}", exc_info=True)

# --- Reproject (critique pour la mosaïque) ---
REPROJECT_AVAILABLE = False
find_optimal_celestial_wcs, reproject_and_coadd, reproject_interp = None, None, None
try:
    from reproject.mosaicking import find_optimal_celestial_wcs as actual_find_optimal_wcs
    from reproject.mosaicking import reproject_and_coadd as actual_reproject_coadd
    from reproject import reproject_interp as actual_reproject_interp
    find_optimal_celestial_wcs, reproject_and_coadd, reproject_interp = actual_find_optimal_wcs, actual_reproject_coadd, actual_reproject_interp
    REPROJECT_AVAILABLE = True
    logger.info("Bibliothèque 'reproject' importée.")
except ImportError as e_reproject_final: logger.critical(f"Échec import reproject: {e_reproject_final}.")
except Exception as e_reproject_other_final: logger.critical(f"Erreur import 'reproject': {e_reproject_other_final}", exc_info=True)

_REPROJECT_INTERP_SUPPORTS_MATCH_BG = False
_REPROJECT_INTERP_SUPPORTS_FILL_VALUE = False
if REPROJECT_AVAILABLE and reproject_interp:
    try:
        _sig_reproj_interp = inspect.signature(reproject_interp)
        _REPROJECT_INTERP_SUPPORTS_MATCH_BG = "match_background" in _sig_reproj_interp.parameters
        _REPROJECT_INTERP_SUPPORTS_FILL_VALUE = "fill_value" in _sig_reproj_interp.parameters
    except Exception:
        _REPROJECT_INTERP_SUPPORTS_MATCH_BG = False
        _REPROJECT_INTERP_SUPPORTS_FILL_VALUE = False

# --- Local Project Module Imports ---
zemosaic_utils, ZEMOSAIC_UTILS_AVAILABLE = None, False
zemosaic_astrometry, ZEMOSAIC_ASTROMETRY_AVAILABLE = None, False
zemosaic_align_stack, ZEMOSAIC_ALIGN_STACK_AVAILABLE = None, False
CALC_GRID_OPTIMIZED_AVAILABLE = False
_calculate_final_mosaic_grid_optimized = None

try:
    import zemosaic_utils
    from zemosaic_utils import (
        gpu_assemble_final_mosaic_reproject_coadd,
        gpu_assemble_final_mosaic_incremental,
        reproject_and_coadd_wrapper,
    )
    ZEMOSAIC_UTILS_AVAILABLE = True
    logger.info("Module 'zemosaic_utils' importé.")
except ImportError as e: logger.error(f"Import 'zemosaic_utils.py' échoué: {e}.")
try: import zemosaic_astrometry; ZEMOSAIC_ASTROMETRY_AVAILABLE = True; logger.info("Module 'zemosaic_astrometry' importé.")
except ImportError as e: logger.error(f"Import 'zemosaic_astrometry.py' échoué: {e}.")
try: import zemosaic_align_stack; ZEMOSAIC_ALIGN_STACK_AVAILABLE = True; logger.info("Module 'zemosaic_align_stack' importé.")
except ImportError as e: logger.error(f"Import 'zemosaic_align_stack.py' échoué: {e}.")
try:
    from .solver_settings import SolverSettings  # type: ignore
except ImportError:
    from solver_settings import SolverSettings  # type: ignore

try:
    from .lecropper import detect_autocrop_rgb as _anchor_detect_autocrop
except ImportError:
    try:
        from lecropper import detect_autocrop_rgb as _anchor_detect_autocrop
    except Exception:
        _anchor_detect_autocrop = None

ANCHOR_AUTOCROP_AVAILABLE = callable(_anchor_detect_autocrop)

# Optional configuration import for GPU toggle
try:
    import zemosaic_config
    ZEMOSAIC_CONFIG_AVAILABLE = True
except Exception:
    zemosaic_config = None  # type: ignore
    ZEMOSAIC_CONFIG_AVAILABLE = False

try:
    from zemosaic_align_stack_gpu import (
        gpu_stack_from_paths as _p3_gpu_stack_from_paths,
        GPUStackingError as _P3GPUStackingError,
        _gpu_is_usable as _p3_gpu_is_usable,
    )
    _P3_GPU_HELPERS_AVAILABLE = True
except Exception:
    _p3_gpu_stack_from_paths = None

    class _P3GPUStackingError(RuntimeError):
        pass

    def _p3_gpu_is_usable(logger=None):
        return False

    _P3_GPU_HELPERS_AVAILABLE = False

_P3_GPU_STATE = {
    "allowed": True,
    "hard_disabled": False,
    "health_checked": False,
    "healthy": False,
    "info_logged": False,
}

import importlib.util

# Global semaphore to throttle concurrent *.npy cache reads in Phase 3
_CACHE_IO_SEMAPHORE = threading.Semaphore(2 if os.name == 'nt' else 4)

# Global semaphore to limit concurrent Phase 3 (master tile) tasks.
# This allows runtime adaptation when other apps (e.g. a video read) are active.
# It is initialized later inside run_hierarchical_mosaic and can be reassigned
# by the runtime monitor to change the concurrency cap without restarting pools.
_PH3_CONCURRENCY_SEMAPHORE = threading.Semaphore(2 if os.name == 'nt' else 4)

# --- Basic IO throughput probing helpers (Windows-friendly, OS-agnostic) ---
def _measure_sequential_read_mbps(file_path: str, bytes_to_read: int = 16 * 1024 * 1024, block_size: int = 1 * 1024 * 1024) -> float | None:
    """Measure approximate sequential read speed on a single file.

    Returns MB/s or None on failure. Uses small sizes to avoid long stalls.
    """
    try:
        path_obj = Path(file_path).expanduser()
        if not path_obj.is_file():
            return None
        size_target = max(block_size, bytes_to_read)
        read_total = 0
        t0 = time.perf_counter()
        with path_obj.open('rb', buffering=0) as f:
            while read_total < size_target:
                chunk = f.read(min(block_size, size_target - read_total))
                if not chunk:
                    break
                read_total += len(chunk)
        dt = max(1e-6, time.perf_counter() - t0)
        return (read_total / (1024 * 1024)) / dt
    except Exception:
        return None


def _measure_sequential_write_mbps(dir_path: str, bytes_to_write: int = 16 * 1024 * 1024, block_size: int = 1 * 1024 * 1024) -> float | None:
    """Measure approximate sequential write speed in a directory.

    Writes and deletes a small temporary file. Returns MB/s or None on failure.
    """
    try:
        dir_obj = Path(dir_path).expanduser()
        if not dir_obj.is_dir():
            return None
        import uuid as _uuid
        tmp_path = dir_obj / f"_zemosaic_io_probe_{_uuid.uuid4().hex}.bin"
        size_target = max(block_size, bytes_to_write)
        data = os.urandom(block_size)
        written_total = 0
        t0 = time.perf_counter()
        with tmp_path.open('wb', buffering=0) as f:
            while written_total < size_target:
                to_write = min(block_size, size_target - written_total)
                f.write(data[:to_write])
                written_total += to_write
            try:
                f.flush(); os.fsync(f.fileno())
            except Exception:
                pass
        dt = max(1e-6, time.perf_counter() - t0)
        try:
            tmp_path.unlink()
        except FileNotFoundError:
            pass
        except Exception:
            pass
        return (written_total / (1024 * 1024)) / dt
    except Exception:
        return None


def _categorize_io_speed(mbps: float | None) -> str:
    """Rough IO category string based on MB/s; conservative thresholds.

    very_slow: < 60 MB/s (typical USB HDD or spinning disk behind a hub)
    slow:      < 120 MB/s
    medium:    < 220 MB/s
    fast:      >= 220 MB/s
    """
    if mbps is None or mbps <= 0:
        return "unknown"
    if mbps < 60:
        return "very_slow"
    if mbps < 120:
        return "slow"
    if mbps < 220:
        return "medium"
    return "fast"

def gpu_is_available() -> bool:
    """Return True if CuPy and a CUDA device are available."""
    if not CUPY_AVAILABLE:
        return False
    try:
        import cupy
        return cupy.is_available()
    except Exception:
        return False

# Exposed compatibility flag expected by some tests
ASTROMETRY_SOLVER_AVAILABLE = ZEMOSAIC_ASTROMETRY_AVAILABLE

# progress_callback(stage: str, current: int, total: int)







# DANS zemosaic_worker.py

# ... (imports et logger configuré comme avant) ...

# --- Helper pour log et callback ---
def _log_and_callback(
    message_key_or_raw,
    progress_value=None,
    level="INFO",
    callback=None,
    **kwargs,
):
    """
    Helper pour loguer un message et appeler le callback GUI.
    - Si level est INFO, WARN, ERROR, SUCCESS, message_key_or_raw est traité comme une clé.
    - Sinon (DEBUG, ETA_LEVEL, etc.), message_key_or_raw est loggué tel quel.
    - Les **kwargs sont passés pour le formatage si message_key_or_raw est une clé.
    """
    # Support backwards compatibility for lvl/prog keyword aliases
    if "lvl" in kwargs and level == "INFO":
        level = kwargs.pop("lvl")
    elif "lvl" in kwargs:
        level = kwargs.pop("lvl")
    if "prog" in kwargs and progress_value is None:
        progress_value = kwargs.pop("prog")
    elif "prog" in kwargs:
        progress_value = kwargs.pop("prog")

    # Count alignment warnings for final summary
    if isinstance(message_key_or_raw, str) and message_key_or_raw in ALIGN_WARNING_COUNTS:
        ALIGN_WARNING_COUNTS[message_key_or_raw] += 1
    log_level_map = {
        "INFO": logging.INFO, "DEBUG": logging.DEBUG, "DEBUG_DETAIL": logging.DEBUG,
        "WARN": logging.WARNING, "ERROR": logging.ERROR, "SUCCESS": logging.INFO,
        "INFO_DETAIL": logging.DEBUG, 
        "ETA_LEVEL": logging.DEBUG, # Pour les messages ETA spécifiques
        "CHRONO_LEVEL": logging.DEBUG # Pour les commandes de chrono
    }
    
    level_str = "INFO" # Défaut
    if isinstance(level, str):
        level_str = level.upper()
    elif level is not None:
        logger.warning(f"_log_and_callback: Argument 'level' inattendu (type: {type(level)}, valeur: {level}). Utilisation de INFO par défaut.")

    # Préparer le message pour le logger Python interne
    final_message_for_py_logger = ""
    user_facing_log_levels = ["INFO", "WARN", "ERROR", "SUCCESS"]

    if level_str in user_facing_log_levels:
        # Pour ces niveaux, on s'attend à une clé. Logguer la clé et les args pour le debug interne.
        final_message_for_py_logger = f"[CLÉ_POUR_GUI: {message_key_or_raw}]"
        if kwargs:
            final_message_for_py_logger += f" (Args: {kwargs})"
    else: 
        # Pour les niveaux DEBUG, ETA, CHRONO, on loggue le message brut.
        # Si des kwargs sont passés avec un message brut (ex: debug), on peut essayer de le formater.
        final_message_for_py_logger = str(message_key_or_raw)
        if kwargs:
            try:
                final_message_for_py_logger = final_message_for_py_logger.format(**kwargs)
            except (KeyError, ValueError, IndexError) as fmt_err:
                logger.debug(f"Échec formatage message brut '{message_key_or_raw}' avec kwargs {kwargs} pour logger interne: {fmt_err}")
                # Garder le message brut si le formatage échoue

    logger.log(log_level_map.get(level_str, logging.INFO), final_message_for_py_logger)
    
    # Appel au callback GUI
    if callback and callable(callback):
        try:
            # On envoie la clé (ou le message brut) et les kwargs au callback GUI.
            # La GUI (sa méthode _log_message) sera responsable de faire la traduction
            # et le formatage final en utilisant ces kwargs si message_key_or_raw est une clé.
            #
            # La signature de _log_message dans la GUI doit être :
            # def _log_message(self, message_key_or_raw, progress_value=None, level="INFO", **kwargs):
            callback(message_key_or_raw, progress_value, level if isinstance(level, str) else "INFO", **kwargs)
        except Exception as e_cb:
            # Logguer l'erreur du callback, mais ne pas planter le worker pour ça
            logger.warning(f"Erreur dans progress_callback lors de l'appel depuis _log_and_callback: {e_cb}", exc_info=False)
            # Peut-être afficher la trace pour le debug du callback lui-même
            # logger.debug("Traceback de l'erreur du callback:", exc_info=True)




def _log_memory_usage(progress_callback: callable, context_message: str = ""): # Fonction helper définie ici ou globalement dans le module
    """Logue l'utilisation actuelle de la mémoire du processus et du système."""
    if not progress_callback or not callable(progress_callback):
        return
    try:
        process = psutil.Process(os.getpid())
        mem_info = process.memory_info()
        rss_mb = mem_info.rss / (1024 * 1024)
        vms_mb = mem_info.vms / (1024 * 1024)

        virtual_mem = psutil.virtual_memory()
        available_ram_mb = virtual_mem.available / (1024 * 1024)
        total_ram_mb = virtual_mem.total / (1024 * 1024)
        percent_ram_used = virtual_mem.percent

        swap_mem = psutil.swap_memory()
        used_swap_mb = swap_mem.used / (1024 * 1024)
        total_swap_mb = swap_mem.total / (1024 * 1024)
        percent_swap_used = swap_mem.percent
        
        log_msg = (
            f"Memory Usage ({context_message}): "
            f"Proc RSS: {rss_mb:.1f}MB, VMS: {vms_mb:.1f}MB. "
            f"Sys RAM: Avail {available_ram_mb:.0f}MB / Total {total_ram_mb:.0f}MB ({percent_ram_used}%% used). "
            f"Sys Swap: Used {used_swap_mb:.0f}MB / Total {total_swap_mb:.0f}MB ({percent_swap_used}%% used)."
        )
        _log_and_callback(log_msg, prog=None, lvl="DEBUG", callback=progress_callback)
        
    except Exception as e_mem_log:
        _log_and_callback(f"Erreur lors du logging mémoire ({context_message}): {e_mem_log}", prog=None, lvl="WARN", callback=progress_callback)


def _log_alignment_warning_summary():
    """Write a summary of alignment warnings to the worker log."""
    total = sum(ALIGN_WARNING_COUNTS.values())
    if total == 0:
        logger.info("Alignment summary: no frames ignored due to errors.")
        return

    logger.info("===== Alignment warning summary =====")
    logger.info("Total frames ignored: %d", total)
    for key, count in ALIGN_WARNING_COUNTS.items():
        if count:
            human = ALIGN_WARNING_SUMMARY.get(key, key)
            logger.info("%d frame(s) - %s", count, human)


def _crop_array_to_signal(
    img: np.ndarray,
    coverage: np.ndarray | None = None,
    margin_frac: float = 0.05,
) -> tuple[np.ndarray, tuple[int, int, int, int]]:
    """Crop ``img`` to the bounding box of useful signal.

    Parameters
    ----------
    img : np.ndarray
        2D/3D array containing image data.
    coverage : np.ndarray | None, optional
        Optional coverage map used as mask (>0 considered valid).
    margin_frac : float, optional
        Additional fractional margin added to each side of the bounding box.

    Returns
    -------
    tuple[np.ndarray, tuple[int, int, int, int]]
        Cropped image and bounding box ``(y0, y1, x0, x1)``.
    """

    if img is None:
        return img, (0, 0, 0, 0)

    arr = np.asarray(img)
    if arr.ndim < 2:
        height = int(arr.shape[0]) if arr.ndim >= 1 else 0
        width = int(arr.shape[1]) if arr.ndim > 1 else 0
        return img, (0, height, 0, width)

    height, width = int(arr.shape[0]), int(arr.shape[1])
    default_bbox = (0, height, 0, width)

    mask: np.ndarray | None = None
    if coverage is not None:
        try:
            cov_arr = np.asarray(coverage)
            if cov_arr.shape[0] == height and cov_arr.shape[1] == width:
                mask = cov_arr > 0
        except Exception:
            mask = None

    if mask is None:
        data_arr = np.asarray(img)
        if data_arr.ndim == 3:
            valid_pixels = np.any(np.isfinite(data_arr) & (data_arr != 0), axis=-1)
        else:
            valid_pixels = np.isfinite(data_arr) & (data_arr != 0)
        mask = valid_pixels

    if not np.any(mask):
        return img, default_bbox

    rows = np.where(np.any(mask, axis=1))[0]
    cols = np.where(np.any(mask, axis=0))[0]
    if rows.size == 0 or cols.size == 0:
        return img, default_bbox

    y_min, y_max = int(rows[0]), int(rows[-1]) + 1
    x_min, x_max = int(cols[0]), int(cols[-1]) + 1

    try:
        margin_frac = float(margin_frac)
    except (TypeError, ValueError):
        margin_frac = 0.0
    margin_frac = max(0.0, margin_frac)

    if margin_frac > 0.0:
        bbox_height = y_max - y_min
        bbox_width = x_max - x_min
        margin_y = int(math.ceil(bbox_height * margin_frac))
        margin_x = int(math.ceil(bbox_width * margin_frac))
        y_min = max(0, y_min - margin_y)
        y_max = min(height, y_max + margin_y)
        x_min = max(0, x_min - margin_x)
        x_max = min(width, x_max + margin_x)

    bbox = (y_min, y_max, x_min, x_max)
    cropped = img[y_min:y_max, x_min:x_max, ...]

    return cropped, bbox


def _auto_crop_mosaic_to_valid_region(
    mosaic: np.ndarray,
    coverage: np.ndarray | None,
    output_wcs,
    log_callback=None,
    threshold: float = 1e-6,
    *,
    follow_signal: bool | None = None,
    margin_frac: float | None = 0.05,
    alpha_map: np.ndarray | None = None,
):
    """Crop blank borders from the mosaic using the coverage map.

    Parameters
    ----------
    mosaic : np.ndarray
        Final stacked mosaic with shape ``(H, W, C)``.
    coverage : np.ndarray | None
        Coverage/weight map returned by ``reproject_and_coadd``.
    output_wcs : astropy.wcs.WCS | Any
        WCS object describing the mosaic; will be updated in-place if cropping occurs.
    log_callback : callable | None
        Optional callback used to emit log messages (same signature as ``_pcb``).
    threshold : float
        Minimum coverage value considered as valid data when computing the crop bounds.

    Returns
    -------
    tuple[np.ndarray, np.ndarray | None, np.ndarray | None]
        Cropped mosaic, coverage, and alpha arrays. If no cropping is
        necessary the original inputs are returned unchanged.
    """

    if mosaic is None:
        return mosaic, coverage, alpha_map

    mosaic_arr = np.asarray(mosaic)
    if mosaic_arr.ndim < 2:
        return mosaic, coverage, alpha_map

    default_bbox = (0, int(mosaic_arr.shape[0]), 0, int(mosaic_arr.shape[1]))

    if follow_signal is None:
        try:
            import zemosaic_config

            cfg = zemosaic_config.load_config() or {}
            follow_signal = bool(cfg.get("crop_follow_signal", False))
        except Exception:
            follow_signal = False
    else:
        follow_signal = bool(follow_signal)

    try:
        margin_value = 0.05 if margin_frac is None else float(margin_frac)
    except (TypeError, ValueError):
        margin_value = 0.05
    margin_value = max(0.0, margin_value)

    bbox = default_bbox
    cropped_mosaic = mosaic
    cropped_coverage = coverage
    alpha_out = alpha_map
    used_signal_crop = False

    if follow_signal:
        try:
            candidate_mosaic, candidate_bbox = _crop_array_to_signal(
                mosaic,
                coverage,
                margin_value,
            )
            if candidate_bbox:
                bbox = candidate_bbox
                used_signal_crop = True
                if bbox != default_bbox:
                    cropped_mosaic = candidate_mosaic
                    if coverage is not None:
                        y0, y1, x0, x1 = bbox
                        cropped_coverage = coverage[y0:y1, x0:x1]
                    if alpha_out is not None:
                        y0, y1, x0, x1 = bbox
                        alpha_out = alpha_out[y0:y1, x0:x1]
                    if logger.isEnabledFor(logging.DEBUG):
                        logger.debug("follow_signal crop applied, bbox=%s", bbox)
        except Exception:
            bbox = default_bbox
            used_signal_crop = False

    if used_signal_crop and bbox == default_bbox:
        return mosaic, coverage, alpha_out

    if not used_signal_crop:
        if coverage is None:
            return mosaic, coverage, alpha_out

        try:
            cov_array = np.asarray(coverage)
        except Exception:
            cov_array = coverage

        if getattr(cov_array, "ndim", 0) != 2:
            return mosaic, coverage, alpha_out

        try:
            valid_mask = np.asarray(cov_array) > float(threshold)
        except Exception:
            return mosaic, coverage, alpha_out

        if not np.any(valid_mask):
            return mosaic, coverage, alpha_out

        rows = np.where(np.any(valid_mask, axis=1))[0]
        cols = np.where(np.any(valid_mask, axis=0))[0]
        if rows.size == 0 or cols.size == 0:
            return mosaic, coverage, alpha_out

        y_min, y_max = int(rows[0]), int(rows[-1]) + 1
        x_min, x_max = int(cols[0]), int(cols[-1]) + 1

        bbox = (y_min, y_max, x_min, x_max)

        if (
            y_min == 0
            and x_min == 0
            and y_max == mosaic.shape[0]
            and x_max == mosaic.shape[1]
        ):
            return mosaic, coverage, alpha_out

        cropped_mosaic = mosaic[y_min:y_max, x_min:x_max, ...]
        cropped_coverage = coverage[y_min:y_max, x_min:x_max]

    if used_signal_crop and bbox == default_bbox:
        # Signal crop requested but no bounding box reduction occurred.
        return mosaic, coverage

    new_shape = tuple(int(v) for v in np.shape(cropped_mosaic))

    y_min, y_max, x_min, x_max = bbox

    if callable(log_callback):
        try:
            log_callback(
                "ASM_REPROJ_COADD: Auto-cropped output to coverage bounds",
                prog=None,
                lvl="INFO_DETAIL",
                y_bounds=f"{y_min}:{y_max}",
                x_bounds=f"{x_min}:{x_max}",
                new_shape=str(new_shape),
            )
        except Exception:
            pass

    try:
        if hasattr(output_wcs, "wcs") and getattr(output_wcs, "wcs") is not None:
            if hasattr(output_wcs.wcs, "crpix") and output_wcs.wcs.crpix is not None:
                output_wcs.wcs.crpix[0] -= float(x_min)
                output_wcs.wcs.crpix[1] -= float(y_min)
            if hasattr(output_wcs.wcs, "naxis1"):
                output_wcs.wcs.naxis1 = int(new_shape[1])
            if hasattr(output_wcs.wcs, "naxis2"):
                output_wcs.wcs.naxis2 = int(new_shape[0])
    except Exception:
        pass

    for attr, val in (
        ("pixel_shape", (int(new_shape[1]), int(new_shape[0]))),
        ("array_shape", (int(new_shape[0]), int(new_shape[1]))),
    ):
        if hasattr(output_wcs, attr):
            try:
                setattr(output_wcs, attr, val)
            except Exception:
                pass

    if alpha_out is not None:
        y0, y1, x0, x1 = bbox
        alpha_out = alpha_out[y0:y1, x0:x1]

    return cropped_mosaic, cropped_coverage, alpha_out


def _wait_for_memmap_files(prefixes, timeout=10.0):
    """Poll until each prefix.dat and prefix.npy exist and are non-empty."""
    import time, os
    start = time.time()
    while True:
        all_ready = True
        for prefix in prefixes:
            dat_f = prefix + '.dat'
            npy_f = prefix + '.npy'
            if not (_path_exists(dat_f) and _path_getsize(dat_f) > 0 and _path_exists(npy_f) and _path_getsize(npy_f) > 0):
                all_ready = False
                break
        if all_ready:
            return
        if time.time() - start > timeout:
            raise RuntimeError(f"Memmap file not ready after {timeout}s: {prefix}")


def astap_paths_valid(astap_exe_path: str, astap_data_dir: str) -> bool:
    """Return True if ASTAP executable and data directory look valid."""
    return (
        astap_exe_path
        and _path_isfile(astap_exe_path)
        and astap_data_dir
        and _path_isdir(astap_data_dir)
    )


def _write_header_to_fits(file_path: str, header_obj, pcb=None):
    """Safely update ``file_path`` FITS header with ``header_obj`` if possible."""
    if not (ASTROPY_AVAILABLE and fits):
        return
    try:
        with fits.open(file_path, mode="update", memmap=False) as hdul:
            hdul[0].header.update(header_obj)
            hdul.flush()
        if pcb:
            pcb("getwcs_info_header_written", lvl="DEBUG_DETAIL", filename=_safe_basename(file_path))
    except Exception as e_update:
        if pcb:
            pcb("getwcs_warn_header_write_failed", lvl="WARN", filename=_safe_basename(file_path), error=str(e_update))


def solve_with_astrometry(
    image_fits_path: str,
    fits_header,
    settings: dict | None,
    progress_callback=None,
):
    """Attempt plate solving via the Astrometry.net service."""

    if not ASTROMETRY_SOLVER_AVAILABLE:
        return None

    try:
        from . import zemosaic_astrometry
    except Exception:
        return None

    solver_dict = settings or {}
    api_key = solver_dict.get("api_key", "")
    timeout = solver_dict.get("timeout")
    down = solver_dict.get("downsample")

    try:
        return zemosaic_astrometry.solve_with_astrometry_net(
            image_fits_path,
            fits_header,
            api_key=api_key,
            timeout_sec=timeout or 60,
            downsample_factor=down,
            update_original_header_in_place=True,
            progress_callback=progress_callback,
        )
    except Exception as e:
        _log_and_callback(
            f"Astrometry solve error: {e}", prog=None, lvl="WARN", callback=progress_callback
        )
        return None


def solve_with_ansvr(
    image_fits_path: str,
    fits_header,
    settings: dict | None,
    progress_callback=None,
):
    """Attempt plate solving using a local ansvr installation."""

    if not ASTROMETRY_SOLVER_AVAILABLE:
        return None

    try:
        from . import zemosaic_astrometry
    except Exception:
        return None

    solver_dict = settings or {}
    path = solver_dict.get("ansvr_path") or solver_dict.get("astrometry_local_path") or solver_dict.get("local_ansvr_path")
    timeout = solver_dict.get("ansvr_timeout") or solver_dict.get("timeout")

    try:
        return zemosaic_astrometry.solve_with_ansvr(
            image_fits_path,
            fits_header,
            ansvr_config_path=path or "",
            timeout_sec=timeout or 120,
            update_original_header_in_place=True,
            progress_callback=progress_callback,
        )
    except Exception as e:
        _log_and_callback(
            f"Ansvr solve error: {e}", prog=None, lvl="WARN", callback=progress_callback
        )
        return None


# Note: Ancienne fonction _prepare_image_for_astap supprimée. Les images sont
# passées à ASTAP telles quelles pour la résolution (pas de conversion mono).


def reproject_tile_to_mosaic(
    tile_path: str,
    tile_wcs,
    mosaic_wcs,
    mosaic_shape_hw,
    feather: bool = True,
    apply_crop: bool = False,
    crop_percent: float = 0.0,
    tile_affine: tuple[float, float] | None = None,
    gain: float | None = None,
    offset: float | None = None,
    match_background: bool = True,
    nan_fill_value: float = 0.0,
    enforce_positive: bool = True,
    return_image: bool = True,
):
    """Reprojecte une tuile sur la grille finale et renvoie l'image et sa carte
    de poids ainsi que la bounding box utile.

    Les bornes sont retournées dans l'ordre ``(xmin, xmax, ymin, ymax)`` afin
    de correspondre aux indices ``[ligne, colonne]`` lors de l'incrémentation
    sur la mosaïque.

    ``tile_wcs`` et ``mosaic_wcs`` peuvent être soit des objets :class:`WCS`
    directement, soit des en-têtes FITS (``dict`` ou :class:`~astropy.io.fits.Header``).
    Cela permet d'utiliser cette fonction avec :class:`concurrent.futures.ProcessPoolExecutor`
    où les arguments doivent être sérialisables.
    """
    if not (REPROJECT_AVAILABLE and reproject_interp and ASTROPY_AVAILABLE and fits):
        return None, None, None, (0, 0, 0, 0)

    # Les objets WCS ne sont pas toujours sérialisables via multiprocessing.
    # Si on reçoit des en-têtes (dict ou fits.Header), reconstruire les WCS ici.
    if ASTROPY_AVAILABLE and WCS:
        if not isinstance(tile_wcs, WCS):
            try:
                tile_wcs = WCS(tile_wcs)
            except Exception:
                return None, None, None, (0, 0, 0, 0)
        if not isinstance(mosaic_wcs, WCS):
            try:
                mosaic_wcs = WCS(mosaic_wcs)
            except Exception:
                return None, None, None, (0, 0, 0, 0)

    try:
        data, weight_map, _ = load_image_with_optional_alpha(
            tile_path,
            tile_label=_safe_basename(tile_path),
        )
    except Exception:
        return None, None, None, (0, 0, 0, 0)
    n_channels = data.shape[-1]
    alpha_weight_map = None
    if isinstance(weight_map, np.ndarray):
        try:
            alpha_weight_map = np.clip(
                np.asarray(weight_map, dtype=np.float32, copy=False),
                0.0,
                1.0,
            )
        except Exception:
            alpha_weight_map = None

    if gain is None or offset is None:
        if tile_affine is not None:
            try:
                gain, offset = tile_affine
            except Exception:
                gain, offset = None, None

    if gain is not None and offset is not None:
        try:
            gain_val = float(gain)
        except Exception:
            gain_val = 1.0
        try:
            offset_val = float(offset)
        except Exception:
            offset_val = 0.0
        if not np.isfinite(gain_val):
            gain_val = 1.0
        if not np.isfinite(offset_val):
            offset_val = 0.0
        if gain_val != 1.0:
            data *= gain_val
        if offset_val != 0.0:
            data += offset_val

    # Optional cropping of the tile before reprojection
    if apply_crop and crop_percent > 1e-3 and ZEMOSAIC_UTILS_AVAILABLE \
            and hasattr(zemosaic_utils, "crop_image_and_wcs"):
        try:
            original_hw = data.shape[:2]
            cropped, cropped_wcs = zemosaic_utils.crop_image_and_wcs(
                data,
                tile_wcs,
                crop_percent / 100.0,
                progress_callback=None,
            )
            if cropped is not None and cropped_wcs is not None:
                data = cropped
                tile_wcs = cropped_wcs
                n_channels = data.shape[-1]
                new_hw = data.shape[:2]
                if (
                    alpha_weight_map is not None
                    and alpha_weight_map.shape == original_hw
                    and original_hw[0] >= new_hw[0]
                    and original_hw[1] >= new_hw[1]
                ):
                    dh = (original_hw[0] - new_hw[0]) // 2
                    dw = (original_hw[1] - new_hw[1]) // 2
                    top = dh
                    bottom = top + new_hw[0]
                    left = dw
                    right = left + new_hw[1]
                    alpha_weight_map = alpha_weight_map[top:bottom, left:right]
                    if alpha_weight_map.shape != new_hw:
                        alpha_weight_map = None
                else:
                    alpha_weight_map = None
        except Exception:
            pass

    if data.ndim != 3:
        raise ValueError(f"Expected HWC data after normalization, got shape {data.shape}")

    base_weight = np.ones(data.shape[:2], dtype=np.float32)
    if (
        feather
        and ZEMOSAIC_UTILS_AVAILABLE
        and hasattr(zemosaic_utils, "make_radial_weight_map")
    ):
        try:
            base_weight = zemosaic_utils.make_radial_weight_map(
                data.shape[0],
                data.shape[1],
                feather_fraction=0.92,
                min_weight_floor=0.10,
            )
            logger.debug("Feather applied with min_weight_floor=0.10")
        except Exception:
            base_weight = np.ones(data.shape[:2], dtype=np.float32)

    # --- Determine bounding box covered by the tile on the mosaic
    if alpha_weight_map is not None and alpha_weight_map.shape == base_weight.shape:
        alpha_component = alpha_weight_map
    else:
        alpha_component = np.ones_like(base_weight, dtype=np.float32)
    combined_weight = alpha_component * base_weight

    footprint_full, _ = reproject_interp(
        (combined_weight, tile_wcs),
        mosaic_wcs,
        shape_out=mosaic_shape_hw,
        order='nearest-neighbor',  # suffit, c'est binaire
        parallel=False,
    )

    j_idx, i_idx = np.where(footprint_full > 0)
    if j_idx.size == 0:
        return None, None, None, (0, 0, 0, 0)

    j0, j1 = int(j_idx.min()), int(j_idx.max()) + 1
    i0, i1 = int(i_idx.min()), int(i_idx.max()) + 1
    h, w = j1 - j0, i1 - i0

    # Create a WCS for the sub-region
    try:
        sub_wcs = mosaic_wcs.deepcopy()
        sub_wcs.wcs.crpix = [mosaic_wcs.wcs.crpix[0] - i0, mosaic_wcs.wcs.crpix[1] - j0]
    except Exception:
        sub_wcs = mosaic_wcs

    # Allocate arrays only for the useful area
    reproj_img = None
    if return_image:
        reproj_img = np.zeros((h, w, n_channels), dtype=np.float32)
    reproj_weight = np.zeros((h, w), dtype=np.float32)

    try:
        weight_reproj_full, _ = reproject_interp(
            (combined_weight, tile_wcs),
            sub_wcs,
            shape_out=(h, w),
            order='bilinear',
            parallel=False,
        )
        weight_reproj_full = np.clip(np.nan_to_num(weight_reproj_full, nan=0.0), 0.0, None)
    except Exception:
        weight_reproj_full = np.zeros((h, w), dtype=np.float32)

    if return_image:
        for c in range(n_channels):
            reproj_c, footprint = reproject_interp(
                (data[..., c], tile_wcs),
                sub_wcs,
                shape_out=(h, w),
                order='bilinear',
                parallel=False,
            )

            total_w = footprint * weight_reproj_full
            reproj_img[..., c] = reproj_c.astype(np.float32)
            reproj_weight += total_w.astype(np.float32)
    else:
        reproj_weight = weight_reproj_full.astype(np.float32, copy=False)

    try:
        alpha_patch, _ = reproject_interp(
            (alpha_component, tile_wcs),
            sub_wcs,
            shape_out=(h, w),
            order='bilinear',
            parallel=False,
        )
        alpha_patch = np.clip(np.nan_to_num(alpha_patch, nan=0.0), 0.0, 1.0).astype(np.float32, copy=False)
    except Exception:
        alpha_patch = np.ones((h, w), dtype=np.float32)

    valid = reproj_weight > 0
    if not np.any(valid):
        return None, None, None, (0, 0, 0, 0)

    # Normalisation d'arrière-plan optionnelle (match_background)
    if return_image and match_background and reproj_img is not None:
        try:
            for c in range(n_channels):
                channel_view = reproj_img[..., c]
                med_c = np.nanmedian(channel_view[valid])
                if np.isfinite(med_c):
                    channel_view -= med_c
        except Exception:
            pass

    if return_image and nan_fill_value is not None and reproj_img is not None:
        np.nan_to_num(
            reproj_img,
            copy=False,
            nan=nan_fill_value,
            posinf=nan_fill_value,
            neginf=nan_fill_value,
        )
    if return_image and enforce_positive and reproj_img is not None:
        np.clip(reproj_img, 0.0, None, out=reproj_img)

    # Les indices sont retournés dans l'ordre (xmin, xmax, ymin, ymax)
    return reproj_img, reproj_weight, alpha_patch, (i0, i1, j0, j1)


def _build_alpha_union_map(
    master_tile_fits_with_wcs_list: list,
    mosaic_wcs,
    mosaic_shape_hw: tuple[int, int],
    *,
    apply_crop: bool,
    crop_percent: float,
    progress_callback: callable | None = None,
):
    """Reproject ALPHA masks from tiles and combine them via max."""

    if not master_tile_fits_with_wcs_list:
        return None

    alpha_union = np.zeros(mosaic_shape_hw, dtype=np.float32)
    for idx, entry in enumerate(master_tile_fits_with_wcs_list, 1):
        try:
            tile_path, tile_wcs = entry
        except Exception:
            continue
        if not tile_path or tile_wcs is None:
            continue
        try:
            _, _, alpha_patch, (xmin, xmax, ymin, ymax) = reproject_tile_to_mosaic(
                tile_path,
                tile_wcs,
                mosaic_wcs,
                mosaic_shape_hw,
                feather=False,
                apply_crop=apply_crop,
                crop_percent=crop_percent,
                tile_affine=None,
                gain=None,
                offset=None,
                match_background=False,
                nan_fill_value=None,
                enforce_positive=False,
                return_image=False,
            )
        except Exception:
            logger.debug(
                "Alpha reprojection failed for tile %s (idx=%d)",
                _safe_basename(tile_path),
                idx,
                exc_info=True,
            )
            continue
        if (
            alpha_patch is None
            or xmin >= xmax
            or ymin >= ymax
            or alpha_patch.shape[0] != (ymax - ymin)
            or alpha_patch.shape[1] != (xmax - xmin)
        ):
            continue
        tgt = alpha_union[ymin:ymax, xmin:xmax]
        np.maximum(tgt, alpha_patch.astype(np.float32, copy=False), out=tgt)

    return alpha_union




# --- Fonctions Utilitaires Internes au Worker ---
def _calculate_final_mosaic_grid(panel_wcs_list: list, panel_shapes_hw_list: list,
                                 drizzle_scale_factor: float = 1.0, progress_callback: callable = None):
    num_initial_inputs = len(panel_wcs_list)
    # Utilisation de clés pour les messages utilisateur
    _log_and_callback("calcgrid_info_start_calc", num_wcs_shapes=num_initial_inputs, scale_factor=drizzle_scale_factor, level="DEBUG_DETAIL", callback=progress_callback)
    
    if not REPROJECT_AVAILABLE:
        _log_and_callback("calcgrid_error_reproject_unavailable", level="ERROR", callback=progress_callback)
        return None, None
    if find_optimal_celestial_wcs is None:
        if CALC_GRID_OPTIMIZED_AVAILABLE and _calculate_final_mosaic_grid_optimized:
            _log_and_callback(
                "calcgrid_warn_find_optimal_celestial_wcs_missing",
                level="WARN",
                callback=progress_callback,
            )
            return _calculate_final_mosaic_grid_optimized(
                panel_wcs_list, panel_shapes_hw_list, drizzle_scale_factor
            )
        _log_and_callback("calcgrid_error_reproject_unavailable", level="ERROR", callback=progress_callback)
        return None, None
    if not (ASTROPY_AVAILABLE and u and Angle):
        _log_and_callback("calcgrid_error_astropy_unavailable", level="ERROR", callback=progress_callback); return None, None
    if num_initial_inputs == 0:
        _log_and_callback("calcgrid_error_no_wcs_shape", level="ERROR", callback=progress_callback); return None, None

    valid_wcs_inputs = []; valid_shapes_inputs_hw = []
    for idx_filt, wcs_filt in enumerate(panel_wcs_list):
        if isinstance(wcs_filt, WCS) and wcs_filt.is_celestial:
            if idx_filt < len(panel_shapes_hw_list):
                shape_filt = panel_shapes_hw_list[idx_filt]
                if isinstance(shape_filt, tuple) and len(shape_filt) == 2 and isinstance(shape_filt[0], int) and shape_filt[0] > 0 and isinstance(shape_filt[1], int) and shape_filt[1] > 0:
                    valid_wcs_inputs.append(wcs_filt); valid_shapes_inputs_hw.append(shape_filt)
                else: _log_and_callback("calcgrid_warn_invalid_shape_skipped", shape=shape_filt, wcs_index=idx_filt, level="WARN", callback=progress_callback)
            else: _log_and_callback("calcgrid_warn_no_shape_for_wcs_skipped", wcs_index=idx_filt, level="WARN", callback=progress_callback)
        else: _log_and_callback("calcgrid_warn_invalid_wcs_skipped", wcs_index=idx_filt, level="WARN", callback=progress_callback)
    
    if not valid_wcs_inputs:
        _log_and_callback("calcgrid_error_no_valid_wcs_shape_after_filter", level="ERROR", callback=progress_callback); return None, None

    panel_wcs_list_to_use = valid_wcs_inputs; panel_shapes_hw_list_to_use = valid_shapes_inputs_hw
    num_valid_inputs = len(panel_wcs_list_to_use)
    _log_and_callback(f"CalcGrid: {num_valid_inputs} WCS/Shapes valides pour calcul.", None, "DEBUG", progress_callback) # Log technique

    inputs_for_optimal_wcs_calc = []
    for i in range(num_valid_inputs):
        wcs_in = panel_wcs_list_to_use[i]
        shape_in_hw = panel_shapes_hw_list_to_use[i] # shape (height, width)
        shape_in_wh_for_wcs_pixel_shape = (shape_in_hw[1], shape_in_hw[0]) # (width, height) for WCS.pixel_shape

        # Ensure WCS.pixel_shape is set for reproject, it might use it internally.
        if wcs_in.pixel_shape is None or wcs_in.pixel_shape != shape_in_wh_for_wcs_pixel_shape:
            try: 
                wcs_in.pixel_shape = shape_in_wh_for_wcs_pixel_shape
                _log_and_callback(f"CalcGrid: WCS {i} pixel_shape set to {shape_in_wh_for_wcs_pixel_shape}", None, "DEBUG_DETAIL", progress_callback)
            except Exception as e_pshape_set: 
                _log_and_callback("calcgrid_warn_set_pixel_shape_failed", wcs_index=i, error=str(e_pshape_set), level="WARN", callback=progress_callback)
        
        # **** LA CORRECTION EST ICI ****
        # find_optimal_celestial_wcs expects a list of (shape, wcs) tuples or HDU objects.
        # The shape should be (height, width).
        inputs_for_optimal_wcs_calc.append((shape_in_hw, wcs_in))
        # *****************************

    if not inputs_for_optimal_wcs_calc:
        _log_and_callback("calcgrid_error_no_wcs_for_optimal_calc", level="ERROR", callback=progress_callback); return None, None
        
    try:
        sum_of_pixel_scales_deg = 0.0; count_of_valid_scales = 0
        # For calculating average input pixel scale, we use panel_wcs_list_to_use (which are just WCS objects)
        for wcs_obj_scale in panel_wcs_list_to_use: 
            if not (wcs_obj_scale and wcs_obj_scale.is_celestial): continue
            try:
                current_pixel_scale_deg = 0.0
                if hasattr(wcs_obj_scale, 'proj_plane_pixel_scales') and callable(wcs_obj_scale.proj_plane_pixel_scales):
                    pixel_scales_angle_tuple = wcs_obj_scale.proj_plane_pixel_scales(); current_pixel_scale_deg = np.mean(np.abs([s.to_value(u.deg) for s in pixel_scales_angle_tuple]))
                elif hasattr(wcs_obj_scale, 'pixel_scale_matrix'): current_pixel_scale_deg = np.sqrt(np.abs(np.linalg.det(wcs_obj_scale.pixel_scale_matrix)))
                else: continue
                if np.isfinite(current_pixel_scale_deg) and current_pixel_scale_deg > 1e-10: sum_of_pixel_scales_deg += current_pixel_scale_deg; count_of_valid_scales += 1
            except Exception: pass # Ignore errors in calculating scale for one WCS
        
        avg_input_pixel_scale_deg = (2.0 / 3600.0) # Fallback 2 arcsec/pix
        if count_of_valid_scales > 0: avg_input_pixel_scale_deg = sum_of_pixel_scales_deg / count_of_valid_scales
        elif num_valid_inputs > 0 : _log_and_callback("calcgrid_warn_scale_fallback", level="WARN", callback=progress_callback)
        
        target_resolution_deg_per_pixel = avg_input_pixel_scale_deg / drizzle_scale_factor
        target_resolution_angle = Angle(target_resolution_deg_per_pixel, unit=u.deg)
        _log_and_callback("calcgrid_info_scales", avg_input_scale_arcsec=avg_input_pixel_scale_deg*3600, target_scale_arcsec=target_resolution_angle.arcsec, level="INFO", callback=progress_callback)
        
        # Now call with inputs_for_optimal_wcs_calc which is a list of (shape_hw, wcs) tuples
        optimal_wcs_out, optimal_shape_hw_out = find_optimal_celestial_wcs(
            inputs_for_optimal_wcs_calc, # This is now a list of (shape_hw, WCS) tuples
            resolution=target_resolution_angle, 
            auto_rotate=True, 
            projection='TAN', 
            reference=None, 
            frame='icrs'
        )
        
        if optimal_wcs_out and optimal_shape_hw_out:
            expected_pixel_shape_wh_for_wcs_out = (optimal_shape_hw_out[1], optimal_shape_hw_out[0])
            if optimal_wcs_out.pixel_shape is None or optimal_wcs_out.pixel_shape != expected_pixel_shape_wh_for_wcs_out:
                try: optimal_wcs_out.pixel_shape = expected_pixel_shape_wh_for_wcs_out
                except Exception: pass
            if not (hasattr(optimal_wcs_out.wcs, 'naxis1') and hasattr(optimal_wcs_out.wcs, 'naxis2')) or not (optimal_wcs_out.wcs.naxis1 > 0 and optimal_wcs_out.wcs.naxis2 > 0) :
                try: optimal_wcs_out.wcs.naxis1 = expected_pixel_shape_wh_for_wcs_out[0]; optimal_wcs_out.wcs.naxis2 = expected_pixel_shape_wh_for_wcs_out[1]
                except Exception: pass
        
        _log_and_callback("calcgrid_info_optimal_grid_calculated", shape=optimal_shape_hw_out, crval=optimal_wcs_out.wcs.crval if optimal_wcs_out and optimal_wcs_out.wcs else 'N/A', level="INFO", callback=progress_callback)
        return optimal_wcs_out, optimal_shape_hw_out
    except ImportError: _log_and_callback("calcgrid_error_find_optimal_wcs_unavailable", level="ERROR", callback=progress_callback); return None, None
    except Exception as e_optimal_wcs_call: 
        _log_and_callback("calcgrid_error_find_optimal_wcs_call", error=str(e_optimal_wcs_call), level="ERROR", callback=progress_callback)
        logger.error("Traceback find_optimal_celestial_wcs:", exc_info=True)
        return None, None


def cluster_seestar_stacks(all_raw_files_with_info: list, stack_threshold_deg: float, progress_callback: callable):
    """Group raw files captured by the Seestar based on their WCS position."""

    if not (ASTROPY_AVAILABLE and SkyCoord and u):
        _log_and_callback("clusterstacks_error_astropy_unavailable", level="ERROR", callback=progress_callback)
        return []

    if not all_raw_files_with_info:
        _log_and_callback("clusterstacks_warn_no_raw_info", level="WARN", callback=progress_callback)
        return []

    _log_and_callback(
        "clusterstacks_info_start",
        num_files=len(all_raw_files_with_info),
        threshold=stack_threshold_deg,
        level="INFO",
        callback=progress_callback,
    )

    panel_centers_sky = []
    panel_data_for_clustering = []

    for i, info in enumerate(all_raw_files_with_info):
        wcs_obj = info["wcs"]
        if not (wcs_obj and wcs_obj.is_celestial):
            continue
        try:
            if wcs_obj.pixel_shape:
                center_world = wcs_obj.pixel_to_world(
                    wcs_obj.pixel_shape[0] / 2.0,
                    wcs_obj.pixel_shape[1] / 2.0,
                )
            elif hasattr(wcs_obj.wcs, "crval"):
                center_world = SkyCoord(
                    ra=wcs_obj.wcs.crval[0] * u.deg,
                    dec=wcs_obj.wcs.crval[1] * u.deg,
                    frame="icrs",
                )
            else:
                continue
            panel_centers_sky.append(center_world)
            panel_data_for_clustering.append(info)
        except Exception:
            continue

    if not panel_centers_sky:
        _log_and_callback("clusterstacks_warn_no_centers", level="WARN", callback=progress_callback)
        return []

    groups = []
    assigned_mask = [False] * len(panel_centers_sky)

    for i in range(len(panel_centers_sky)):
        if assigned_mask[i]:
            continue
        current_group_infos = [panel_data_for_clustering[i]]
        assigned_mask[i] = True
        current_group_center_seed = panel_centers_sky[i]
        for j in range(i + 1, len(panel_centers_sky)):
            if assigned_mask[j]:
                continue
            if current_group_center_seed.separation(panel_centers_sky[j]).deg < stack_threshold_deg:
                current_group_infos.append(panel_data_for_clustering[j])
                assigned_mask[j] = True
        groups.append(current_group_infos)

    _log_and_callback("clusterstacks_info_finished", num_groups=len(groups), level="INFO", callback=progress_callback)
    return groups

def get_wcs_and_pretreat_raw_file(
    file_path: str,
    astap_exe_path: str,
    astap_data_dir: str,
    astap_search_radius: float,
    astap_downsample: int,
    astap_sensitivity: int,
    astap_timeout_seconds: int,
    progress_callback: callable,
    hotpix_mask_dir: str | None = None,
    solver_settings: dict | None = None,
):
    filename = _safe_basename(file_path)
    # Utiliser une fonction helper pour les logs internes à cette fonction si _log_and_callback
    # est trop lié à la structure de run_hierarchical_mosaic
    _pcb_local = lambda msg_key, lvl="DEBUG", **kwargs: \
        progress_callback(msg_key, None, lvl, **kwargs) if progress_callback else print(f"GETWCS_LOG {lvl}: {msg_key} {kwargs}")

    try:
        if is_path_excluded(file_path, EXCLUDED_DIRS):
            logger.debug("Skip excluded path: %s", file_path)
            return None, None, None, None
    except Exception:
        if UNALIGNED_DIRNAME in _normpath_parts(file_path):
            logger.debug("Skip excluded path: %s", file_path)
            return None, None, None, None

    if solver_settings is None:
        solver_settings = {}
    elif not isinstance(solver_settings, dict):
        try:
            solver_settings = dict(solver_settings)
        except Exception:
            solver_settings = getattr(solver_settings, "__dict__", {}) or {}
            if not isinstance(solver_settings, dict):
                solver_settings = {}

    header_precheck = None
    preexisting_wcs_flag = False
    preexisting_wcs_failure_reason = None
    if ASTROPY_AVAILABLE and fits is not None:
        try:
            with fits.open(file_path, mode="readonly", memmap=False) as hdul_hdr:
                header_precheck = hdul_hdr[0].header.copy()
        except Exception:
            header_precheck = None
        else:
            if hasattr(zemosaic_utils, "validate_wcs_header"):
                try:
                    preexisting_wcs_flag, _, failure_reason = zemosaic_utils.validate_wcs_header(header_precheck)
                except Exception as exc_validate:
                    preexisting_wcs_flag = False
                    failure_reason = f"validate_exception: {exc_validate}"
                if not preexisting_wcs_flag:
                    preexisting_wcs_failure_reason = failure_reason
            elif hasattr(zemosaic_utils, "has_valid_wcs"):
                try:
                    preexisting_wcs_flag = bool(zemosaic_utils.has_valid_wcs(header_precheck))
                except Exception:
                    preexisting_wcs_flag = False

    # Charger configuration pour options de prétraitement (si disponible)
    _cfg_pre = {}
    try:
        if ZEMOSAIC_CONFIG_AVAILABLE and zemosaic_config:
            _cfg_pre = zemosaic_config.load_config() or {}
    except Exception:
        _cfg_pre = {}
    _bg_gpu_enabled = bool(_cfg_pre.get("preprocess_remove_background_gpu", False))
    _bg_sigma = float(_cfg_pre.get("preprocess_background_sigma", 24.0))
    force_resolve_existing_wcs_cfg = bool(_cfg_pre.get("force_resolve_existing_wcs", False))
    force_resolve_existing_wcs = bool(
        solver_settings.get("force_resolve_existing_wcs", force_resolve_existing_wcs_cfg)
    )
    try:
        affine_offset_limit_adu = float(solver_settings.get("intertile_offset_limit_adu", 50.0))
    except Exception:
        affine_offset_limit_adu = 50.0
    affine_offset_limit_adu = max(0.0, abs(affine_offset_limit_adu))
    gain_limits_cfg = solver_settings.get("intertile_gain_limits")
    if isinstance(gain_limits_cfg, (list, tuple)) and len(gain_limits_cfg) == 2:
        try:
            gain_limit_min = float(gain_limits_cfg[0])
            gain_limit_max = float(gain_limits_cfg[1])
        except Exception:
            gain_limit_min, gain_limit_max = 0.75, 1.25
    else:
        gain_limit_min, gain_limit_max = 0.75, 1.25
    if gain_limit_min > gain_limit_max:
        gain_limit_min, gain_limit_max = gain_limit_max, gain_limit_min

    _pcb_local(f"GetWCS_Pretreat: Début pour '{filename}'.", lvl="DEBUG_DETAIL") # Niveau DEBUG_DETAIL pour être moins verbeux

    hp_mask_path = None

    if not (ZEMOSAIC_UTILS_AVAILABLE and zemosaic_utils):
        _pcb_local("getwcs_error_utils_unavailable", lvl="ERROR")
        return None, None, None, None
        
    res_load = zemosaic_utils.load_and_validate_fits(
        file_path,
        normalize_to_float32=False,
        attempt_fix_nonfinite=True,
        progress_callback=progress_callback,
    )
    if isinstance(res_load, tuple):
        img_data_raw_adu = res_load[0]
        header_orig = res_load[1] if len(res_load) > 1 else None
    else:
        img_data_raw_adu = res_load
        header_orig = None

    if header_orig is None and header_precheck is not None:
        try:
            header_orig = header_precheck.copy()
        except Exception:
            header_orig = copy.deepcopy(header_precheck)

    if img_data_raw_adu is None or header_orig is None:
        _pcb_local("getwcs_error_load_failed", lvl="ERROR", filename=filename)
        # Le fichier n'a pas pu être chargé, on ne peut pas le déplacer car on ne sait pas s'il existe ou est corrompu.
        # Ou on pourrait essayer de le déplacer s'il existe. Pour l'instant, on retourne None.
        return None, None, None, None

    # ... (log de post-load) ...
    _pcb_local(f"  Post-Load: '{filename}' - Shape: {img_data_raw_adu.shape}, Dtype: {img_data_raw_adu.dtype}", lvl="DEBUG_VERY_DETAIL")

    img_data_processed_adu = img_data_raw_adu.astype(np.float32, copy=True)

    # --- Débayerisation ---
    if img_data_processed_adu.ndim == 2:
        _pcb_local(f"  Monochrome détecté pour '{filename}'. Débayerisation...", lvl="DEBUG_DETAIL")
        bayer_pattern = header_orig.get('BAYERPAT', header_orig.get('CFAIMAGE', 'GRBG'))
        if not isinstance(bayer_pattern, str) or bayer_pattern.upper() not in ['GRBG','RGGB','GBRG','BGGR']: bayer_pattern = 'GRBG'
        else: bayer_pattern = bayer_pattern.upper()
        
        bitpix = header_orig.get('BITPIX', 16)
        # ... (logique de max_val_for_norm_before_debayer inchangée) ...
        max_val_for_norm_before_debayer = (2**abs(bitpix))-1. if bitpix!=0 and np.issubdtype(img_data_processed_adu.dtype,np.integer) else (65535. if np.issubdtype(img_data_processed_adu.dtype,np.unsignedinteger) else 1.)
        if abs(bitpix)>16 and np.issubdtype(img_data_processed_adu.dtype,np.integer): max_val_for_norm_before_debayer=(2**16)-1.
        if max_val_for_norm_before_debayer<=0: max_val_for_norm_before_debayer=1.

        img_norm_for_debayer = np.zeros_like(img_data_processed_adu,dtype=np.float32)
        min_adu_pre_debayer,max_adu_pre_debayer=np.nanmin(img_data_processed_adu),np.nanmax(img_data_processed_adu)
        range_adu_pre_debayer=max_adu_pre_debayer-min_adu_pre_debayer
        if range_adu_pre_debayer>1e-9: img_norm_for_debayer=(img_data_processed_adu-min_adu_pre_debayer)/range_adu_pre_debayer
        elif np.any(np.isfinite(img_data_processed_adu)): img_norm_for_debayer=np.full_like(img_data_processed_adu,0.5)
        img_norm_for_debayer=np.clip(img_norm_for_debayer,0.,1.)
        
        try:
            img_rgb_norm_01 = zemosaic_utils.debayer_image(img_norm_for_debayer, bayer_pattern, progress_callback=progress_callback)
            if range_adu_pre_debayer>1e-9: img_data_processed_adu=(img_rgb_norm_01*range_adu_pre_debayer)+min_adu_pre_debayer
            else: img_data_processed_adu=np.full_like(img_rgb_norm_01,min_adu_pre_debayer if np.isfinite(min_adu_pre_debayer) else 0.)
        except Exception as e_debayer: 
            _pcb_local("getwcs_warn_debayer_failed", lvl="WARN", filename=filename, error=str(e_debayer))
            img_data_processed_adu = np.stack([img_data_processed_adu]*3, axis=-1) # Fallback stack
    
    if img_data_processed_adu.ndim == 2: # Toujours monochrome après tentative de débayerisation
        _pcb_local("getwcs_warn_still_2d_after_debayer_attempt", lvl="WARN", filename=filename)
        img_data_processed_adu = np.stack([img_data_processed_adu]*3, axis=-1)
    
    if img_data_processed_adu.ndim != 3 or img_data_processed_adu.shape[-1] != 3:
        _pcb_local("getwcs_error_shape_after_debayer_final_check", lvl="ERROR", filename=filename, shape=str(img_data_processed_adu.shape))
        return None, None, None, None

    # --- Correction Hot Pixels + optional GPU background smoothing ---
    _pcb_local(f"  Correction HP pour '{filename}'...", lvl="DEBUG_DETAIL")
    if hotpix_mask_dir:
        hotpix_path = Path(hotpix_mask_dir).expanduser()
        hotpix_path.mkdir(parents=True, exist_ok=True)
        hp_stem = Path(filename).stem
        hp_mask_path = str(hotpix_path / f"hp_mask_{hp_stem}_{uuid.uuid4().hex}.npy")

    img_data_hp_corrected_adu = None
    try:
        # Prefer GPU hot-pixel correction when available
        if hasattr(zemosaic_utils, 'detect_and_correct_hot_pixels_gpu') and zemosaic_utils.gpu_is_available():
            img_data_hp_corrected_adu = zemosaic_utils.detect_and_correct_hot_pixels_gpu(
                img_data_processed_adu,
                threshold=3.0,
                neighborhood_size=5,
                progress_callback=progress_callback,
            )
        else:
            raise RuntimeError('GPU HP not available')
    except Exception:
        if 'save_mask_path' in zemosaic_utils.detect_and_correct_hot_pixels.__code__.co_varnames:
            img_data_hp_corrected_adu = zemosaic_utils.detect_and_correct_hot_pixels(
                img_data_processed_adu,
                3.0,
                5,
                progress_callback=progress_callback,
                save_mask_path=hp_mask_path,
            )
        else:
            img_data_hp_corrected_adu = zemosaic_utils.detect_and_correct_hot_pixels(
                img_data_processed_adu,
                3.0,
                5,
                progress_callback=progress_callback,
            )

    if img_data_hp_corrected_adu is not None:
        img_data_processed_adu = img_data_hp_corrected_adu
    else:
        _pcb_local("getwcs_warn_hp_returned_none_using_previous", lvl="WARN", filename=filename)

    # Optional GPU background smoothing (stabilize inter-batch photometry)
    # IMPORTANT: remove only the low-frequency GRADIENT (bg - median(bg)) to avoid truncating
    # histogram at zero and avoid dark rings around stars. Do NOT hard-clip to 0 here.
    try:
        if _bg_gpu_enabled and hasattr(zemosaic_utils, 'estimate_background_map_gpu') and zemosaic_utils.gpu_is_available():
            bg = zemosaic_utils.estimate_background_map_gpu(img_data_processed_adu, method='gaussian', sigma=_bg_sigma)
            if bg is not None and np.any(np.isfinite(bg)):
                # Use luminance gradient so the subtraction is achromatic
                if bg.ndim == 3 and bg.shape[-1] == 3:
                    lum_bg = 0.299 * bg[..., 0].astype(np.float32) + 0.587 * bg[..., 1].astype(np.float32) + 0.114 * bg[..., 2].astype(np.float32)
                else:
                    lum_bg = bg.astype(np.float32)
                med_lum = np.nanmedian(lum_bg) if np.any(np.isfinite(lum_bg)) else 0.0
                grad = (lum_bg - med_lum).astype(np.float32)
                if img_data_processed_adu.ndim == 3 and img_data_processed_adu.shape[-1] == 3:
                    for c in range(3):
                        img_data_processed_adu[..., c] = img_data_processed_adu[..., c].astype(np.float32) - grad
                else:
                    img_data_processed_adu = img_data_processed_adu.astype(np.float32) - grad
                _pcb_local("  Background luminance gradient removed (achromatic), no hard clipping.", lvl="DEBUG_DETAIL")
    except Exception:
        pass

    header_for_wcs_check = header_orig if header_orig is not None else header_precheck
    skip_solver_due_to_existing_wcs = False
    wcs_validation_reason = None
    preexisting_wcs_obj = None
    if header_for_wcs_check is not None and hasattr(zemosaic_utils, "validate_wcs_header"):
        try:
            valid_wcs, candidate_wcs, failure_reason = zemosaic_utils.validate_wcs_header(header_for_wcs_check)
        except Exception as exc_validate_hdr:
            valid_wcs, candidate_wcs, failure_reason = False, None, f"validate_exception: {exc_validate_hdr}"
        skip_solver_due_to_existing_wcs = bool(valid_wcs)
        if skip_solver_due_to_existing_wcs:
            preexisting_wcs_obj = candidate_wcs
        else:
            wcs_validation_reason = failure_reason
    elif header_for_wcs_check is not None and hasattr(zemosaic_utils, "has_valid_wcs"):
        try:
            skip_solver_due_to_existing_wcs = bool(zemosaic_utils.has_valid_wcs(header_for_wcs_check))
        except Exception:
            skip_solver_due_to_existing_wcs = bool(preexisting_wcs_flag)
    else:
        skip_solver_due_to_existing_wcs = bool(preexisting_wcs_flag)

    if skip_solver_due_to_existing_wcs and preexisting_wcs_obj is None and header_for_wcs_check is not None and ASTROPY_AVAILABLE and WCS:
        try:
            candidate_wcs_hdr = WCS(header_for_wcs_check, naxis=2, relax=True)
            if getattr(candidate_wcs_hdr, "is_celestial", False):
                preexisting_wcs_obj = candidate_wcs_hdr
            else:
                skip_solver_due_to_existing_wcs = False
                if wcs_validation_reason is None:
                    wcs_validation_reason = "wcs_not_celestial"
        except Exception as e_wcs_hdr:
            skip_solver_due_to_existing_wcs = False
            if wcs_validation_reason is None:
                wcs_validation_reason = f"astropy_wcs_exception: {e_wcs_hdr}"
            _pcb_local("getwcs_warn_header_wcs_read_failed", lvl="WARN", filename=filename, error=str(e_wcs_hdr))
            logger.warning("Existing WCS header invalid for '%s': %s", filename, e_wcs_hdr)

    if not skip_solver_due_to_existing_wcs and wcs_validation_reason is None:
        wcs_validation_reason = preexisting_wcs_failure_reason

    if force_resolve_existing_wcs and skip_solver_due_to_existing_wcs:
        _pcb_local(
            "getwcs_info_force_resolve_existing_wcs",
            lvl="INFO",
            filename=filename,
        )
        logger.info("Force resolving existing WCS for '%s' due to configuration override.", filename)
        skip_solver_due_to_existing_wcs = False
        preexisting_wcs_obj = None

    if not skip_solver_due_to_existing_wcs and wcs_validation_reason:
        _pcb_local(
            "getwcs_info_existing_wcs_rejected",
            lvl="WARN",
            filename=filename,
            reason=wcs_validation_reason,
        )
        logger.warning("Existing WCS for '%s' rejected: %s", filename, wcs_validation_reason)

    # --- Résolution WCS ---
    _pcb_local(f"  Résolution WCS pour '{filename}'...", lvl="DEBUG_DETAIL")
    wcs_brute = preexisting_wcs_obj if preexisting_wcs_obj is not None else None
    # Évite d'écrire le header FITS si le WCS est déjà présent dans le fichier d'origine.
    # Nous ne réécrivons le header que si un solver externe (ASTAP/ASTROMETRY/ANSVR)
    # a effectivement injecté/ajusté des clés WCS dans header_orig.
    should_write_header_back = False
    if preexisting_wcs_obj is not None:
        skip_msg = f"Skip WCS solve for '{filename}' (WCS present)."
        _pcb_local(skip_msg, lvl="INFO")
        logger.info(skip_msg)
    if wcs_brute is None and ASTROPY_AVAILABLE and WCS: # S'assurer que WCS est bien l'objet d'Astropy
        try:
            wcs_from_header = WCS(header_orig, naxis=2, relax=True) # Utiliser WCS d'Astropy
            if wcs_from_header.is_celestial and hasattr(wcs_from_header.wcs,'crval') and \
               (hasattr(wcs_from_header.wcs,'cdelt') or hasattr(wcs_from_header.wcs,'cd') or hasattr(wcs_from_header.wcs,'pc')):
                wcs_brute = wcs_from_header
                _pcb_local(f"    WCS trouvé dans header FITS de '{filename}'.", lvl="DEBUG_DETAIL")
                skip_msg = f"Skip WCS solve for '{filename}' (WCS present)."
                _pcb_local(skip_msg, lvl="INFO")
                logger.info(skip_msg)
                # WCS déjà présent => pas besoin de réécrire le header
                should_write_header_back = False
        except Exception as e_wcs_hdr:
            _pcb_local("getwcs_warn_header_wcs_read_failed", lvl="WARN", filename=filename, error=str(e_wcs_hdr))
            wcs_brute = None
            
    solver_choice_effective = (solver_settings or {}).get("solver_choice", "ASTAP")
    api_key_len = len((solver_settings or {}).get("api_key", ""))
    _pcb_local(
        f"Solver choice effective={solver_choice_effective}",
        lvl="DEBUG_DETAIL",
    )
    if wcs_brute is None and ZEMOSAIC_ASTROMETRY_AVAILABLE and zemosaic_astrometry:
        try:
            # Utiliser directement le fichier original sans conversion mono ni FITS minimal
            input_for_solver = file_path

            if solver_choice_effective == "ASTROMETRY":
                _pcb_local("GetWCS: using ASTROMETRY", lvl="DEBUG")
                wcs_brute = solve_with_astrometry(
                    input_for_solver,
                    header_orig,
                    solver_settings or {},
                    progress_callback,
                )
                if not wcs_brute and astap_paths_valid(astap_exe_path, astap_data_dir):
                    _pcb_local("Astrometry failed; fallback to ASTAP", lvl="INFO")
                    _pcb_local("GetWCS: using ASTAP (fallback)", lvl="DEBUG")
                    wcs_brute = zemosaic_astrometry.solve_with_astap(
                        image_fits_path=input_for_solver,
                        original_fits_header=header_orig,
                        astap_exe_path=astap_exe_path,
                        astap_data_dir=astap_data_dir,
                        search_radius_deg=astap_search_radius,
                        downsample_factor=astap_downsample,
                        sensitivity=astap_sensitivity,
                        timeout_sec=astap_timeout_seconds,
                        update_original_header_in_place=True,
                        progress_callback=progress_callback,
                    )
                # Si un solver a réussi, le header_orig a potentiellement été mis à jour
                if wcs_brute:
                    should_write_header_back = True
                if wcs_brute:
                    _pcb_local("getwcs_info_astrometry_solved", lvl="INFO_DETAIL", filename=filename)
            elif solver_choice_effective == "ANSVR":
                _pcb_local("GetWCS: using ANSVR", lvl="DEBUG")
                wcs_brute = solve_with_ansvr(
                    input_for_solver,
                    header_orig,
                    solver_settings or {},
                    progress_callback,
                )
                if not wcs_brute and astap_paths_valid(astap_exe_path, astap_data_dir):
                    _pcb_local("Ansvr failed; fallback to ASTAP", lvl="INFO")
                    _pcb_local("GetWCS: using ASTAP (fallback)", lvl="DEBUG")
                    wcs_brute = zemosaic_astrometry.solve_with_astap(
                        image_fits_path=input_for_solver,
                        original_fits_header=header_orig,
                        astap_exe_path=astap_exe_path,
                        astap_data_dir=astap_data_dir,
                        search_radius_deg=astap_search_radius,
                        downsample_factor=astap_downsample,
                        sensitivity=astap_sensitivity,
                        timeout_sec=astap_timeout_seconds,
                        update_original_header_in_place=True,
                        progress_callback=progress_callback,
                    )
                # Si ANSVR/ASTAP réussit, le header a été mis à jour par le solver
                if wcs_brute:
                    should_write_header_back = True
                if wcs_brute:
                    _pcb_local("getwcs_info_astrometry_solved", lvl="INFO_DETAIL", filename=filename)
            else:
                _pcb_local("GetWCS: using ASTAP", lvl="DEBUG")
                wcs_brute = zemosaic_astrometry.solve_with_astap(
                    image_fits_path=input_for_solver,
                    original_fits_header=header_orig,
                    astap_exe_path=astap_exe_path,
                    astap_data_dir=astap_data_dir,
                    search_radius_deg=astap_search_radius,
                    downsample_factor=astap_downsample,
                    sensitivity=astap_sensitivity,
                    timeout_sec=astap_timeout_seconds,
                    update_original_header_in_place=True,
                    progress_callback=progress_callback,
                )
                # ASTAP a potentiellement mis à jour le header_orig
                if wcs_brute:
                    should_write_header_back = True
                if wcs_brute:
                    _pcb_local("getwcs_info_astap_solved", lvl="INFO_DETAIL", filename=filename)
                else:
                    _pcb_local("getwcs_warn_astap_failed", lvl="WARN", filename=filename)
        except Exception as e_solver_call:
            _pcb_local("getwcs_error_astap_exception", lvl="ERROR", filename=filename, error=str(e_solver_call))
            logger.error(f"Erreur solver pour {filename}", exc_info=True)
            wcs_brute = None
        finally:
            del img_data_raw_adu
            gc.collect()
    elif wcs_brute is None: # Ni header, ni ASTAP n'a fonctionné ou n'était dispo
        _pcb_local("getwcs_warn_no_wcs_source_available_or_failed", lvl="WARN", filename=filename)
        # Action de déplacement sera gérée par le check suivant

    # --- Vérification finale du WCS et action de déplacement si échec ---
    if wcs_brute and wcs_brute.is_celestial:
        # Mettre à jour pixel_shape si nécessaire
        if wcs_brute.pixel_shape is None or not (wcs_brute.pixel_shape[0]>0 and wcs_brute.pixel_shape[1]>0):
            n1_final = header_orig.get('NAXIS1', img_data_processed_adu.shape[1])
            n2_final = header_orig.get('NAXIS2', img_data_processed_adu.shape[0])
            if n1_final > 0 and n2_final > 0:
                try: wcs_brute.pixel_shape = (int(n1_final), int(n2_final))
                except Exception as e_ps_final: 
                    _pcb_local("getwcs_error_set_pixel_shape_final_wcs_invalid", lvl="ERROR", filename=filename, error=str(e_ps_final))
                    # WCS devient invalide ici
                    wcs_brute = None # Forcer le déplacement
            else:
                _pcb_local("getwcs_error_invalid_naxis_for_pixel_shape_wcs_invalid", lvl="ERROR", filename=filename)
                wcs_brute = None # Forcer le déplacement
        
        if wcs_brute and wcs_brute.is_celestial: # Re-vérifier après la tentative de set_pixel_shape
            _pcb_local("getwcs_info_pretreatment_wcs_ok", lvl="DEBUG", filename=filename)
            # Écriture du header uniquement si un solver a réellement mis à jour le header
            if should_write_header_back:
                _write_header_to_fits(file_path, header_orig, _pcb_local)
            return img_data_processed_adu, wcs_brute, header_orig, hp_mask_path
        # else: tombe dans le bloc de déplacement ci-dessous

    # Si on arrive ici, c'est que wcs_brute est None ou non céleste
    _pcb_local("getwcs_action_moving_unsolved_file", lvl="WARN", filename=filename)
    status, destination_path = _move_to_unaligned_safe(
        file_path,
        Path(file_path).expanduser().parent,
        logger=logger,
    )

    if status == "moved" and destination_path is not None:
        _pcb_local(
            f"  Fichier '{filename}' déplacé vers '{destination_path.parent}'.",
            lvl="INFO",
        )
    elif status in {"already_moved", "missing"}:
        _pcb_local(
            f"  Fichier '{filename}' déjà déplacé ou introuvable (course détectée).",
            lvl="DEBUG_DETAIL",
        )
    elif status == "skipped_excluded":
        _pcb_local(
            f"  Fichier '{filename}' déjà dans un dossier exclu, déplacement ignoré.",
            lvl="WARN",
        )
    elif status in {"conflict", "failed"}:
        _pcb_local(
            "getwcs_error_moving_unaligned_file",
            lvl="ERROR",
            filename=filename,
            error=f"status={status}",
        )

    if img_data_processed_adu is not None: del img_data_processed_adu
    gc.collect()
    return None, None, None, None








# Dans zemosaic_worker.py

# ... (vos imports existants : os, shutil, time, traceback, gc, logging, np, astropy, reproject, et les modules zemosaic_...)

def _safe_load_cache(path: str, *, pcb: Callable | None = None, tile_id: int | None = None):
    """Load a numpy cache with a WinError-1455 aware fallback.

    Attempts a memory-mapped load first (mmap_mode='r'). If an OSError
    containing WinError 1455 is raised, retries without memmap. Any other
    exception is re-raised.
    Returns the loaded numpy array.
    """
    try:
        return np.load(path, allow_pickle=False, mmap_mode="r")
    except OSError as exc:
        # Detect Windows paging file insufficiency
        msg = str(exc)
        if "WinError 1455" in msg or "1455" in msg or (hasattr(exc, "winerror") and getattr(exc, "winerror") == 1455):
            try:
                if pcb is not None:
                    try:
                        pcb("stack_mem_fallback_memmap_to_ram", prog=None, lvl="WARN", tile_id=tile_id, path=_safe_basename(path))
                    except Exception:
                        pass
                # Retry without memmap
                return np.load(path, allow_pickle=False, mmap_mode=None)
            except Exception:
                raise
        # Other OSError: re-raise so caller can handle
        raise


def _stack_master_tile_cpu(
    aligned_images_for_stack: list,
    *,
    stack_norm_method: str,
    stack_weight_method: str,
    stack_reject_algo: str,
    stack_kappa_low: float,
    stack_kappa_high: float,
    parsed_winsor_limits: tuple[float, float],
    stack_final_combine: str,
    apply_radial_weight: bool,
    radial_feather_fraction: float,
    radial_shape_power: float,
    winsor_pool_workers: int | None,
    winsor_max_frames_per_pass: int | None,
    progress_callback: callable,
    zconfig: SimpleNamespace,
    parallel_plan: ParallelPlan | None,
    pcb_tile: callable,
    func_id_log_base: str,
    tile_id: int,
) -> tuple[np.ndarray | None, dict[str, Any]]:
    """Stack aligned images using the existing CPU path.

    Returns stacked data and the accompanying stack metadata.
    """

    stack_metadata: dict[str, Any] = {}

    current_parallel_plan = parallel_plan or getattr(zconfig, "parallel_plan", None)
    effective_winsor_frames_per_pass = int(winsor_max_frames_per_pass) if winsor_max_frames_per_pass is not None else 0
    if effective_winsor_frames_per_pass < 0:
        effective_winsor_frames_per_pass = 0
    try:
        sample_frame = aligned_images_for_stack[0] if aligned_images_for_stack else None
        if sample_frame is not None:
            per_frame_bytes = int(np.asarray(sample_frame).nbytes)
            available_bytes = int(psutil.virtual_memory().available)
            overhead = 3.2
            target_fraction = 0.55
            min_pass = max(1, int(getattr(zconfig, "winsor_min_frames_per_pass", 2)))
            if per_frame_bytes > 0:
                preemptive_limit = max(
                    min_pass,
                    int((available_bytes * target_fraction) // max(1, int(per_frame_bytes * overhead))),
                )
                if preemptive_limit < len(aligned_images_for_stack):
                    if effective_winsor_frames_per_pass <= 0 or preemptive_limit < effective_winsor_frames_per_pass:
                        effective_winsor_frames_per_pass = preemptive_limit
                    try:
                        setattr(zconfig, "stack_memmap_enabled", True)
                    except Exception:
                        pass
                    try:
                        pcb_tile(
                            "stack_mem_preemptive_stream",
                            prog=None,
                            lvl="INFO_DETAIL",
                            frames_per_pass=effective_winsor_frames_per_pass,
                            tile_id=tile_id,
                        )
                    except Exception:
                        pass
    except Exception:
        pass

    if stack_reject_algo == "winsorized_sigma_clip":
        master_tile_stacked_HWC, _ = zemosaic_align_stack.stack_winsorized_sigma_clip(
            aligned_images_for_stack,
            weight_method=stack_weight_method,
            zconfig=zconfig,
            kappa=stack_kappa_low,
            winsor_limits=parsed_winsor_limits,
            apply_rewinsor=True,
            winsor_max_frames_per_pass=effective_winsor_frames_per_pass,
            winsor_max_workers=int(winsor_pool_workers) if winsor_pool_workers is not None else 1,
            stack_metadata=stack_metadata,
            parallel_plan=current_parallel_plan,
        )
    elif stack_reject_algo == "kappa_sigma":
        master_tile_stacked_HWC, _ = zemosaic_align_stack.stack_kappa_sigma_clip(
            aligned_images_for_stack,
            weight_method=stack_weight_method,
            zconfig=zconfig,
            sigma_low=stack_kappa_low,
            sigma_high=stack_kappa_high,
            stack_metadata=stack_metadata,
            parallel_plan=current_parallel_plan,
        )
    elif stack_reject_algo == "linear_fit_clip":
        master_tile_stacked_HWC, _ = zemosaic_align_stack.stack_linear_fit_clip(
            aligned_images_for_stack,
            weight_method=stack_weight_method,
            zconfig=zconfig,
            sigma=stack_kappa_high,
            stack_metadata=stack_metadata,
            parallel_plan=current_parallel_plan,
        )
    else:
        master_tile_stacked_HWC = zemosaic_align_stack.stack_aligned_images(
            aligned_image_data_list=aligned_images_for_stack,
            normalize_method=stack_norm_method,
            weighting_method=stack_weight_method,
            rejection_algorithm=stack_reject_algo,
            final_combine_method=stack_final_combine,
            sigma_clip_low=stack_kappa_low,
            sigma_clip_high=stack_kappa_high,
            winsor_limits=parsed_winsor_limits,
            minimum_signal_adu_target=0.0,
            apply_radial_weight=apply_radial_weight,
            radial_feather_fraction=radial_feather_fraction,
            radial_shape_power=radial_shape_power,
            winsor_max_workers=winsor_pool_workers,
            progress_callback=progress_callback,
            zconfig=zconfig,
            stack_metadata=stack_metadata,
            parallel_plan=current_parallel_plan,
        )

    del aligned_images_for_stack
    gc.collect()

    return master_tile_stacked_HWC, stack_metadata


def _phase3_gpu_candidate(parallel_plan: ParallelPlan | None, logger: logging.Logger | None) -> bool:
    if _P3_GPU_STATE["hard_disabled"]:
        return False
    if not _P3_GPU_STATE.get("allowed", True):
        return False
    if not _P3_GPU_HELPERS_AVAILABLE:
        return False
    try:
        if parallel_plan is not None and not getattr(parallel_plan, "use_gpu", True):
            return False
    except Exception:
        pass
    if not _P3_GPU_STATE["health_checked"]:
        healthy = bool(_p3_gpu_is_usable(logger))
        _P3_GPU_STATE["healthy"] = healthy
        _P3_GPU_STATE["health_checked"] = True
    return _P3_GPU_STATE["healthy"]


def _is_gpu_oom_error(exc: Exception) -> bool:
    try:
        import cupy
        from cupy.cuda import memory

        if isinstance(exc, memory.OutOfMemoryError):
            return True
    except Exception:
        pass
    try:
        return "out of memory" in str(exc).lower()
    except Exception:
        return False


def _shrink_parallel_plan_for_gpu(parallel_plan: ParallelPlan | None) -> ParallelPlan | None:
    if parallel_plan is None or ParallelPlan is None:
        return parallel_plan
    try:
        current_bytes = getattr(parallel_plan, "gpu_max_chunk_bytes", None)
        if current_bytes is None or current_bytes <= 0:
            current_bytes = getattr(parallel_plan, "max_chunk_bytes", None)
        if current_bytes is None or current_bytes <= 0:
            current_bytes = 512 * 1024 * 1024
        tightened_bytes = max(32 * 1024 * 1024, int(current_bytes * 0.5))
        current_rows = getattr(parallel_plan, "gpu_rows_per_chunk", None)
        tightened_rows = max(1, int(math.ceil(current_rows / 2))) if current_rows else current_rows
        return replace(parallel_plan, gpu_max_chunk_bytes=tightened_bytes, gpu_rows_per_chunk=tightened_rows)
    except Exception:
        return parallel_plan


def _stack_master_tile_auto(
    image_descriptors: list,
    *,
    stack_norm_method: str,
    stack_weight_method: str,
    stack_reject_algo: str,
    stack_kappa_low: float,
    stack_kappa_high: float,
    parsed_winsor_limits: tuple[float, float],
    stack_final_combine: str,
    poststack_equalize_rgb: bool,
    apply_radial_weight: bool,
    radial_feather_fraction: float,
    radial_shape_power: float,
    winsor_pool_workers: int | None,
    winsor_max_frames_per_pass: int | None,
    progress_callback: callable,
    zconfig: SimpleNamespace,
    parallel_plan: ParallelPlan | None,
    pcb_tile: callable,
    func_id_log_base: str,
    tile_id: int,
    logger: logging.Logger | None,
) -> tuple[np.ndarray | None, dict[str, Any], bool]:
    stacking_params = {
        "stack_norm_method": stack_norm_method,
        "stack_weight_method": stack_weight_method,
        "stack_reject_algo": stack_reject_algo,
        "stack_kappa_low": stack_kappa_low,
        "stack_kappa_high": stack_kappa_high,
        "parsed_winsor_limits": parsed_winsor_limits,
        "stack_final_combine": stack_final_combine,
        "poststack_equalize_rgb": poststack_equalize_rgb,
        "apply_radial_weight": apply_radial_weight,
        "radial_feather_fraction": radial_feather_fraction,
        "radial_shape_power": radial_shape_power,
        "winsor_max_frames_per_pass": winsor_max_frames_per_pass,
    }

    use_gpu_candidate = _phase3_gpu_candidate(parallel_plan, logger)
    retry_parallel_plan = parallel_plan
    if use_gpu_candidate and _p3_gpu_stack_from_paths is not None:
        if logger and not _P3_GPU_STATE.get("info_logged"):
            try:
                logger.info("[P3][GPU] Phase 3 GPU auto mode enabled (mode=C, candidate=True)")
            except Exception:
                pass
            _P3_GPU_STATE["info_logged"] = True
        try:
            stacked_gpu, meta_gpu = _p3_gpu_stack_from_paths(
                image_descriptors,
                stacking_params,
                parallel_plan=parallel_plan,
                logger=logger,
                pcb_tile=pcb_tile,
                zconfig=zconfig,
            )
            _P3_GPU_STATE["healthy"] = True
            return stacked_gpu, meta_gpu, True
        except Exception as exc:
            if isinstance(exc, _P3GPUStackingError):
                if logger:
                    logger.warning(
                        "[P3][GPU] Stack failed (GPUStackingError): %s -- retrying once on GPU",
                        exc,
                    )
            else:
                if logger:
                    logger.warning(
                        "[P3][GPU] Unexpected GPU error: %s -- retrying once on GPU",
                        exc,
                        exc_info=True,
                    )
            if _is_gpu_oom_error(exc):
                try:
                    if ZEMOSAIC_UTILS_AVAILABLE and zemosaic_utils and hasattr(
                        zemosaic_utils, "free_cupy_memory_pools"
                    ):
                        zemosaic_utils.free_cupy_memory_pools()
                except Exception:
                    pass
                retry_parallel_plan = _shrink_parallel_plan_for_gpu(parallel_plan)
        try:
            stacked_gpu, meta_gpu = _p3_gpu_stack_from_paths(
                image_descriptors,
                stacking_params,
                parallel_plan=retry_parallel_plan,
                logger=logger,
                pcb_tile=pcb_tile,
                zconfig=zconfig,
            )
            _P3_GPU_STATE["healthy"] = True
            return stacked_gpu, meta_gpu, True
        except Exception as exc:
            if logger:
                logger.error(
                    "[P3][GPU] Second GPU attempt failed; disabling Phase 3 GPU for this run: %s",
                    exc,
                    exc_info=True,
                )
                try:
                    logger.warning(
                        "[P3][GPU] GPU disabled for remaining Phase 3 tiles after repeated failures."
                    )
                except Exception:
                    pass
            _P3_GPU_STATE["hard_disabled"] = True
            _P3_GPU_STATE["healthy"] = False

    stacked_cpu, meta_cpu = _stack_master_tile_cpu(
        image_descriptors,
        stack_norm_method=stack_norm_method,
        stack_weight_method=stack_weight_method,
        stack_reject_algo=stack_reject_algo,
        stack_kappa_low=stack_kappa_low,
        stack_kappa_high=stack_kappa_high,
        parsed_winsor_limits=parsed_winsor_limits,
        stack_final_combine=stack_final_combine,
        apply_radial_weight=apply_radial_weight,
        radial_feather_fraction=radial_feather_fraction,
        radial_shape_power=radial_shape_power,
        winsor_pool_workers=winsor_pool_workers,
        winsor_max_frames_per_pass=winsor_max_frames_per_pass,
        progress_callback=progress_callback,
        zconfig=zconfig,
        parallel_plan=parallel_plan,
        pcb_tile=pcb_tile,
        func_id_log_base=func_id_log_base,
        tile_id=tile_id,
    )
    return stacked_cpu, meta_cpu, False


def create_master_tile(
    seestar_stack_group_info: list[dict],
    tile_id: int,
    output_temp_dir: str,
    # Paramètres de stacking existants
    stack_norm_method: str,
    stack_weight_method: str, # Ex: "none", "noise_variance", "noise_fwhm", "noise_plus_fwhm"
    stack_reject_algo: str,
    stack_kappa_low: float,
    stack_kappa_high: float,
    parsed_winsor_limits: tuple[float, float],
    stack_final_combine: str,
    poststack_equalize_rgb: bool,
    # --- NOUVEAUX PARAMÈTRES POUR LA PONDÉRATION RADIALE ---
    apply_radial_weight: bool,             # Vient de la GUI/config
    radial_feather_fraction: float,      # Vient de la GUI/config
    radial_shape_power: float,           # Pourrait être une constante ou configurable
    min_radial_weight_floor: float,
    # --- FIN NOUVEAUX PARAMÈTRES ---
    quality_crop_enabled: bool,
    quality_crop_band_px: int,
    quality_crop_k_sigma: float,
    quality_crop_margin_px: int,
    quality_crop_min_run: int,
    altaz_cleanup_enabled: bool,
    altaz_margin_percent: float,
    altaz_decay: float,
    altaz_nanize: bool,
    quality_gate_enabled: bool,
    quality_gate_threshold: float,
    quality_gate_edge_band_px: int,
    quality_gate_k_sigma: float,
    quality_gate_erode_px: int,
    quality_gate_move_rejects: bool,
    # Paramètres ASTAP (pourraient être enlevés si plus du tout utilisés ici)
    astap_exe_path_global: str, 
    astap_data_dir_global: str, 
    astap_search_radius_global: float,
    astap_downsample_global: int,
    astap_sensitivity_global: int,
    astap_timeout_seconds_global: int,
    winsor_pool_workers: int,
    winsor_max_frames_per_pass: int,
    progress_callback: callable,
    resource_strategy: dict | None = None,
    center_out_context: CenterOutNormalizationContext | None = None,
    center_out_settings: dict | None = None,
    center_out_rank: int | None = None,
    parallel_plan: ParallelPlan | None = None,
    allow_batch_duplication: bool = True,
    target_stack_size: int = 5,
    min_safe_stack_size: int = 3,
    dbg_tile_ids: set[int] | None = None,
):
    """
    Crée une "master tuile" à partir d'un groupe d'images.
    Lit les données image prétraitées depuis un cache disque (.npy).
    Utilise les WCS et Headers déjà résolus et stockés en mémoire.
    Transmet toutes les options de stacking, y compris la pondération radiale.

    Returns
    -------
    tuple[tuple[str | None, object | None], list[list[dict]]]
        - ``(path, wcs)`` du master stack produit (``None`` si échec).
        - Liste de sous-groupes à retraiter (copie des ``raw_info`` pour les images non alignées).
    """
    pcb_tile = lambda msg_key, prog=None, lvl="INFO_DETAIL", **kwargs: _log_and_callback(msg_key, prog, lvl, callback=progress_callback, **kwargs)
    # Load persistent configuration to forward GPU preference
    if ZEMOSAIC_CONFIG_AVAILABLE and zemosaic_config:
        try:
            zconfig = SimpleNamespace(**zemosaic_config.load_config())
        except Exception:
            zconfig = SimpleNamespace()
    else:
        zconfig = SimpleNamespace()
    if parallel_plan is not None:
        setattr(zconfig, "parallel_plan", parallel_plan)

    debug_tile = bool(dbg_tile_ids) and tile_id in dbg_tile_ids

    # Ensure stacking GPU preference mirrors the Phase 5 GPU intent when not explicitly set.
    try:
        phase5_pref = bool(getattr(zconfig, "use_gpu_phase5"))
    except Exception:
        phase5_pref = False
    try:
        stack_pref = getattr(zconfig, "stack_use_gpu")
    except AttributeError:
        stack_pref = None
    except Exception:
        stack_pref = None
    if stack_pref is None:
        setattr(zconfig, "stack_use_gpu", phase5_pref)
    else:
        try:
            setattr(zconfig, "stack_use_gpu", bool(stack_pref))
        except Exception:
            setattr(zconfig, "stack_use_gpu", phase5_pref)
    if not hasattr(zconfig, "use_gpu_stack") or getattr(zconfig, "use_gpu_stack") is None:
        setattr(zconfig, "use_gpu_stack", getattr(zconfig, "stack_use_gpu", phase5_pref))
    try:
        setattr(zconfig, "poststack_equalize_rgb", bool(poststack_equalize_rgb))
    except Exception:
        pass
    # Do not alias Phase‑5 GPU flag onto a generic 'use_gpu' here.
    # Stacking code now honors only explicit stacking flags
    # (e.g., 'stack_use_gpu' / 'use_gpu_stack') or 'use_gpu' if set by user.
    if resource_strategy:
        try:
            if resource_strategy.get('gpu_batch_hint'):
                setattr(zconfig, 'gpu_batch_hint', int(resource_strategy.get('gpu_batch_hint')))
            if 'memmap' in resource_strategy:
                setattr(zconfig, 'stack_memmap_enabled', bool(resource_strategy.get('memmap')))
            if resource_strategy.get('memmap_budget_mb') is not None:
                setattr(zconfig, 'stack_memmap_budget_mb', resource_strategy.get('memmap_budget_mb'))
        except Exception:
            pass
        try:
            pcb_tile(
                f"{func_id_log_base}_autocaps_hint",
                prog=None,
                lvl="INFO_DETAIL",
                cap=resource_strategy.get('cap'),
                memmap=resource_strategy.get('memmap'),
                gpu_hint=resource_strategy.get('gpu_batch_hint'),
            )
        except Exception:
            pass
    func_id_log_base = "mastertile"
    # Detect and apply EXTREME_GROUP overrides for very large groups
    try:
        num_frames = int(len(seestar_stack_group_info) if seestar_stack_group_info is not None else 0)
    except Exception:
        num_frames = 0
    extreme_group_threshold = int(getattr(zconfig, "extreme_group_threshold", 1000))
    extreme_group_mode = num_frames >= extreme_group_threshold
    _extreme_group_originals = {}
    if extreme_group_mode:
        try:
            # Save originals (may be missing)
            _extreme_group_originals["stack_mem_preemptive_stream_enabled"] = getattr(zconfig, "stack_mem_preemptive_stream_enabled", None)
            _extreme_group_originals["stack_memmap_enabled"] = getattr(zconfig, "stack_memmap_enabled", None)
            _extreme_group_originals["winsor_max_frames_per_pass"] = getattr(zconfig, "winsor_max_frames_per_pass", None)
            # Apply aggressive overrides for the current tile only
            try:
                setattr(zconfig, "stack_mem_preemptive_stream_enabled", True)
            except Exception:
                pass
            try:
                setattr(zconfig, "stack_memmap_enabled", True)
            except Exception:
                pass
            # Cap winsor frames per pass to a conservative value (defaults to 600)
            winsor_cap = int(getattr(zconfig, "extreme_group_winsor_cap", 600))
            try:
                setattr(zconfig, "winsor_max_frames_per_pass", winsor_cap)
            except Exception:
                pass
            try:
                pcb_tile("mastertile_extreme_group", prog=None, lvl="WARN", tile_id=tile_id, num_frames=num_frames, reason="[EXTREME_GROUP]")
            except Exception:
                pass
        except Exception:
            _extreme_group_originals = {}

    pcb_tile(f"{func_id_log_base}_info_creation_started_from_cache", prog=None, lvl="INFO",
             num_raw=len(seestar_stack_group_info), tile_id=tile_id)
    failed_groups_to_retry: list[list[dict]] = []
    pcb_tile(
        f"    {func_id_log_base}_{tile_id}: Options Stacking - Norm='{stack_norm_method}', "
        f"Weight='{stack_weight_method}' (RadialWeight={apply_radial_weight}), "
        f"Reject='{stack_reject_algo}', Combine='{stack_final_combine}', RGBEqualize={poststack_equalize_rgb}",
        prog=None,
        lvl="DEBUG",
    )

    if not (ZEMOSAIC_UTILS_AVAILABLE and zemosaic_utils and ZEMOSAIC_ALIGN_STACK_AVAILABLE and zemosaic_align_stack and ASTROPY_AVAILABLE and fits): # Ajout de 'fits' pour header_mt_save
        # ... (votre gestion d'erreur de dépendances existante) ...
        if not ZEMOSAIC_UTILS_AVAILABLE: pcb_tile(f"{func_id_log_base}_error_utils_unavailable", prog=None, lvl="ERROR", tile_id=tile_id)
        if not ZEMOSAIC_ALIGN_STACK_AVAILABLE: pcb_tile(f"{func_id_log_base}_error_alignstack_unavailable", prog=None, lvl="ERROR", tile_id=tile_id)
        if not ASTROPY_AVAILABLE or not fits: pcb_tile(f"{func_id_log_base}_error_astropy_unavailable", prog=None, lvl="ERROR", tile_id=tile_id)
        return (None, None), failed_groups_to_retry
        
    if not seestar_stack_group_info:
        pcb_tile(f"{func_id_log_base}_error_no_images_provided", prog=None, lvl="ERROR", tile_id=tile_id)
        return (None, None), failed_groups_to_retry

    quality_crop_enabled_effective = bool(quality_crop_enabled)
    quality_gate_enabled_effective = bool(quality_gate_enabled)
    min_safe_stack_effective = max(1, int(min_safe_stack_size))
    target_stack_effective = max(min_safe_stack_effective, int(target_stack_size))

    original_batch_size = len(seestar_stack_group_info)
    if allow_batch_duplication and original_batch_size < target_stack_effective and original_batch_size > 0:
        repeat = math.ceil(target_stack_effective / original_batch_size)
        seestar_stack_group_info = (seestar_stack_group_info * repeat)[:target_stack_effective]
        pcb_tile(
            f"[Batch] Duplicating frames: original={original_batch_size} → final={len(seestar_stack_group_info)}",
            prog=None,
            lvl="INFO_DETAIL",
            tile_id=int(tile_id),
        )
        try:
            if logger:
                logger.info(
                    "[Batch] Duplicating frames: original=%d → final=%d for tile %s",
                    original_batch_size,
                    len(seestar_stack_group_info),
                    str(tile_id),
                )
        except Exception:
            pass
    
    # Choix de l'image de référence (généralement la première du groupe après tri ou la plus centrale)
    reference_image_index_in_group = 0 # Pourrait être plus sophistiqué à l'avenir
    if not (0 <= reference_image_index_in_group < len(seestar_stack_group_info)): 
        pcb_tile(f"{func_id_log_base}_error_invalid_ref_index", prog=None, lvl="ERROR", tile_id=tile_id, ref_idx=reference_image_index_in_group, group_size=len(seestar_stack_group_info))
        return (None, None), failed_groups_to_retry
    
    ref_info_for_tile = seestar_stack_group_info[reference_image_index_in_group]
    wcs_for_master_tile = ref_info_for_tile.get('wcs')
    # Le header est un dict venant du cache, il faut le convertir en objet fits.Header si besoin
    header_dict_for_master_tile_base = ref_info_for_tile.get('header') 

    if not (wcs_for_master_tile and wcs_for_master_tile.is_celestial and header_dict_for_master_tile_base):
        pcb_tile(f"{func_id_log_base}_error_invalid_ref_wcs_header", prog=None, lvl="ERROR", tile_id=tile_id)
        return (None, None), failed_groups_to_retry
    
    # Conversion du dict en objet astropy.io.fits.Header pour la sauvegarde
    header_for_master_tile_base = fits.Header(header_dict_for_master_tile_base.cards if hasattr(header_dict_for_master_tile_base,'cards') else header_dict_for_master_tile_base)
    
    ref_path_raw = ref_info_for_tile.get('path_raw', 'UnknownRawRef')
    pcb_tile(f"{func_id_log_base}_info_reference_set", prog=None, lvl="DEBUG_DETAIL", ref_index=reference_image_index_in_group, ref_filename=_safe_basename(ref_path_raw), tile_id=tile_id)

    # Acquire a dynamic Phase 3 I/O concurrency slot to avoid disk stalls
    # when the system is busy (e.g., another app reading video files).
    try:
        _PH3_CONCURRENCY_SEMAPHORE.acquire()
    except Exception:
        pass

    pcb_tile(f"{func_id_log_base}_info_loading_from_cache_started", prog=None, lvl="DEBUG_DETAIL", num_images=len(seestar_stack_group_info), tile_id=tile_id)
    
    tile_images_data_HWC_adu = []
    tile_original_raw_headers = [] # Liste des dictionnaires de header originaux

    for i, raw_file_info in enumerate(seestar_stack_group_info):
        cached_image_file_path = raw_file_info.get('path_preprocessed_cache')
        original_raw_path = raw_file_info.get('path_raw', 'UnknownRawPathForTileImg') # Plus descriptif

        if not (cached_image_file_path and _path_exists(cached_image_file_path)):
            pcb_tile(f"{func_id_log_base}_warn_cache_file_missing", prog=None, lvl="WARN", filename=_safe_basename(original_raw_path), cache_path=cached_image_file_path, tile_id=tile_id)
            continue
        
        # pcb_tile(f"    {func_id_log_base}_{tile_id}_Img{i}: Lecture cache '{_safe_basename(cached_image_file_path)}'", prog=None, lvl="DEBUG_VERY_DETAIL")
        
        try:
            # Throttle concurrent cache reads and use memory-mapped load to reduce RAM spikes
            with _CACHE_IO_SEMAPHORE:
                img_data_adu = _safe_load_cache(cached_image_file_path, pcb=pcb_tile, tile_id=tile_id)
            if not (isinstance(img_data_adu, np.ndarray) and img_data_adu.dtype == np.float32 and img_data_adu.ndim == 3 and img_data_adu.shape[-1] == 3):
                pcb_tile(f"{func_id_log_base}_warn_invalid_cached_data", prog=None, lvl="WARN", filename=_safe_basename(cached_image_file_path), 
                         shape=img_data_adu.shape if hasattr(img_data_adu, 'shape') else 'N/A', 
                         dtype=img_data_adu.dtype if hasattr(img_data_adu, 'dtype') else 'N/A', tile_id=tile_id)
                del img_data_adu; gc.collect(); continue
            # Assurer des buffers C-contigus ET écriturables pour l'aligneur
            # (les memmaps en lecture seule peuvent provoquer des échecs silencieux)
            try:
                img_data_adu = np.array(img_data_adu, dtype=np.float32, order='C', copy=True)
            except Exception:
                # En cas d'exception, forcer une copie contiguë
                img_data_adu = np.ascontiguousarray(img_data_adu, dtype=np.float32)
            
            tile_images_data_HWC_adu.append(img_data_adu)
            # Stocker le dict de header, pas l'objet fits.Header, car c'est ce qui est dans raw_file_info
            tile_original_raw_headers.append(raw_file_info.get('header')) 
        except MemoryError as e_mem_load_cache:
             pcb_tile(f"{func_id_log_base}_error_memory_loading_cache", prog=None, lvl="ERROR", filename=_safe_basename(cached_image_file_path), error=str(e_mem_load_cache), tile_id=tile_id)
             # Release the concurrency slot before aborting
             try:
                 _PH3_CONCURRENCY_SEMAPHORE.release()
             except Exception:
                 pass
             del tile_images_data_HWC_adu, tile_original_raw_headers; gc.collect(); return (None, None), failed_groups_to_retry
        except Exception as e_load_cache:
            pcb_tile(f"{func_id_log_base}_error_loading_cache", prog=None, lvl="ERROR", filename=_safe_basename(cached_image_file_path), error=str(e_load_cache), tile_id=tile_id)
            logger.error(f"Erreur chargement cache {cached_image_file_path} pour tuile {tile_id}", exc_info=True)
            continue
            
    # Release the concurrency slot as soon as disk reads are done for this tile
    try:
        _PH3_CONCURRENCY_SEMAPHORE.release()
    except Exception:
        pass

    if not tile_images_data_HWC_adu:
        pcb_tile(f"{func_id_log_base}_error_no_valid_images_from_cache", prog=None, lvl="ERROR", tile_id=tile_id)
        return (None, None), failed_groups_to_retry
    # pcb_tile(f"{func_id_log_base}_info_loading_from_cache_finished", prog=None, lvl="DEBUG_DETAIL", num_loaded=len(tile_images_data_HWC_adu), tile_id=tile_id)

    pcb_tile(f"{func_id_log_base}_info_intra_tile_alignment_started", prog=None, lvl="DEBUG_DETAIL", num_to_align=len(tile_images_data_HWC_adu), tile_id=tile_id)
    # Limit concurrency during alignment/stacking as well to reduce peak RAM
    try:
        _PH3_CONCURRENCY_SEMAPHORE.acquire()
    except Exception:
        pass
    aligned_images_for_stack, failed_alignment_indices = zemosaic_align_stack.align_images_in_group(
        image_data_list=tile_images_data_HWC_adu,
        reference_image_index=reference_image_index_in_group,
        progress_callback=progress_callback
    )
    if failed_alignment_indices:
        retry_group: list[dict] = []
        for idx_fail in failed_alignment_indices:
            if 0 <= idx_fail < len(seestar_stack_group_info):
                raw_info = seestar_stack_group_info[idx_fail]
                if isinstance(raw_info, dict):
                    info_copy = dict(raw_info)
                    current_retry = int(info_copy.get('retry_attempt', 0))
                    info_copy['retry_attempt'] = current_retry + 1
                    origin_chain = list(info_copy.get('retry_origin_chain', []))
                    origin_chain.append(int(tile_id))
                    info_copy['retry_origin_chain'] = origin_chain
                else:
                    info_copy = raw_info
                retry_group.append(info_copy)
        if retry_group:
            failed_groups_to_retry.append(retry_group)

    del tile_images_data_HWC_adu; gc.collect()

    valid_aligned_images = [img for img in aligned_images_for_stack if img is not None]
    if aligned_images_for_stack:
        del aligned_images_for_stack # Libérer la liste originale après filtrage

    num_actually_aligned_for_header = len(valid_aligned_images)
    pcb_tile(f"{func_id_log_base}_info_intra_tile_alignment_finished", prog=None, lvl="DEBUG_DETAIL", num_aligned=num_actually_aligned_for_header, tile_id=tile_id)

    if not valid_aligned_images:
        pcb_tile(f"{func_id_log_base}_error_no_images_after_alignment", prog=None, lvl="ERROR", tile_id=tile_id)
        try:
            _PH3_CONCURRENCY_SEMAPHORE.release()
        except Exception:
            pass
        return (None, None), failed_groups_to_retry

    n_used_for_stack = len(valid_aligned_images)
    salvage_mode = n_used_for_stack < min_safe_stack_effective
    if salvage_mode:
        pcb_tile(
            f"Tile {tile_id}: salvage mode (n={n_used_for_stack}). Proceeding with standard QC and crop.",
            prog=None,
            lvl="WARN",
        )
        try:
            if logger:
                logger.warning(
                    "Tile %s: salvage mode (n=%d). Proceeding with standard QC and crop.",
                    str(tile_id),
                    n_used_for_stack,
                )
        except Exception:
            pass

    if debug_tile and valid_aligned_images:
        _dbg_rgb_stats("P3_pre_stack_core", valid_aligned_images[0], logger=logger)

    pcb_tile(f"{func_id_log_base}_info_stacking_started", prog=None, lvl="DEBUG_DETAIL",
             num_to_stack=len(valid_aligned_images), tile_id=tile_id) # Les options sont loggées au début
    master_tile_stacked_HWC, stack_metadata, used_gpu = _stack_master_tile_auto(
        valid_aligned_images,
        stack_norm_method=stack_norm_method,
        stack_weight_method=stack_weight_method,
        stack_reject_algo=stack_reject_algo,
        stack_kappa_low=stack_kappa_low,
        stack_kappa_high=stack_kappa_high,
        parsed_winsor_limits=parsed_winsor_limits,
        stack_final_combine=stack_final_combine,
        poststack_equalize_rgb=poststack_equalize_rgb,
        apply_radial_weight=apply_radial_weight,
        radial_feather_fraction=radial_feather_fraction,
        radial_shape_power=radial_shape_power,
        winsor_pool_workers=winsor_pool_workers,
        winsor_max_frames_per_pass=winsor_max_frames_per_pass,
        progress_callback=progress_callback,
        zconfig=zconfig,
        parallel_plan=parallel_plan,
        pcb_tile=pcb_tile,
        func_id_log_base=func_id_log_base,
        tile_id=tile_id,
        logger=logger,
    )

    if debug_tile:
        _dbg_rgb_stats("P3_post_stack_core", master_tile_stacked_HWC, logger=logger)

    try:
        del valid_aligned_images
    except Exception:
        pass
    gc.collect()

    if master_tile_stacked_HWC is None:
        pcb_tile(f"{func_id_log_base}_error_stacking_failed", prog=None, lvl="ERROR", tile_id=tile_id)
        try:
            _PH3_CONCURRENCY_SEMAPHORE.release()
        except Exception:
            pass
        return (None, None), failed_groups_to_retry

    pcb_tile(f"{func_id_log_base}_info_stacking_finished", prog=None, lvl="DEBUG_DETAIL", tile_id=tile_id,
             shape=master_tile_stacked_HWC.shape)
             # min_val=np.nanmin(master_tile_stacked_HWC), # Peut être verbeux
             # max_val=np.nanmax(master_tile_stacked_HWC),
             # mean_val=np.nanmean(master_tile_stacked_HWC))

    stack_metadata["phase3_used_gpu"] = bool(used_gpu)
    rgb_eq_info = stack_metadata.get("rgb_equalization", {})
    try:
        gain_r = float(rgb_eq_info.get("gain_r", 1.0))
    except (TypeError, ValueError):
        gain_r = 1.0
    try:
        gain_g = float(rgb_eq_info.get("gain_g", 1.0))
    except (TypeError, ValueError):
        gain_g = 1.0
    try:
        gain_b = float(rgb_eq_info.get("gain_b", 1.0))
    except (TypeError, ValueError):
        gain_b = 1.0
    try:
        target_median_val = float(rgb_eq_info.get("target_median", float("nan")))
    except (TypeError, ValueError):
        target_median_val = float("nan")
    eq_enabled = bool(rgb_eq_info.get("enabled", False))
    eq_applied = bool(rgb_eq_info.get("applied", False))
    target_str = f"{target_median_val:.6g}" if np.isfinite(target_median_val) else "nan"
    history_msg = (
        f"RGB equalized per sub-stack (enabled={str(eq_enabled)}, applied={str(eq_applied)}): "
        f"gains=({gain_r:.6f},{gain_g:.6f},{gain_b:.6f}), target={target_str}"
    )

    if debug_tile and poststack_equalize_rgb:
        _dbg_rgb_stats("P3_pre_poststack_rgb_eq", master_tile_stacked_HWC, logger=logger)

    pcb_tile(
        f"[RGB-EQ] poststack_equalize_rgb enabled={eq_enabled}, applied={eq_applied}, "
        f"gains=({gain_r:.6f},{gain_g:.6f},{gain_b:.6f}), target={target_str}",
        prog=None,
        lvl="INFO" if eq_enabled else "DEBUG_DETAIL",
    )

    if debug_tile and (poststack_equalize_rgb or eq_applied):
        _dbg_rgb_stats("P3_post_poststack_rgb_eq", master_tile_stacked_HWC, logger=logger)

    norm_result = None
    norm_mode = "disabled"
    norm_details: dict = {}
    if center_out_context and isinstance(center_out_settings, dict):
        try:
            norm_settings = {
                "enabled": bool(center_out_settings.get("enabled", True)),
                "preview_size": int(center_out_settings.get("preview_size", 256)),
                "sky_percentile": tuple(center_out_settings.get("sky_percentile", (25.0, 60.0))),
                "clip_sigma": float(center_out_settings.get("clip_sigma", 2.5)),
                "min_overlap_fraction": float(center_out_settings.get("min_overlap_fraction", 0.03)),
            }
        except Exception:
            norm_settings = {"enabled": False}
        if norm_settings.get("enabled", False):
            master_tile_stacked_HWC, norm_result, norm_mode, norm_details = apply_center_out_normalization_p3(
                master_tile_stacked_HWC,
                wcs_for_master_tile,
                tile_id,
                center_out_context,
                norm_settings,
                pcb_tile,
            )
            if norm_result:
                pcb_tile(
                    f"{func_id_log_base}_center_out_applied",
                    prog=None,
                    lvl="INFO_DETAIL",
                    tile_id=tile_id,
                    gain=f"{norm_result[0]:.6f}",
                    offset=f"{norm_result[1]:.6f}",
                    mode=norm_mode,
                    samples=norm_details.get("samples"),
                )
            else:
                pcb_tile(
                    f"{func_id_log_base}_center_out_skipped",
                    prog=None,
                    lvl="DEBUG_DETAIL",
                    tile_id=tile_id,
                    reason=norm_mode,
                )

    quality_crop_rect: tuple[int, int, int, int] | None = None
    if quality_crop_enabled_effective:
        try:
            band_px = max(4, int(quality_crop_band_px))
        except Exception:
            band_px = 32
        try:
            margin_px = max(0, int(quality_crop_margin_px))
        except Exception:
            margin_px = 8
        try:
            k_sigma = float(quality_crop_k_sigma)
            if not math.isfinite(k_sigma):
                raise ValueError("k_sigma not finite")
        except Exception:
            k_sigma = 2.0
        k_sigma = max(0.1, min(k_sigma, 10.0))

        data_for_crop = np.asarray(master_tile_stacked_HWC)
        axis_mode = "HWC"
        if data_for_crop.ndim == 3 and data_for_crop.shape[0] == 3 and data_for_crop.shape[-1] != 3:
            data_for_crop = np.moveaxis(data_for_crop, 0, -1)
            axis_mode = "CHW"
        elif data_for_crop.ndim == 2:
            data_for_crop = data_for_crop[..., np.newaxis]
            axis_mode = "HW"

        try:
            if data_for_crop.ndim < 3:
                raise ValueError("insufficient dimensions for quality crop")

            if data_for_crop.shape[-1] >= 3:
                R = data_for_crop[..., 0]
                G = data_for_crop[..., 1]
                B = data_for_crop[..., 2]
            else:
                mono = data_for_crop[..., 0]
                R = G = B = mono

            lum2d = np.nanmean(np.stack([R, G, B], axis=0), axis=0).astype(np.float32)
            R = np.nan_to_num(np.asarray(R, dtype=np.float32), nan=0.0)
            G = np.nan_to_num(np.asarray(G, dtype=np.float32), nan=0.0)
            B = np.nan_to_num(np.asarray(B, dtype=np.float32), nan=0.0)
            lum2d = np.nan_to_num(lum2d, nan=0.0)

            detect_autocrop_rgb = getattr(lecropper, "detect_autocrop_rgb", None)
            if not callable(detect_autocrop_rgb):
                raise ImportError("lecropper.detect_autocrop_rgb unavailable")

            y0, x0, y1, x1 = detect_autocrop_rgb(
                lum2d,
                R,
                G,
                B,
                band_px=band_px,
                k_sigma=k_sigma,
                margin_px=margin_px,
            )

            h_lum, w_lum = lum2d.shape
            if not (0 <= y0 < y1 <= h_lum and 0 <= x0 < x1 <= w_lum):
                raise ValueError("invalid crop rectangle")

            crop_area = (y1 - y0) * (x1 - x0)
            full_area = h_lum * w_lum
            if crop_area <= 0 or (crop_area / max(1, full_area)) >= 0.97:
                pcb_tile(
                    f"MT_CROP: quality crop skipped (rect={y0,x0,y1,x1}, area_ratio={crop_area/max(1, full_area):.3f})",
                    prog=None,
                    lvl="WARN",
                )
            else:
                cropped = data_for_crop[y0:y1, x0:x1, ...]
                if axis_mode == "CHW":
                    master_tile_stacked_HWC = np.moveaxis(cropped, -1, 0)
                elif axis_mode == "HW":
                    master_tile_stacked_HWC = cropped[..., 0]
                else:
                    master_tile_stacked_HWC = cropped
                quality_crop_rect = (int(y0), int(x0), int(y1), int(x1))

                if wcs_for_master_tile is not None:
                    try:
                        if hasattr(wcs_for_master_tile, "deepcopy"):
                            wcs_cropped = wcs_for_master_tile.deepcopy()
                        else:
                            wcs_cropped = copy.deepcopy(wcs_for_master_tile)
                    except Exception:
                        wcs_cropped = None

                    if wcs_cropped is not None and hasattr(wcs_cropped, "wcs"):
                        try:
                            wcs_cropped.wcs.crpix[0] -= x0
                            wcs_cropped.wcs.crpix[1] -= y0
                        except Exception as e_crpix:
                            pcb_tile(
                                f"MT_CROP: quality-based WCS shift failed: {e_crpix}",
                                prog=None,
                                lvl="WARN",
                            )
                        else:
                            wcs_for_master_tile = wcs_cropped
                    elif wcs_cropped is not None:
                        wcs_for_master_tile = wcs_cropped

                    if wcs_for_master_tile is wcs_cropped and hasattr(wcs_cropped, "pixel_shape"):
                        try:
                            new_h, new_w = master_tile_stacked_HWC.shape[:2]
                            wcs_cropped.pixel_shape = (new_w, new_h)
                            if hasattr(wcs_cropped, "array_shape"):
                                wcs_cropped.array_shape = (new_h, new_w)
                        except Exception:
                            pass

                pcb_tile(
                    f"MT_CROP: quality-based rect={quality_crop_rect} (band={band_px}, k={k_sigma:.2f}, margin={margin_px})",
                    prog=None,
                    lvl="INFO_DETAIL",
                )
        except Exception as e_crop:
            pcb_tile(
                f"MT_CROP: quality-based crop failed ({e_crop})",
                prog=None,
                lvl="WARN",
            )

    pipeline_cfg = {
        "quality_crop_enabled": quality_crop_enabled_effective,
        "quality_crop_band_px": quality_crop_band_px,
        "quality_crop_k_sigma": quality_crop_k_sigma,
        "quality_crop_margin_px": quality_crop_margin_px,
        "quality_crop_min_run": quality_crop_min_run,
        "altaz_cleanup_enabled": altaz_cleanup_enabled,
        "altaz_margin_percent": altaz_margin_percent,
        "altaz_decay": altaz_decay,
        "altaz_nanize": altaz_nanize,
    }
    master_tile_stacked_HWC, pipeline_alpha_mask = _apply_lecropper_pipeline(master_tile_stacked_HWC, pipeline_cfg)
    if master_tile_stacked_HWC is None:
        raise RuntimeError("lecropper pipeline returned no data for master tile")
    alpha_mask_out: np.ndarray | None = None
    pipeline_alpha_u8 = _normalize_alpha_mask(
        pipeline_alpha_mask,
        target_hw=master_tile_stacked_HWC.shape[:2],
        opacity_threshold=ALPHA_OPACITY_THRESHOLD,
    )
    if pipeline_alpha_u8 is not None:
        alpha_mask_out = pipeline_alpha_u8
    try:
        if alpha_mask_out is None:
            arr_for_alpha = np.asarray(master_tile_stacked_HWC)
            if arr_for_alpha.ndim == 3:
                valid_mask = np.any(np.isfinite(arr_for_alpha), axis=-1)
            else:
                valid_mask = np.isfinite(arr_for_alpha)
            alpha_mask_out = np.where(valid_mask, 255, 0).astype(np.uint8)
    except Exception:
        alpha_mask_out = None

    alpha_mask_for_quality: np.ndarray | None = pipeline_alpha_mask
    if alpha_mask_for_quality is None and alpha_mask_out is not None:
        alpha_mask_for_quality = alpha_mask_out

    quality_gate_eval: Optional[dict[str, Any]] = _evaluate_quality_gate_metrics(
        tile_id,
        master_tile_stacked_HWC,
        enabled=quality_gate_enabled_effective,
        threshold=quality_gate_threshold,
        edge_band=quality_gate_edge_band_px,
        k_sigma=quality_gate_k_sigma,
        erode_px=quality_gate_erode_px,
        pcb=pcb_tile,
        alpha_mask=alpha_mask_for_quality,
        alpha_soft_threshold=QUALITY_GATE_ALPHA_SOFT_THRESHOLD,
    )

    try:
        pcb_tile(
            "MT_PIPELINE: lecropper_applied=True, "
            f"quality_crop={bool(quality_crop_enabled_effective)}, "
            f"altaz_cleanup={bool(altaz_cleanup_enabled)}, "
            f"quality_gate={bool(quality_gate_enabled_effective)}, "
            f"salvage_mode={bool(salvage_mode)}",
            prog=None,
            lvl="INFO_DETAIL",
        )
    except Exception:
        pass

    # pcb_tile(f"{func_id_log_base}_info_saving_started", prog=None, lvl="DEBUG_DETAIL", tile_id=tile_id)
    output_temp_dir_path = Path(output_temp_dir).expanduser()
    output_temp_dir_path.mkdir(parents=True, exist_ok=True)
    temp_fits_filename = f"master_tile_{tile_id:03d}.fits"
    temp_fits_path = output_temp_dir_path / temp_fits_filename

    try:
        # Créer un nouvel objet Header pour la sauvegarde
        header_mt_save = fits.Header()
        if wcs_for_master_tile:
            try: 
                # S'assurer que wcs_for_master_tile a les NAXIS bien définis pour to_header
                # La shape de master_tile_stacked_HWC est (H, W, C)
                # Pour le WCS 2D, on a besoin de (W, H)
                if master_tile_stacked_HWC.ndim >= 2:
                    h_final, w_final = master_tile_stacked_HWC.shape[:2]
                    # Mettre à jour les attributs NAXIS du WCS si nécessaire,
                    # car to_header les utilise.
                    # wcs_for_master_tile.wcs.naxis1 = w_final # Ne pas modifier l'objet WCS original directement ici
                    # wcs_for_master_tile.wcs.naxis2 = h_final # car il est partagé/réutilisé.
                    # Créer une copie du WCS pour modification locale avant to_header si besoin.
                    # Cependant, save_fits_image devrait gérer les NAXIS en fonction des données.
                    pass

                header_mt_save.update(wcs_for_master_tile.to_header(relax=True))
            except Exception as e_wcs_hdr: 
                pcb_tile(f"{func_id_log_base}_warn_wcs_header_error_saving", prog=None, lvl="WARN", tile_id=tile_id, error=str(e_wcs_hdr))
        
        
        
        header_mt_save['ZMT_TYPE']=('Master Tile','ZeMosaic Processed Tile'); header_mt_save['ZMT_ID']=(tile_id,'Master Tile ID')
        header_mt_save['ZMT_NRAW']=(len(seestar_stack_group_info),'Raw frames in this tile group')
        header_mt_save['ZMT_NALGN']=(num_actually_aligned_for_header,'Successfully aligned frames for stack')
        header_mt_save['MT_NFRAMES'] = (int(num_actually_aligned_for_header), 'Frames stacked into this master tile')
        header_mt_save['ZMT_NORM'] = (str(stack_norm_method), 'Normalization method')
        header_mt_save['ZMT_WGHT'] = (str(stack_weight_method), 'Weighting method')
        if apply_radial_weight: # Log des paramètres radiaux
            header_mt_save['ZMT_RADW'] = (True, 'Radial weighting applied')
            header_mt_save['ZMT_RADF'] = (radial_feather_fraction, 'Radial feather fraction')
            header_mt_save['ZMT_RADP'] = (radial_shape_power, 'Radial shape power')
        else:
            header_mt_save['ZMT_RADW'] = (False, 'Radial weighting applied')

        header_mt_save['RGBGAINR'] = (gain_r, 'RGB equalization gain (red)')
        header_mt_save['RGBGAING'] = (gain_g, 'RGB equalization gain (green)')
        header_mt_save['RGBGAINB'] = (gain_b, 'RGB equalization gain (blue)')
        header_mt_save['RGBEQMED'] = (target_median_val, 'RGB equalization target median')
        try:
            header_mt_save.add_history(history_msg)
        except Exception:
            header_mt_save['HISTORY'] = history_msg

        header_mt_save['ZMT_REJ'] = (str(stack_reject_algo), 'Rejection algorithm')
        if stack_reject_algo == "kappa_sigma":
            header_mt_save['ZMT_KAPLO'] = (stack_kappa_low, 'Kappa Sigma Low threshold')
            header_mt_save['ZMT_KAPHI'] = (stack_kappa_high, 'Kappa Sigma High threshold')
        elif stack_reject_algo == "winsorized_sigma_clip":
            header_mt_save['ZMT_WINLO'] = (parsed_winsor_limits[0], 'Winsor Lower limit %')
            header_mt_save['ZMT_WINHI'] = (parsed_winsor_limits[1], 'Winsor Upper limit %')
            # Les paramètres Kappa sont aussi pertinents pour Winsorized
        header_mt_save['ZMT_KAPLO'] = (stack_kappa_low, 'Kappa Low for Winsorized')
        header_mt_save['ZMT_KAPHI'] = (stack_kappa_high, 'Kappa High for Winsorized')
        header_mt_save['ZMT_COMB'] = (str(stack_final_combine), 'Final combine method')
        header_mt_save['ALPHAEXT'] = (1 if alpha_mask_out is not None else 0, 'Alpha mask ext present')

        if center_out_context and center_out_settings:
            header_mt_save['ZMT_ANCH'] = (
                int(center_out_context.anchor_original_id),
                'Anchor tile id (original index)'
            )
            if norm_result:
                header_mt_save['ZMT_P3CO'] = (1, 'Phase 3 center-out normalization applied')
                header_mt_save['ZMT_AGAIN'] = (float(norm_result[0]), 'Phase 3 center-out gain')
                header_mt_save['ZMT_AOFF'] = (float(norm_result[1]), 'Phase 3 center-out offset')
            else:
                header_mt_save['ZMT_P3CO'] = (0, 'Phase 3 center-out normalization applied')
                header_mt_save['ZMT_AGAIN'] = (1.0, 'Phase 3 center-out gain')
                header_mt_save['ZMT_AOFF'] = (0.0, 'Phase 3 center-out offset')
        else:
            header_mt_save['ZMT_P3CO'] = (0, 'Phase 3 center-out normalization applied')
            header_mt_save['ZMT_AGAIN'] = (1.0, 'Phase 3 center-out gain')
            header_mt_save['ZMT_AOFF'] = (0.0, 'Phase 3 center-out offset')
            header_mt_save['ZMT_ANCH'] = (-1, 'Anchor tile id (original index)')

        if header_for_master_tile_base: # C'est déjà un objet fits.Header
            ref_path_raw_for_hdr = seestar_stack_group_info[reference_image_index_in_group].get('path_raw', 'UnknownRef')
            header_mt_save['ZMT_REF'] = (_safe_basename(ref_path_raw_for_hdr), 'Reference raw frame for this tile WCS')
            keys_from_ref = ['OBJECT','DATE-AVG','FILTER','INSTRUME','FOCALLEN','XPIXSZ','YPIXSZ', 'GAIN', 'OFFSET'] # Ajout GAIN, OFFSET
            for key_h in keys_from_ref:
                if key_h in header_for_master_tile_base:
                    try: 
                        # Tenter d'obtenir la valeur et le commentaire
                        card = header_for_master_tile_base.cards[key_h]
                        header_mt_save[key_h] = (card.value, card.comment)
                    except (KeyError, AttributeError): # Si la carte n'a pas de commentaire ou si ce n'est pas un objet CardImage
                        header_mt_save[key_h] = header_for_master_tile_base[key_h]
            
            total_exposure_tile = 0.
            num_exposure_summed = 0
            for hdr_raw_item_dict in tile_original_raw_headers: # Ce sont des dicts
                if hdr_raw_item_dict is None: continue
                try: 
                    exposure_val = hdr_raw_item_dict.get('EXPTIME', hdr_raw_item_dict.get('EXPOSURE', 0.0))
                    total_exposure_tile += float(exposure_val if exposure_val is not None else 0.0)
                    num_exposure_summed +=1
                except (TypeError, ValueError) : pass
            header_mt_save['EXPTOTAL']=(round(total_exposure_tile,2),'[s] Sum of EXPTIME for this tile')
            header_mt_save['NEXP_SUM']=(num_exposure_summed,'Number of exposures summed for EXPTOTAL')


        if quality_crop_rect:
            header_mt_save['ZMT_QCRO'] = (True, 'Quality-based crop applied')
            header_mt_save['ZMT_QBOX'] = (
                "{},{},{},{}".format(*quality_crop_rect),
                'Quality crop rectangle (y0,x0,y1,x1)',
            )
        else:
            header_mt_save['ZMT_QCRO'] = (False, 'Quality-based crop applied')

        if quality_gate_eval and quality_gate_eval.get("metrics"):
            metrics = quality_gate_eval.get("metrics") or {}
            score = float(quality_gate_eval.get("score", 0.0))
            accepted_flag = bool(quality_gate_eval.get("accepted", True))
            header_mt_save['ZMT_QS'] = (round(score, 3), 'ZeQuality score (0=good)')
            header_mt_save['ZMT_QBD'] = (0 if accepted_flag else 1, '1 if auto-rejected')
            header_mt_save['ZMT_EOC'] = (round(metrics.get("EOC", 0.0), 3), 'Extended Object Coverage')
            header_mt_save['ZMT_TRL'] = (round(metrics.get("TRL", 0.0), 3), 'Trailiness index')
            header_mt_save['ZMT_CER'] = (round(metrics.get("CER", 0.0), 3), 'Corner Emptiness Ratio')
        elif quality_gate_enabled:
            header_mt_save['ZMT_QBD'] = (0, '1 if auto-rejected')

        zemosaic_utils.save_fits_image(
            image_data=master_tile_stacked_HWC,
            output_path=str(temp_fits_path),
            header=header_mt_save,
            overwrite=True,
            save_as_float=True,
            progress_callback=progress_callback,
            axis_order="HWC",
            alpha_mask=alpha_mask_out,
        )

        final_tile_path: Optional[str] = str(temp_fits_path)
        final_wcs = wcs_for_master_tile

        if quality_gate_eval and quality_gate_eval.get("metrics"):
            metrics = quality_gate_eval.get("metrics") or {}
            score = float(quality_gate_eval.get("score", 0.0))
            accepted_flag = bool(quality_gate_eval.get("accepted", True))
            status_label = "ACCEPT" if accepted_flag else "REJECT"
            pcb_tile(
                "mt_quality_gate_result",
                prog=None,
                lvl="INFO",
                tile_id=int(tile_id),
                path=_safe_basename(temp_fits_path),
                score=f"{score:.3f}",
                threshold=f"{float(quality_gate_threshold):.3f}",
                status=status_label,
            )
            logger.info(
                "[ZeQualityMT] tile=%s score=%.3f thr=%.3f -> %s",
                tile_id,
                score,
                float(quality_gate_threshold),
                status_label,
            )
            if not accepted_flag:
                if quality_gate_move_rejects:
                    moved_path, moved = _move_quality_reject_file(str(temp_fits_path))
                    if moved:
                        pcb_tile(
                            "mt_quality_gate_moved",
                            prog=None,
                            lvl="WARN",
                            tile_id=int(tile_id),
                            dst=moved_path,
                        )
                        logger.warning("[ZeQualityMT] tile=%s moved to %s", tile_id, moved_path)
                final_tile_path = None
                final_wcs = None

        if final_tile_path:
            try:
                _register_master_tile_identity(final_tile_path, f"tile:{int(tile_id):04d}")
            except Exception:
                _register_master_tile_identity(final_tile_path, tile_id)

        pcb_tile(
            f"{func_id_log_base}_info_saved",
            prog=None,
            lvl="INFO_DETAIL",
            tile_id=tile_id,
            format_type='float32',
            filename=_safe_basename(final_tile_path or str(temp_fits_path)),
        )
        # pcb_tile(f"{func_id_log_base}_info_saving_finished", prog=None, lvl="DEBUG_DETAIL", tile_id=tile_id)
        try:
            _PH3_CONCURRENCY_SEMAPHORE.release()
        except Exception:
            pass
        return (final_tile_path, final_wcs), failed_groups_to_retry
        
    except Exception as e_save_mt:
        pcb_tile(f"{func_id_log_base}_error_saving", prog=None, lvl="ERROR", tile_id=tile_id, error=str(e_save_mt))
        logger.error(f"Traceback pour {func_id_log_base}_{tile_id} sauvegarde:", exc_info=True)
        try:
            _PH3_CONCURRENCY_SEMAPHORE.release()
        except Exception:
            pass
        return (None, None), failed_groups_to_retry
    finally:
        if 'master_tile_stacked_HWC' in locals() and master_tile_stacked_HWC is not None: 
            del master_tile_stacked_HWC
        gc.collect()
        # Restore any EXTREME_GROUP overrides applied to zconfig for this tile
        try:
            if '_extreme_group_originals' in locals() and isinstance(_extreme_group_originals, dict) and getattr(zconfig, None) is not None:
                try:
                    if 'stack_mem_preemptive_stream_enabled' in _extreme_group_originals:
                        orig = _extreme_group_originals.get('stack_mem_preemptive_stream_enabled')
                        if orig is None:
                            try:
                                delattr(zconfig, 'stack_mem_preemptive_stream_enabled')
                            except Exception:
                                pass
                        else:
                            try:
                                setattr(zconfig, 'stack_mem_preemptive_stream_enabled', orig)
                            except Exception:
                                pass
                except Exception:
                    pass
                try:
                    if 'stack_memmap_enabled' in _extreme_group_originals:
                        orig = _extreme_group_originals.get('stack_memmap_enabled')
                        if orig is None:
                            try:
                                delattr(zconfig, 'stack_memmap_enabled')
                            except Exception:
                                pass
                        else:
                            try:
                                setattr(zconfig, 'stack_memmap_enabled', orig)
                            except Exception:
                                pass
                except Exception:
                    pass
                try:
                    if 'winsor_max_frames_per_pass' in _extreme_group_originals:
                        orig = _extreme_group_originals.get('winsor_max_frames_per_pass')
                        if orig is None:
                            try:
                                delattr(zconfig, 'winsor_max_frames_per_pass')
                            except Exception:
                                pass
                        else:
                            try:
                                setattr(zconfig, 'winsor_max_frames_per_pass', orig)
                            except Exception:
                                pass
                except Exception:
                    pass
        except Exception:
            pass



# Dans zemosaic_worker.py

# ... (s'assurer que zemosaic_utils est importé et ZEMOSAIC_UTILS_AVAILABLE est défini)
# ... (s'assurer que WCS, fits d'Astropy sont importés, ainsi que reproject_interp)
# ... (définition de logger, _log_and_callback, etc.)



def assemble_final_mosaic_incremental(
    master_tile_fits_with_wcs_list: list,
    final_output_wcs: WCS,
    final_output_shape_hw: tuple,
    progress_callback: callable,
    n_channels: int = 3,
    dtype_accumulator: np.dtype = np.float64,
    dtype_norm: np.dtype = np.float32,
    apply_crop: bool = False,
    crop_percent: float = 0.0,
    processing_threads: int = 0,
    memmap_dir: str | None = None,
    cleanup_memmap: bool = True,
    intertile_photometric_match: bool = False,
    intertile_preview_size: int = 512,
    intertile_overlap_min: float = 0.05,
    intertile_sky_percentile: tuple[float, float] | list[float] = (30.0, 70.0),
    intertile_robust_clip_sigma: float = 2.5,
    intertile_global_recenter: bool = True,
    intertile_recenter_clip: tuple[float, float] | list[float] = (0.85, 1.18),
    use_auto_intertile: bool = False,
    match_background: bool = True,
    feather_parity: bool = False,
    use_radial_feather: bool | None = None,
    two_pass_coverage_renorm: bool = False,
    tile_affine_corrections: list[tuple[float, float]] | None = None,
    enforce_positive: bool | None = None,
    base_progress_phase5: float | None = None,
    progress_weight_phase5: float | None = None,
    start_time_total_run: float | None = None,
    global_anchor_shift: tuple[float, float] | None = None,
    stats_callback: Callable[[int, int, bool], None] | None = None,
):
    """Assemble les master tiles par co-addition sur disque."""
    import time
    # Marquer le début de la phase 5 incrémentale
    start_time_inc = time.monotonic()
    total_tiles = len(master_tile_fits_with_wcs_list)
    FLUSH_BATCH_SIZE = 10  # nombre de tuiles entre chaque flush sur le memmap
    pcb_asm = lambda msg_key, prog=None, lvl="INFO_DETAIL", **kwargs: _log_and_callback(
        msg_key, prog, lvl, callback=progress_callback, **kwargs
    )

    pending_affine_list: list[tuple[float, float]] | None = None
    nontrivial_detected = False
    if tile_affine_corrections:
        pending_affine_list, nontrivial_detected = _sanitize_affine_corrections(
            tile_affine_corrections,
            len(master_tile_fits_with_wcs_list),
        )

    # Default to radial feathering for parity with the legacy pipeline unless explicitly disabled.
    use_feather = True if use_radial_feather is None else bool(use_radial_feather)
    parity_mode = bool(intertile_photometric_match and match_background and feather_parity)
    if feather_parity:
        if use_feather:
            use_feather = False
        pcb_asm("run_warn_incremental_feather_parity_enabled", prog=None, lvl="WARN")

    if enforce_positive is None:
        enforce_positive_flag = not parity_mode
    else:
        enforce_positive_flag = bool(enforce_positive)

    if progress_weight_phase5 is None:
        progress_weight_phase5 = globals().get("PROGRESS_WEIGHT_PHASE5_ASSEMBLY", 0.0)
    if base_progress_phase5 is None:
        base_progress_phase5 = 0.0

    pcb_asm(
        f"ASM_INC: Début. Options rognage - Appliquer: {apply_crop}, %: {crop_percent if apply_crop else 'N/A'}",
        lvl="DEBUG_DETAIL",
    )

    if not (REPROJECT_AVAILABLE and reproject_interp and ASTROPY_AVAILABLE and fits):
        missing_deps = []
        if not REPROJECT_AVAILABLE or not reproject_interp:
            missing_deps.append("Reproject (reproject_interp)")
        if not ASTROPY_AVAILABLE or not fits:
            missing_deps.append("Astropy (fits)")
        pcb_asm(
            "assemble_error_core_deps_unavailable_incremental",
            prog=None,
            lvl="ERROR",
            missing=", ".join(missing_deps),
        )
        return None, None, None

    if not master_tile_fits_with_wcs_list:
        pcb_asm("assemble_error_no_tiles_provided_incremental", prog=None, lvl="ERROR")
        return None, None, None

    # ``final_output_shape_hw`` MUST be provided in ``(height, width)`` order.
    if (
        not isinstance(final_output_shape_hw, (tuple, list))
        or len(final_output_shape_hw) != 2
    ):
        pcb_asm(
            "assemble_error_invalid_final_shape_inc",
            prog=None,
            lvl="ERROR",
            shape=str(final_output_shape_hw),
        )
        return None, None, None

    h, w = map(int, final_output_shape_hw)

    # --- Extra validation to help catch swapped width/height ---
    try:
        w_wcs = int(getattr(final_output_wcs, "pixel_shape", (w, h))[0])
        h_wcs = int(getattr(final_output_wcs, "pixel_shape", (w, h))[1])
    except Exception:
        w_wcs = int(getattr(final_output_wcs.wcs, "naxis1", w)) if hasattr(final_output_wcs, "wcs") else w
        h_wcs = int(getattr(final_output_wcs.wcs, "naxis2", h)) if hasattr(final_output_wcs, "wcs") else h

    expected_hw = (h_wcs, w_wcs)
    if (h, w) != expected_hw:
        if (w, h) == expected_hw:
            pcb_asm(
                "assemble_warn_swapped_final_shape_inc",
                prog=None,
                lvl="WARN",
                provided=str(final_output_shape_hw),
                expected=str(expected_hw),
            )
            h, w = expected_hw
        else:
            pcb_asm(
                "assemble_error_mismatch_final_shape_inc",
                prog=None,
                lvl="ERROR",
                provided=str(final_output_shape_hw),
                expected=str(expected_hw),
            )
            return None, None, None

    if match_background:
        pcb_asm("run_info_incremental_match_background", prog=None, lvl="INFO_DETAIL")

    if (
        intertile_photometric_match
        and pending_affine_list is None
        and total_tiles >= 2
        and ZEMOSAIC_UTILS_AVAILABLE
        and hasattr(zemosaic_utils, "create_downscaled_luminance_preview")
        and hasattr(zemosaic_utils, "compute_intertile_affine_calibration")
    ):
        affine_start = time.monotonic()
        pcb_asm(
            "run_info_incremental_affine_start",
            prog=None,
            lvl="INFO",
            num_tiles=total_tiles,
        )
        tile_sources = [
            _TileAffineSource(path=tile_path, wcs=tile_wcs)
            for tile_path, tile_wcs in master_tile_fits_with_wcs_list
        ]
        pending_affine_list, nontrivial_detected, affine_status, affine_error = (
            _compute_intertile_affine_corrections_from_sources(
                sources=tile_sources,
                final_output_wcs=final_output_wcs,
                final_output_shape_hw=final_output_shape_hw,
                preview_size=int(intertile_preview_size),
                min_overlap_fraction=float(intertile_overlap_min),
                sky_percentile=intertile_sky_percentile,
                robust_clip_sigma=float(intertile_robust_clip_sigma),
                use_auto_intertile=use_auto_intertile,
                logger_obj=logger,
                progress_callback=progress_callback,
                intertile_global_recenter=bool(intertile_global_recenter),
                intertile_recenter_clip=intertile_recenter_clip,
            )
        )
        if affine_status == "preview_failed":
            pcb_asm(
                "assemble_warn_intertile_photometric_failed",
                prog=None,
                lvl="WARN",
                error="preview_failed",
            )
            pending_affine_list = None
            nontrivial_detected = False
        elif affine_status == "compute_failed":
            pcb_asm(
                "assemble_warn_intertile_photometric_failed",
                prog=None,
                lvl="WARN",
                error=str(affine_error),
            )
            pending_affine_list = None
            nontrivial_detected = False
        affine_elapsed = time.monotonic() - affine_start
        if nontrivial_detected:
            try:
                pcb_asm(
                    "assemble_info_intertile_photometric_applied",
                    prog=None,
                    lvl="INFO_DETAIL",
                    num_tiles=sum(
                        1
                        for g_val, o_val in pending_affine_list or []
                        if abs(g_val - 1.0) > 1e-6 or abs(o_val) > 1e-6
                    ),
                )
            except Exception:
                pass
        pcb_asm(
            "run_info_incremental_affine_done",
            prog=None,
            lvl="INFO",
            elapsed=f"{affine_elapsed:.2f}",
            num_tiles=len(pending_affine_list or []),
        )
    pending_affine_list, anchor_shift_applied = _compose_global_anchor_shift(
        pending_affine_list,
        total_tiles,
        global_anchor_shift,
    )
    if anchor_shift_applied:
        nontrivial_detected = True

    affine_log_indices = _select_affine_log_indices(pending_affine_list)

    sum_shape = (h, w, n_channels)
    weight_shape = (h, w)

    internal_temp_dir = False
    if memmap_dir is None:
        memmap_dir_path = Path(
            tempfile.mkdtemp(prefix="zemosaic_memmap_", dir=str(get_runtime_temp_dir()))
        )
        internal_temp_dir = True
    else:
        memmap_dir_path = Path(memmap_dir).expanduser()
        memmap_dir_path.mkdir(parents=True, exist_ok=True)
    sum_path = memmap_dir_path / "SOMME.fits"
    weight_path = memmap_dir_path / "WEIGHT.fits"
    alpha_path = memmap_dir_path / "ALPHA.fits"

    try:
        fits.writeto(sum_path, np.zeros(sum_shape, dtype=dtype_accumulator), overwrite=True)
        fits.writeto(weight_path, np.zeros(weight_shape, dtype=dtype_norm), overwrite=True)
        fits.writeto(alpha_path, np.zeros(weight_shape, dtype=np.float32), overwrite=True)
    except Exception as e_create:
        pcb_asm("assemble_error_memmap_write_failed_inc", prog=None, lvl="ERROR", error=str(e_create))
        logger.error("Failed to create memmap FITS", exc_info=True)
        return None, None, None


    try:
        req_workers = int(processing_threads)
    except Exception:
        req_workers = 0
    if req_workers > 0:
        max_procs = req_workers
    else:
        max_procs = min(os.cpu_count() or 1, len(master_tile_fits_with_wcs_list))
    pcb_asm(f"ASM_INC: Using {max_procs} process workers", lvl="DEBUG_DETAIL")

    parent_is_daemon = multiprocessing.current_process().daemon
    Executor = ThreadPoolExecutor if parent_is_daemon else ProcessPoolExecutor


    try:
        with Executor(max_workers=max_procs) as ex, \
                fits.open(sum_path, mode="update", memmap=True) as hsum, \
                fits.open(weight_path, mode="update", memmap=True) as hwei, \
                fits.open(alpha_path, mode="update", memmap=True) as halpha:
            fsum = hsum[0].data
            fwei = hwei[0].data
            falpha = halpha[0].data

            tiles_since_flush = 0

            future_map = {}
            for tile_idx, (tile_path, tile_wcs) in enumerate(master_tile_fits_with_wcs_list, 1):
                pcb_asm(
                    "assemble_info_processing_tile",
                    prog=None,
                    lvl="INFO_DETAIL",
                    tile_num=tile_idx,
                    total_tiles=len(master_tile_fits_with_wcs_list),
                    filename=_safe_basename(tile_path),
                )
                # Les objets WCS peuvent poser problème lors de la sérialisation.
                # On transmet donc leurs en-têtes et ils seront reconstruits dans le worker.
                tile_wcs_hdr = tile_wcs.to_header() if hasattr(tile_wcs, "to_header") else tile_wcs
                output_wcs_hdr = final_output_wcs.to_header() if hasattr(final_output_wcs, "to_header") else final_output_wcs
                gain_val = None
                offset_val = None
                if pending_affine_list and (tile_idx - 1) < len(pending_affine_list):
                    raw_affine = pending_affine_list[tile_idx - 1]
                    try:
                        gain_val = float(raw_affine[0])
                    except Exception:
                        gain_val = 1.0
                    try:
                        offset_val = float(raw_affine[1])
                    except Exception:
                        offset_val = 0.0
                    if not np.isfinite(gain_val):
                        gain_val = 1.0
                    if not np.isfinite(offset_val):
                        offset_val = 0.0
                    if (
                        affine_log_indices
                        and tile_idx in affine_log_indices
                        and (abs(gain_val - 1.0) > 1e-6 or abs(offset_val) > 1e-6)
                    ):
                        try:
                            pcb_asm(
                                "run_info_incremental_apply_gain_offset",
                                prog=None,
                                lvl="INFO_DETAIL",
                                tile_num=tile_idx,
                                gain=f"{gain_val:.6f}",
                                offset=f"{offset_val:.6f}",
                            )
                        except Exception:
                            pass
                future = ex.submit(
                    reproject_tile_to_mosaic,
                    tile_path,
                    tile_wcs_hdr,
                    output_wcs_hdr,
                    final_output_shape_hw,
                    feather=use_feather,
                    apply_crop=apply_crop,
                    crop_percent=crop_percent,
                    tile_affine=None,
                    gain=gain_val,
                    offset=offset_val,
                    match_background=match_background,
                    nan_fill_value=0.0,
                    enforce_positive=enforce_positive_flag,
                )
                future_map[future] = tile_idx

            processed = 0
            total_steps = len(future_map)
            start_time_iter = time.time()
            last_time = start_time_iter
            step_times = []
            for fut in as_completed(future_map):
                idx = future_map[fut]
                try:
                    # reproject_tile_to_mosaic renvoie les bornes de la tuile
                    # sous la forme (xmin, xmax, ymin, ymax) afin de
                    # correspondre aux indices de colonne puis de ligne.
                    I_tile, W_tile, alpha_tile, (xmin, xmax, ymin, ymax) = fut.result()
                except MemoryError as e_mem:
                    pcb_asm(
                        "assemble_error_memory_tile_reprojection_inc",
                        prog=None,
                        lvl="ERROR",
                        tile_num=idx,
                        error=str(e_mem),
                    )
                    logger.error(
                        f"MemoryError reproject_tile_to_mosaic tuile {idx}",
                        exc_info=True,
                    )
                    processed += 1
                    continue
                except BrokenProcessPool as bpp:
                    pcb_asm(
                        "assemble_error_broken_process_pool_incremental",
                        prog=None,
                        lvl="ERROR",
                        tile_num=idx,
                        error=str(bpp),
                    )
                    logger.error(
                        "BrokenProcessPool during tile reprojection",
                        exc_info=True,
                    )
                    return None, None, None
                except Exception as e_reproj:
                    pcb_asm(
                        "assemble_error_tile_reprojection_failed_inc",
                        prog=None,
                        lvl="ERROR",
                        tile_num=idx,
                        error=str(e_reproj),
                    )
                    logger.error(
                        f"Erreur reproject_tile_to_mosaic tuile {idx}",
                        exc_info=True,
                    )
                    processed += 1
                    continue

                if I_tile is not None and W_tile is not None:
                    mask = W_tile > 0
                    tgt_sum = fsum[ymin:ymax, xmin:xmax]
                    tgt_wgt = fwei[ymin:ymax, xmin:xmax]
                    for c in range(n_channels):
                        tgt_sum[..., c][mask] += I_tile[..., c][mask] * W_tile[mask]
                    tgt_wgt[mask] += W_tile[mask]
                    if alpha_tile is None or alpha_tile.shape != W_tile.shape:
                        alpha_tile = np.where(W_tile > 0, 1.0, 0.0).astype(np.float32, copy=False)
                    else:
                        alpha_tile = np.clip(np.asarray(alpha_tile, dtype=np.float32, copy=False), 0.0, 1.0)
                    tgt_alpha = falpha[ymin:ymax, xmin:xmax]
                    np.maximum(tgt_alpha, alpha_tile, out=tgt_alpha)
                    tiles_since_flush += 1
                    if tiles_since_flush >= FLUSH_BATCH_SIZE:
                        hsum.flush()
                        hwei.flush()
                        halpha.flush()
                        tiles_since_flush = 0

                processed += 1
                now = time.time()
                step_times.append(now - last_time)
                last_time = now
                if progress_callback:
                    try:
                        progress_callback("phase5_incremental", processed, total_steps)
                    except Exception:
                        pass
                if processed % FLUSH_BATCH_SIZE == 0 or processed == total_tiles:
                    pcb_asm(
                        "assemble_progress_tiles_processed_inc",
                        prog=None,
                        lvl="INFO_DETAIL",
                        num_done=processed,
                        total_num=total_tiles,
                    )

                    # --- Calcul et mise à jour de l’ETA global ---
                    elapsed_inc = time.monotonic() - start_time_inc
                    time_per_tile = elapsed_inc / processed
                    eta_tiles_sec = (total_tiles - processed) * time_per_tile

                    if (
                        progress_weight_phase5
                        and start_time_total_run is not None
                        and total_tiles > 0
                    ):
                        try:
                            current_progress_pct = base_progress_phase5 + (
                                (processed / total_tiles) * progress_weight_phase5
                            )
                            current_progress_pct = max(
                                current_progress_pct,
                                base_progress_phase5 + 0.01,
                            )
                            elapsed_total = time.monotonic() - start_time_total_run
                            sec_per_pct = (
                                elapsed_total / current_progress_pct
                                if current_progress_pct > 0
                                else 0.0
                            )
                            total_eta_sec = eta_tiles_sec + (
                                max(0.0, 100 - current_progress_pct) * sec_per_pct
                            )
                            update_gui_eta(total_eta_sec)
                        except Exception:
                            pass
                if stats_callback:
                    try:
                        stats_callback(processed, total_tiles, False)
                    except Exception:
                        pass

            if tiles_since_flush > 0:
                hsum.flush()
                hwei.flush()
                halpha.flush()
                tiles_since_flush = 0
    except Exception as e_pool:
        pcb_asm("assemble_error_incremental_pool_failed", prog=None, lvl="ERROR", error=str(e_pool))
        logger.error("Error during incremental assembly", exc_info=True)
        return None, None, None

    with fits.open(sum_path, memmap=True) as hsum, \
            fits.open(weight_path, memmap=True) as hwei, \
            fits.open(alpha_path, memmap=True) as halpha:
        sum_data = hsum[0].data.astype(np.float32)
        weight_data = hwei[0].data.astype(np.float32)
        alpha_data = halpha[0].data.astype(np.float32)
        mosaic = np.zeros_like(sum_data, dtype=np.float32)
        np.divide(sum_data, weight_data[..., None], out=mosaic, where=weight_data[..., None] > 0)

    if step_times:
        avg_step = sum(step_times) / len(step_times)
        total_elapsed = time.time() - start_time_iter
        pcb_asm(
            "assemble_debug_incremental_timing",
            prog=None,
            lvl="DEBUG_DETAIL",
            avg=f"{avg_step:.2f}",
            total=f"{total_elapsed:.2f}",
        )

    pcb_asm("assemble_info_finished_incremental", prog=None, lvl="INFO", shape=str(mosaic.shape))

    # Harmonize incremental output with reproject/coadd by removing empty borders
    try:
        mosaic, weight_data, alpha_data = _auto_crop_mosaic_to_valid_region(
            mosaic,
            weight_data,
            final_output_wcs,
            log_callback=pcb_asm,
            alpha_map=alpha_data,
        )
        pcb_asm(
            "assemble_info_incremental_autocrop_done",
            prog=None,
            lvl="INFO_DETAIL",
            shape=str(getattr(mosaic, "shape", None)),
        )
    except Exception as exc_crop:
        pcb_asm(
            "assemble_warn_incremental_autocrop_failed",
            prog=None,
            lvl="WARN",
            error=str(exc_crop),
        )

    if mosaic.ndim != 3:
        raise ValueError(f"Expected incremental mosaic in HWC order, got {mosaic.shape}")
    logger.debug("Mosaic shape (HWC): %s", mosaic.shape)
    if logger.isEnabledFor(logging.DEBUG):
        mask = weight_data > 0
        if np.any(mask):
            stats = []
            for c in range(mosaic.shape[-1]):
                vals = mosaic[..., c][mask]
                if vals.size:
                    stats.append((float(np.mean(vals)), float(np.std(vals))))
                else:
                    stats.append((float("nan"), float("nan")))
            logger.debug(
                "Incremental overlap stats (mean/std per channel): %s",
                stats,
            )

    if cleanup_memmap:
        for p in (sum_path, weight_path, alpha_path):
            try:
                Path(p).unlink()
            except OSError:
                pass

        if internal_temp_dir:
            try:
                memmap_dir_path.rmdir()
            except OSError:
                pass


    return mosaic, weight_data, alpha_data

def _reproject_and_coadd_channel_worker(channel_data_list, output_wcs_header, output_shape_hw, match_bg, mm_sum_prefix=None, mm_cov_prefix=None):
    """Worker function to run reproject_and_coadd in a separate process."""
    from astropy.wcs import WCS
    from reproject import reproject_interp
    import numpy as np

    final_wcs = WCS(output_wcs_header)
    data_list = []
    wcs_list = []
    for arr, hdr in channel_data_list:
        data_list.append(arr)
        wcs_list.append(WCS(hdr))




    # The memmap prefixes are produced by other workers. Ensure they exist before
    # reading if provided. Wait here until both files are fully written.

    import inspect
    sig = inspect.signature(reproject_and_coadd)
    bg_kw = "match_background" if "match_background" in sig.parameters else (
        "match_bg" if "match_bg" in sig.parameters else None
    )

    kwargs = {
        "output_projection": final_wcs,
        "shape_out": output_shape_hw,
        "reproject_function": reproject_interp,
        "combine_function": "mean",
    }
    if bg_kw:
        kwargs[bg_kw] = match_bg

    stacked, coverage = reproject_and_coadd_wrapper(
        data_list=data_list,
        wcs_list=wcs_list,
        shape_out=output_shape_hw,
        output_projection=final_wcs,
        use_gpu=False,
        cpu_func=reproject_and_coadd,
        **kwargs,
    )

    if mm_sum_prefix and mm_cov_prefix:
        _wait_for_memmap_files([mm_sum_prefix, mm_cov_prefix])
    return stacked.astype(np.float32), coverage.astype(np.float32)


def assemble_final_mosaic_reproject_coadd(
    master_tile_fits_with_wcs_list: list,
    final_output_wcs: WCS,
    final_output_shape_hw: tuple,
    progress_callback: callable,
    n_channels: int = 3,
    match_bg: bool = True,
    apply_crop: bool = False,
    crop_percent: float = 0.0,
    use_memmap: bool = False,
    memmap_dir: str | None = None,
    cleanup_memmap: bool = True,
    assembly_process_workers: int = 0,
    re_solve_cropped_tiles: bool = False,
    solver_settings: dict | None = None,
    solver_instance=None,
    use_gpu: bool = False,
    base_progress_phase5: float | None = None,
    progress_weight_phase5: float | None = None,
    start_time_total_run: float | None = None,
    intertile_photometric_match: bool = False,
    intertile_preview_size: int = 512,
    intertile_overlap_min: float = 0.05,
    intertile_sky_percentile: tuple[float, float] | list[float] = (30.0, 70.0),
    intertile_robust_clip_sigma: float = 2.5,
    intertile_global_recenter: bool = True,
    intertile_recenter_clip: tuple[float, float] | list[float] | None = (0.85, 1.18),
    use_auto_intertile: bool = False,
    collect_tile_data: list | None = None,
    tile_affine_corrections: list[tuple[float, float]] | None = None,
    global_anchor_shift: tuple[float, float] | None = None,
    phase45_enabled: bool = False,
    parallel_plan: ParallelPlan | None = None,
    enable_tile_weighting: bool = False,
    tile_weight_mode: str | None = None,
    stats_callback: Callable[[int, int, bool], None] | None = None,
):
    """Assemble les master tiles en utilisant ``reproject_and_coadd``."""
    _pcb = lambda msg_key, prog=None, lvl="INFO_DETAIL", **kwargs: _log_and_callback(
        msg_key, prog, lvl, callback=progress_callback, **kwargs
    )

    _log_memory_usage(progress_callback, "Début assemble_final_mosaic_reproject_coadd")
    _pcb(
        f"ASM_REPROJ_COADD: Options de rognage - Appliquer: {apply_crop}, Pourcentage: {crop_percent if apply_crop else 'N/A'}",
        lvl="DEBUG_DETAIL",
    )

    explicit_workers = None
    try:
        if assembly_process_workers and int(assembly_process_workers) > 0:
            explicit_workers = int(assembly_process_workers)
            assembly_process_workers = explicit_workers
    except Exception:
        explicit_workers = None

    start_time_phase = time.monotonic()
    plan_rows_cpu_hint: int | None = None
    plan_rows_gpu_hint: int | None = None
    plan_chunk_cpu_hint: int | None = None
    plan_chunk_gpu_hint: int | None = None
    cpu_workers_hint = None
    if parallel_plan is not None:
        cpu_workers_hint = getattr(parallel_plan, "cpu_workers", None)
        try:
            cpu_workers_hint = int(cpu_workers_hint)
        except Exception:
            cpu_workers_hint = None
        if explicit_workers is None and cpu_workers_hint and cpu_workers_hint > 0:
            assembly_process_workers = cpu_workers_hint
        use_memmap = bool(getattr(parallel_plan, "use_memmap", use_memmap))
        row_cpu = getattr(parallel_plan, "rows_per_chunk", None)
        if row_cpu is not None:
            try:
                plan_rows_cpu_hint = max(1, int(row_cpu))
            except Exception:
                plan_rows_cpu_hint = None
        row_gpu = getattr(parallel_plan, "gpu_rows_per_chunk", None)
        if row_gpu is not None:
            try:
                plan_rows_gpu_hint = max(1, int(row_gpu))
            except Exception:
                plan_rows_gpu_hint = None
        chunk_cpu = getattr(parallel_plan, "max_chunk_bytes", None)
        if chunk_cpu is not None:
            try:
                plan_chunk_cpu_hint = max(1, int(chunk_cpu))
            except Exception:
                plan_chunk_cpu_hint = None
        chunk_gpu = getattr(parallel_plan, "gpu_max_chunk_bytes", None)
        if chunk_gpu is not None:
            try:
                plan_chunk_gpu_hint = max(1, int(chunk_gpu))
            except Exception:
                plan_chunk_gpu_hint = None
    try:
        _pcb(
            "phase5_plan_hints",
            prog=None,
            lvl="INFO_DETAIL",
            cpu_workers=int(cpu_workers_hint or 0),
            gpu=bool(use_gpu),
            rows_cpu=int(plan_rows_cpu_hint or 0),
            rows_gpu=int(plan_rows_gpu_hint or 0),
            chunk_cpu_mb=float(plan_chunk_cpu_hint / (1024 ** 2)) if plan_chunk_cpu_hint else 0.0,
            chunk_gpu_mb=float(plan_chunk_gpu_hint / (1024 ** 2)) if plan_chunk_gpu_hint else 0.0,
            memmap=bool(use_memmap),
        )
    except Exception:
        pass

    # Emit ETA during the preparation phase (before channels start)
    def _update_eta_prepare(done_tiles: int, total_tiles_local: int):
        if (
            base_progress_phase5 is None
            or progress_weight_phase5 is None
            or start_time_total_run is None
        ):
            return
        try:
            prep_fraction = 0.0
            if total_tiles_local > 0:
                prep_fraction = max(0.0, min(1.0, float(done_tiles) / float(total_tiles_local)))
            # Use a small pseudo progress for ETA only to avoid 0%% division
            current_progress_pct = base_progress_phase5 + (0.1 * prep_fraction) * progress_weight_phase5
            current_progress_pct = max(current_progress_pct, base_progress_phase5 + 0.01)
            elapsed_phase_local = time.monotonic() - start_time_phase
            eta_pre_sec = 0.0
            if done_tiles > 0 and total_tiles_local > 0:
                time_per_tile = elapsed_phase_local / float(done_tiles)
                eta_pre_sec = max(0.0, (total_tiles_local - done_tiles) * time_per_tile)
            elapsed_total = time.monotonic() - start_time_total_run
            sec_per_pct = elapsed_total / max(1.0, current_progress_pct)
            total_eta_sec = eta_pre_sec + (100 - current_progress_pct) * sec_per_pct
            h, rem = divmod(int(total_eta_sec), 3600)
            m, s = divmod(rem, 60)
            _pcb(f"ETA_UPDATE:{h:02d}:{m:02d}:{s:02d}", prog=None, lvl="ETA_LEVEL")
        except Exception:
            pass

    def _update_eta(completed_channels: int):
        if (
            base_progress_phase5 is not None
            and progress_weight_phase5 is not None
            and start_time_total_run is not None
            and completed_channels > 0
        ):
            elapsed_phase = time.monotonic() - start_time_phase
            time_per_ch = elapsed_phase / completed_channels
            eta_ch_sec = (n_channels - completed_channels) * time_per_ch
            current_progress_pct = base_progress_phase5 + (
                completed_channels / n_channels
            ) * progress_weight_phase5
            elapsed_total = time.monotonic() - start_time_total_run
            # Avoid zero-division at early stage; use at least 1%% of run for denominator
            sec_per_pct = elapsed_total / max(1.0, current_progress_pct)
            total_eta_sec = eta_ch_sec + (100 - current_progress_pct) * sec_per_pct
            h, rem = divmod(int(total_eta_sec), 3600)
            m, s = divmod(rem, 60)
            _pcb(
                f"ETA_UPDATE:{h:02d}:{m:02d}:{s:02d}",
                prog=None,
                lvl="ETA_LEVEL",
            )

    # Ensure wrapper uses the possibly monkeypatched CPU implementation
    try:
        zemosaic_utils.cpu_reproject_and_coadd = reproject_and_coadd
    except Exception:
        pass


    if not (REPROJECT_AVAILABLE and reproject_and_coadd and ASTROPY_AVAILABLE and fits):
        missing_deps = []
        if not REPROJECT_AVAILABLE or not reproject_and_coadd:
            missing_deps.append("Reproject")
        if not ASTROPY_AVAILABLE or not fits:
            missing_deps.append("Astropy (fits)")
        _pcb(
            "assemble_error_core_deps_unavailable_reproject_coadd",
            prog=None,
            lvl="ERROR",
            missing=", ".join(missing_deps),
        )
        return None, None, None

    if not master_tile_fits_with_wcs_list:
        _pcb("assemble_error_no_tiles_provided_reproject_coadd", prog=None, lvl="ERROR")
        return None, None, None

    if (
        not isinstance(final_output_shape_hw, (tuple, list))
        or len(final_output_shape_hw) != 2
    ):
        _pcb(
            "assemble_error_invalid_final_shape_reproj_coadd",
            prog=None,
            lvl="ERROR",
            shape=str(final_output_shape_hw),
        )
        return None, None, None

    h, w = map(int, final_output_shape_hw)

    try:
        w_wcs = int(getattr(final_output_wcs, "pixel_shape", (w, h))[0])
        h_wcs = int(getattr(final_output_wcs, "pixel_shape", (w, h))[1])
    except Exception:
        w_wcs = int(getattr(final_output_wcs.wcs, "naxis1", w)) if hasattr(final_output_wcs, "wcs") else w
        h_wcs = int(getattr(final_output_wcs.wcs, "naxis2", h)) if hasattr(final_output_wcs, "wcs") else h

    expected_hw = (h_wcs, w_wcs)
    if (h, w) != expected_hw:
        if (w, h) == expected_hw:
            _pcb(
                "assemble_warn_swapped_final_shape_reproj_coadd",
                prog=None,
                lvl="WARN",
                provided=str(final_output_shape_hw),
                expected=str(expected_hw),
            )
            h, w = expected_hw
            final_output_shape_hw = (h, w)
        else:
            _pcb(
                "assemble_error_mismatch_final_shape_reproj_coadd",
                prog=None,
                lvl="ERROR",
                provided=str(final_output_shape_hw),
                expected=str(expected_hw),
            )
            return None, None, None

    # Convertir la sortie WCS en header FITS si possible une seule fois
    output_header = (
        final_output_wcs.to_header()
        if hasattr(final_output_wcs, "to_header")
        else final_output_wcs
    )
    weight_mode_normalized = str(tile_weight_mode or "n_frames").strip().lower()
    tile_weighting_active = bool(enable_tile_weighting)
    if weight_mode_normalized not in {"n_frames"}:
        tile_weighting_active = False
    tile_weight_values: list[float] = []

    def _extract_tile_weight(header_obj) -> float | None:
        if header_obj is None:
            return None
        getter = header_obj.get if hasattr(header_obj, "get") else None
        for key in ("MT_NFRAMES", "ZMT_NALGN", "ZMT_NRAW"):
            try:
                value = getter(key) if getter else header_obj[key]  # type: ignore[index]
            except Exception:
                value = None
            if value is None:
                continue
            try:
                value_f = float(value)
            except Exception:
                continue
            if math.isfinite(value_f) and value_f > 0:
                return value_f
        return None


    effective_tiles: list[dict[str, Any]] = []
    hdr_for_output = None
    total_tiles_for_prep = len(master_tile_fits_with_wcs_list)
    for idx, (tile_path, tile_wcs) in enumerate(master_tile_fits_with_wcs_list, 1):
        tile_header = None
        alpha_mask_arr: np.ndarray | None = None
        with fits.open(tile_path, memmap=False) as hdul:
            primary_hdu = hdul[0]
            data = np.asarray(primary_hdu.data, dtype=np.float32, order="C")
            try:
                tile_header = primary_hdu.header.copy()
            except Exception:
                tile_header = None
            if "ALPHA" in hdul and hdul["ALPHA"].data is not None:
                try:
                    alpha_mask_arr = np.asarray(hdul["ALPHA"].data, dtype=np.float32, copy=False)
                except Exception:
                    alpha_mask_arr = None

        # Master tiles saved via ``save_fits_image`` use the ``HWC`` axis order
        # which stores color images in ``C x H x W`` within the FITS file. When
        # reading them back for final assembly we expect ``H x W x C``.
        if data.ndim == 3 and data.shape[0] in (1, 3) and data.shape[-1] != data.shape[0]:
            data = np.moveaxis(data, 0, -1)
        if data.ndim == 2:
            data = data[..., np.newaxis]

        if alpha_mask_arr is not None:
            alpha_mask_arr = np.squeeze(alpha_mask_arr)
            if alpha_mask_arr.ndim == 3 and alpha_mask_arr.shape[0] == 1:
                alpha_mask_arr = alpha_mask_arr[0]
            alpha_mask_arr = np.nan_to_num(alpha_mask_arr, nan=0.0, posinf=0.0, neginf=0.0)
            max_alpha_val = float(np.nanmax(alpha_mask_arr)) if alpha_mask_arr.size else 0.0
            if alpha_mask_arr.dtype.kind in {"i", "u"} and max_alpha_val > 1.0:
                alpha_mask_arr = alpha_mask_arr.astype(np.float32, copy=False) / 255.0
            elif alpha_mask_arr.dtype.kind not in {"f"}:
                alpha_mask_arr = alpha_mask_arr.astype(np.float32, copy=False)
            alpha_mask_arr = np.clip(alpha_mask_arr, 0.0, 1.0)

        if (
            apply_crop
            and crop_percent > 1e-3
            and ZEMOSAIC_UTILS_AVAILABLE
            and hasattr(zemosaic_utils, "crop_image_and_wcs")
        ):
            try:
                original_hw = data.shape[:2]
                cropped, cropped_wcs = zemosaic_utils.crop_image_and_wcs(
                    data,
                    tile_wcs,
                    crop_percent / 100.0,
                    progress_callback=None,
                )
                if cropped is not None and cropped_wcs is not None:
                    data = cropped
                    tile_wcs = cropped_wcs
                    if (
                        alpha_mask_arr is not None
                        and alpha_mask_arr.shape == original_hw
                        and data.shape[:2] != original_hw
                    ):
                        dh = (original_hw[0] - data.shape[0]) // 2
                        dw = (original_hw[1] - data.shape[1]) // 2
                        top = max(dh, 0)
                        left = max(dw, 0)
                        bottom = top + data.shape[0]
                        right = left + data.shape[1]
                        alpha_mask_arr = alpha_mask_arr[top:bottom, left:right]
                    elif alpha_mask_arr is not None and alpha_mask_arr.shape != data.shape[:2]:
                        alpha_mask_arr = None
            except Exception:
                pass

        if re_solve_cropped_tiles and solver_instance is not None:
            try:
                hdr = fits.Header()
                hdr['BITPIX'] = -32
                if 'BSCALE' in hdr:
                    del hdr['BSCALE']
                if 'BZERO' in hdr:
                    del hdr['BZERO']
                use_hints = solver_settings.get("use_radec_hints", False) if solver_settings else False
                if use_hints and hasattr(tile_wcs, "wcs"):
                    cx = tile_wcs.pixel_shape[0] / 2
                    cy = tile_wcs.pixel_shape[1] / 2
                    ra_dec = tile_wcs.wcs_pix2world([[cx, cy]], 0)[0]
                    hdr["RA"] = ra_dec[0]
                    hdr["DEC"] = ra_dec[1]
                solver_instance.solve(
                    str(tile_path), hdr, solver_settings or {}, update_header_with_solution=True
                )
                hdr_for_output = hdr
            except Exception:
                pass
        if alpha_mask_arr is not None:
            if alpha_mask_arr.shape != data.shape[:2]:
                logger.debug(
                    "[Alpha] Master tile mask shape mismatch: tile=%s, mask_shape=%s, data_shape=%s",
                    _safe_basename(tile_path),
                    alpha_mask_arr.shape,
                    data.shape,
                )
                alpha_mask_arr = None
            else:
                zero_mask = alpha_mask_arr <= ALPHA_OPACITY_THRESHOLD
                if np.any(zero_mask):
                    data = np.asarray(data, dtype=np.float32, order="C", copy=True)
                    data[zero_mask[..., None]] = np.nan

        if alpha_mask_arr is not None:
            coverage_mask_entry = alpha_mask_arr.astype(np.float32, copy=True)
        else:
            valid_pixels = (
                np.any(np.isfinite(data), axis=-1) if data.ndim == 3 else np.isfinite(data)
            )
            coverage_mask_entry = valid_pixels.astype(np.float32)

        tile_weight_value = 1.0
        if tile_weighting_active and weight_mode_normalized == "n_frames":
            weight_candidate = _extract_tile_weight(tile_header)
            if weight_candidate is not None:
                tile_weight_value = float(weight_candidate)
        if not math.isfinite(tile_weight_value) or tile_weight_value <= 0:
            tile_weight_value = 1.0
        if tile_weighting_active:
            tile_weight_values.append(tile_weight_value)

        tile_entry = {
            "data": data,
            "wcs": tile_wcs,
            "path": tile_path,
            "tile_id": _resolve_tile_identifier(tile_path, tile_header, idx - 1),
            "coverage_mask": coverage_mask_entry,
            "tile_weight": tile_weight_value,
        }
        effective_tiles.append(tile_entry)

        if idx % 10 == 0 or idx == len(master_tile_fits_with_wcs_list):
            _pcb(
                "assemble_progress_tiles_processed_inc",
                prog=None,
                lvl="INFO_DETAIL",
                num_done=idx,
                total_num=len(master_tile_fits_with_wcs_list),
            )

        # Keep ETA responsive during preparation
        if idx == 1 or (idx % 5 == 0) or (idx == total_tiles_for_prep):
            _update_eta_prepare(idx, total_tiles_for_prep)


    tile_weighting_applied = tile_weighting_active and bool(tile_weight_values)
    if tile_weighting_applied:
        weights_arr = np.asarray(tile_weight_values, dtype=np.float64)
        if weights_arr.size:
            w_min = float(np.nanmin(weights_arr))
            w_max = float(np.nanmax(weights_arr))
            w_mean = float(np.nanmean(weights_arr))
            try:
                _pcb("[INFO] Tile-weighting enabled — mode=N_FRAMES", prog=None, lvl="INFO")
                _pcb(
                    f"[INFO] Weights summary: min={w_min:.3g}, max={w_max:.3g}, mean={w_mean:.3g}",
                    prog=None,
                    lvl="INFO",
                )
            except Exception:
                pass
        else:
            tile_weighting_applied = False
    else:
        tile_weighting_applied = False

    # Optional inter-tile photometric (gain/offset) calibration
    pending_affine_list, nontrivial_affine = _sanitize_affine_corrections(
        tile_affine_corrections,
        len(effective_tiles),
    )

    if (
        pending_affine_list is None
        and intertile_photometric_match
        and len(effective_tiles) >= 2
    ):
        tile_sources = []
        for entry in effective_tiles:
            tile_sources.append(
                _TileAffineSource(
                    path=entry.get("path"),
                    wcs=entry.get("wcs"),
                    data=entry.get("data"),
                )
            )

        pending_affine_list, nontrivial_affine, affine_status, affine_error = (
            _compute_intertile_affine_corrections_from_sources(
                sources=tile_sources,
                final_output_wcs=final_output_wcs,
                final_output_shape_hw=final_output_shape_hw,
                preview_size=int(intertile_preview_size),
                min_overlap_fraction=float(intertile_overlap_min),
                sky_percentile=intertile_sky_percentile,
                robust_clip_sigma=float(intertile_robust_clip_sigma),
                use_auto_intertile=use_auto_intertile,
                logger_obj=logger,
                progress_callback=progress_callback,
                intertile_global_recenter=bool(intertile_global_recenter),
                intertile_recenter_clip=intertile_recenter_clip,
            )
        )

        if affine_status == "preview_failed":
            _pcb(
                "assemble_warn_intertile_photometric_failed",
                prog=None,
                lvl="WARN",
                error="preview_failed",
            )
            pending_affine_list = None
            nontrivial_affine = False
        elif affine_status == "compute_failed":
            _pcb(
                "assemble_warn_intertile_photometric_failed",
                prog=None,
                lvl="WARN",
                error=str(affine_error),
            )
            pending_affine_list = None
            nontrivial_affine = False

    total_tiles_prepared = len(effective_tiles)
    pending_affine_list, anchor_shift_applied = _compose_global_anchor_shift(
        pending_affine_list,
        total_tiles_prepared,
        global_anchor_shift,
    )
    if anchor_shift_applied:
        nontrivial_affine = True

    affine_by_id: dict[str, tuple[float, float]] | None = None
    if pending_affine_list:
        affine_by_id, affine_mismatch_detail = _build_affine_lookup_for_tiles(
            effective_tiles,
            pending_affine_list,
        )
        if affine_mismatch_detail:
            logger.warning(
                "affine_list_mismatch: expected %d corrections for %d tiles, got %d — skip photometric corrections and rely on match_background",
                len(effective_tiles),
                len(effective_tiles),
                len(pending_affine_list or []),
            )
            logger.warning("affine_list_mismatch detail: %s", affine_mismatch_detail)
            pending_affine_list = None
            affine_by_id = None
            nontrivial_affine = False
        else:
            if phase45_enabled:
                logger.info(
                    "intertile: recomputed affine corrections on effective set: tiles=%d",
                    len(effective_tiles),
                )
            logger.info(
                "apply_photometric: using affine_by_id for %d tiles",
                len(effective_tiles),
            )
    else:
        nontrivial_affine = False
        pending_affine_list = None

    if pending_affine_list and affine_by_id:
        if use_gpu:
            nontrivial_affine = True
            for entry in effective_tiles:
                tile_id = entry.get("tile_id")
                if not tile_id:
                    continue
                gain_val, offset_val = affine_by_id.get(tile_id, (1.0, 0.0))
                logger.info(
                    "apply_photometric: tile=%s gain=%.5f offset=%.5f",
                    tile_id,
                    gain_val,
                    offset_val,
                )
        else:
            corrected_tiles = 0
            for tile_entry in effective_tiles:
                tile_id = tile_entry.get("tile_id")
                if not tile_id or tile_entry.get("data") is None:
                    continue
                gain_val, offset_val = affine_by_id.get(tile_id, (1.0, 0.0))
                try:
                    arr_np = np.asarray(tile_entry["data"], dtype=np.float32, order="C")
                    gain_to_apply = float(gain_val)
                    offset_to_apply = float(offset_val)
                    if match_bg:
                        gain_before = gain_to_apply
                        offset_before = offset_to_apply
                        if gain_to_apply < gain_limit_min:
                            gain_to_apply = gain_limit_min
                        elif gain_to_apply > gain_limit_max:
                            gain_to_apply = gain_limit_max
                        if affine_offset_limit_adu > 0.0:
                            if abs(offset_to_apply) > affine_offset_limit_adu:
                                offset_to_apply = 0.0
                            else:
                                offset_to_apply = max(-affine_offset_limit_adu, min(offset_to_apply, affine_offset_limit_adu))
                        if gain_to_apply != gain_before or offset_to_apply != offset_before:
                            try:
                                _pcb(
                                    "assemble_warn_affine_clamped",
                                    prog=None,
                                    lvl="INFO_DETAIL",
                                    tile_id=tile_id,
                                    gain_before=gain_before,
                                    gain_after=gain_to_apply,
                                    offset_before=offset_before,
                                    offset_after=offset_to_apply,
                                )
                            except Exception:
                                pass
                    if gain_to_apply != 1.0:
                        np.multiply(arr_np, gain_to_apply, out=arr_np, casting="unsafe")
                    if offset_to_apply != 0.0:
                        np.add(arr_np, offset_to_apply, out=arr_np, casting="unsafe")
                    tile_entry["data"] = arr_np
                    corrected_tiles += 1
                    logger.info(
                        "apply_photometric: tile=%s gain=%.5f offset=%.5f",
                        tile_id,
                        gain_to_apply,
                        offset_to_apply,
                    )
                except Exception:
                    continue
            if corrected_tiles:
                try:
                    _pcb(
                        "assemble_info_intertile_photometric_applied",
                        prog=None,
                        lvl="INFO_DETAIL",
                        num_tiles=corrected_tiles,
                    )
                except Exception:
                    pass
                nontrivial_affine = True
            else:
                nontrivial_affine = False
                pending_affine_list = None
    else:
        nontrivial_affine = False
        pending_affine_list = None

    if collect_tile_data is not None:
        try:
            collect_tile_data.clear()
        except Exception:
            collect_tile_data[:] = []
        for entry in effective_tiles:
            arr = entry.get("data") if isinstance(entry, dict) else None
            tile_wcs = entry.get("wcs") if isinstance(entry, dict) else None
            coverage_mask = entry.get("coverage_mask") if isinstance(entry, dict) else None
            if arr is None or tile_wcs is None:
                continue
            try:
                arr_copy = np.array(arr, copy=True)
            except Exception:
                try:
                    arr_copy = arr.copy()
                except Exception:
                    arr_copy = np.asarray(arr, dtype=np.float32)
            cov_copy = None
            if coverage_mask is not None:
                try:
                    cov_copy = np.array(coverage_mask, copy=True)
                except Exception:
                    cov_copy = np.asarray(coverage_mask, dtype=np.float32)
            collect_tile_data.append((arr_copy, tile_wcs, cov_copy))


    # Build kwargs dynamically to remain compatible with different reproject versions
    reproj_kwargs = {}
    process_workers_supported = False
    try:
        import inspect
        sig = inspect.signature(reproject_and_coadd)
        if "match_background" in sig.parameters:
            reproj_kwargs["match_background"] = match_bg
        elif "match_bg" in sig.parameters:
            reproj_kwargs["match_bg"] = match_bg
        if "process_workers" in sig.parameters:
            process_workers_supported = True
            reproj_kwargs["process_workers"] = assembly_process_workers
        if "use_memmap" in sig.parameters:
            reproj_kwargs["use_memmap"] = use_memmap
        elif "intermediate_memmap" in sig.parameters:
            reproj_kwargs["intermediate_memmap"] = use_memmap
        if "memmap_dir" in sig.parameters:
            reproj_kwargs["memmap_dir"] = memmap_dir
        if "cleanup_memmap" in sig.parameters:
            reproj_kwargs["cleanup_memmap"] = False
    except Exception:
        # If introspection fails just fall back to basic arguments
        reproj_kwargs = {"match_background": match_bg}
    if assembly_process_workers and not process_workers_supported:
        try:
            _pcb(
                "assemble_info_reproject_process_workers_unsupported",
                prog=None,
                lvl="INFO_DETAIL",
                workers=int(assembly_process_workers),
            )
        except Exception:
            pass

    # Provide GPU-only tuning hints (safely ignored by CPU via wrapper filtering)
    try:
        reproj_kwargs["bg_preview_size"] = int(max(128, int(intertile_preview_size)))
    except Exception:
        reproj_kwargs["bg_preview_size"] = 512
    try:
        reproj_kwargs["intertile_sky_percentile"] = (
            tuple(intertile_sky_percentile)
            if isinstance(intertile_sky_percentile, (list, tuple)) and len(intertile_sky_percentile) >= 2
            else (30.0, 70.0)
        )
    except Exception:
        reproj_kwargs["intertile_sky_percentile"] = (30.0, 70.0)
    try:
        reproj_kwargs["intertile_robust_clip_sigma"] = float(intertile_robust_clip_sigma)
    except Exception:
        reproj_kwargs["intertile_robust_clip_sigma"] = 2.5

    # Apply auto-tune chunking hints when available
    row_hint = None
    chunk_hint = None
    if use_gpu:
        row_hint = plan_rows_gpu_hint or plan_rows_cpu_hint
        chunk_hint = plan_chunk_gpu_hint or plan_chunk_cpu_hint
    else:
        row_hint = plan_rows_cpu_hint
        chunk_hint = plan_chunk_cpu_hint
    if row_hint:
        reproj_kwargs["rows_per_chunk"] = int(max(1, row_hint))
    if chunk_hint:
        reproj_kwargs["max_chunk_bytes"] = int(max(1, chunk_hint))

    # If we are going to use the GPU, pass the precomputed affine corrections down
    # so they are applied inside the GPU reprojection (parity with CPU path).
    if use_gpu and pending_affine_list is not None:
        reproj_kwargs["tile_affine_corrections"] = pending_affine_list
        try:
            _pcb(
                f"ASM_REPROJ_COADD: Passing intertile affine corrections to GPU (n={len(pending_affine_list)})",
                lvl="DEBUG_DETAIL",
            )
        except Exception:
            pass


    # Prepare output containers: either RAM lists or disk-backed memmaps
    mosaic_channels = []
    coverage = None
    mosaic_memmap = None
    coverage_memmap = None
    mosaic_mm_path = None
    coverage_mm_path = None
    if use_memmap:
        try:
            if memmap_dir:
                mm_dir_path = Path(memmap_dir).expanduser()
                mm_dir_path.mkdir(parents=True, exist_ok=True)
            else:
                mm_dir_path = Path(
                    tempfile.mkdtemp(prefix="zemosaic_coadd_", dir=str(get_runtime_temp_dir()))
                )
            mosaic_mm_path = mm_dir_path / f"mosaic_{h}x{w}x{n_channels}.dat"
            coverage_mm_path = mm_dir_path / f"coverage_{h}x{w}.dat"
            mosaic_memmap = np.memmap(str(mosaic_mm_path), dtype=np.float32, mode='w+', shape=(h, w, n_channels))
            coverage_memmap = np.memmap(str(coverage_mm_path), dtype=np.float32, mode='w+', shape=(h, w))
            _pcb(
                "assemble_debug_memmap_paths",
                prog=None,
                lvl="DEBUG_DETAIL",
                mosaic_path=str(mosaic_mm_path),
                coverage_path=str(coverage_mm_path),
            )
        except Exception as e_mm:
            mosaic_memmap = None
            coverage_memmap = None
            _pcb("assemble_warn_memmap_create_failed", prog=None, lvl="WARN", error=str(e_mm))
    try:
        total_steps = n_channels
        start_time_loop = time.time()
        last_time = start_time_loop
        step_times = []
        if use_gpu:
            try:
                _pcb(
                    f"ASM_REPROJ_COADD: GPU background match enabled (preview={reproj_kwargs.get('bg_preview_size','N/A')}, sky={reproj_kwargs.get('intertile_sky_percentile','N/A')}, clip={reproj_kwargs.get('intertile_robust_clip_sigma','N/A')})",
                    lvl="DEBUG_DETAIL",
                )
            except Exception:
                pass
        for ch in range(n_channels):
            valid_entries: list[dict[str, Any]] = []
            for entry in effective_tiles:
                arr = entry.get("data") if isinstance(entry, dict) else None
                if arr is None:
                    continue
                if arr.ndim != 3:
                    raise ValueError(
                        f"Master tile data must be HWC before channel slicing, got {arr.shape}"
                    )
                if ch >= arr.shape[-1]:
                    raise ValueError(
                        f"Channel index {ch} out of bounds for tile shape {arr.shape}"
                    )
                valid_entries.append(entry)

            data_list = [entry.get("data")[..., ch] for entry in valid_entries]
            wcs_list = [entry.get("wcs") for entry in valid_entries]
            weights_for_entries = None
            if tile_weighting_applied:
                weights_for_entries = [
                    float(entry.get("tile_weight", 1.0)) if isinstance(entry, dict) else 1.0
                    for entry in valid_entries
                ]

            reproj_call_kwargs = dict(reproj_kwargs)
            if use_gpu:
                for unsupported_kw in ("intertile_global_recenter",):
                    if unsupported_kw in reproj_call_kwargs:
                        reproj_call_kwargs.pop(unsupported_kw, None)
                        logger.debug(
                            "[GPU Reproject] Ignoring unsupported kwarg: %s",
                            unsupported_kw,
                        )

            def _invoke_reproject(local_kwargs: dict):
                invoke_kwargs = dict(local_kwargs)
                if tile_weighting_applied and weights_for_entries is not None:
                    invoke_kwargs["tile_weights"] = weights_for_entries
                return reproject_and_coadd_wrapper(
                    data_list=data_list,
                    wcs_list=wcs_list,
                    shape_out=final_output_shape_hw,
                    output_projection=output_header,
                    use_gpu=use_gpu,
                    cpu_func=reproject_and_coadd,
                    reproject_function=reproject_interp,
                    combine_function="mean",
                    progress_callback=_pcb,
                    **local_kwargs,
                )

            try:
                chan_mosaic, chan_cov = _invoke_reproject(reproj_call_kwargs)
            except TypeError as gpu_kw_err:
                if use_gpu:
                    logger.warning(
                        "[GPU Reproject] Unexpected kwargs triggered TypeError: %s",
                        gpu_kw_err,
                    )
                    retry_kwargs = reproj_call_kwargs.copy()
                    removed_after_error: list[str] = []
                    err_msg = str(gpu_kw_err)
                    for key in list(retry_kwargs.keys()):
                        if f"'{key}'" in err_msg:
                            retry_kwargs.pop(key, None)
                            removed_after_error.append(key)
                    if removed_after_error:
                        logger.debug(
                            "[GPU Reproject] Retrying without kwargs: %s",
                            ", ".join(removed_after_error),
                        )
                        chan_mosaic, chan_cov = _invoke_reproject(retry_kwargs)
                    else:
                        raise
                else:
                    raise
            # Store channel result to memmap if enabled, else keep in RAM list
            ch_f32 = chan_mosaic.astype(np.float32)
            if mosaic_memmap is not None:
                mosaic_memmap[..., ch] = ch_f32
                mosaic_memmap.flush()
                del ch_f32
            else:
                mosaic_channels.append(ch_f32)

            if coverage is None:
                cov_f32 = chan_cov.astype(np.float32)
                if coverage_memmap is not None:
                    coverage_memmap[:] = cov_f32
                    coverage_memmap.flush()
                    coverage = coverage_memmap
                else:
                    coverage = cov_f32
            now = time.time()
            step_times.append(now - last_time)
            last_time = now
            if progress_callback:
                try:
                    progress_callback("phase5_reproject", ch + 1, total_steps)
                except Exception:
                    pass
            _update_eta(ch + 1)
            if stats_callback:
                try:
                    tiles_total_ctx = total_tiles_prepared or len(valid_entries)
                    if tiles_total_ctx <= 0:
                        tiles_total_ctx = len(valid_entries)
                    tiles_done_est = tiles_total_ctx
                    if tiles_total_ctx and total_steps:
                        tiles_done_est = int(
                            min(
                                tiles_total_ctx,
                                math.ceil(((ch + 1) / total_steps) * tiles_total_ctx),
                            )
                        )
                    stats_callback(tiles_done_est, tiles_total_ctx, False)
                except Exception:
                    pass
            _log_memory_usage(progress_callback, f"Phase5 Reproject: mémoire après canal {ch+1}")
    except Exception as e_reproject:
        _pcb("assemble_error_reproject_coadd_call_failed", lvl="ERROR", error=str(e_reproject))
        logger.error(
            "Erreur fatale lors de l'appel à reproject_and_coadd:",
            exc_info=True,
        )
        return None, None, None

    if mosaic_memmap is not None:
        mosaic_data = mosaic_memmap
    else:
        mosaic_data = np.stack(mosaic_channels, axis=-1)
    if step_times:
        avg_step = sum(step_times) / len(step_times)
        total_elapsed = time.time() - start_time_loop
        _pcb(
            "assemble_debug_reproject_timing",
            prog=None,
            lvl="DEBUG_DETAIL",
            avg=f"{avg_step:.2f}",
            total=f"{total_elapsed:.2f}",
        )
    if re_solve_cropped_tiles and solver_instance is not None and hdr_for_output is not None:
        try:
            fits.writeto("final_mosaic.fits", mosaic_data.astype(np.float32), hdr_for_output, overwrite=True)
        except Exception:
            pass

    alpha_union = None
    try:
        alpha_union = _build_alpha_union_map(
            master_tile_fits_with_wcs_list,
            final_output_wcs,
            final_output_shape_hw,
            apply_crop=apply_crop,
            crop_percent=crop_percent,
            progress_callback=_pcb,
        )
    except Exception:
        alpha_union = None

    mosaic_data, coverage, alpha_union = _auto_crop_mosaic_to_valid_region(
        mosaic_data,
        coverage,
        final_output_wcs,
        log_callback=_pcb,
        alpha_map=alpha_union,
    )

    # --- ALPHA FINAL: propagate union alpha to FITS + PNG ---
    alpha_final = None
    try:
        if alpha_union is not None:
            a = np.asarray(alpha_union)
            if a.ndim == 3 and a.shape[-1] == 1:
                a = a[..., 0]
            elif a.ndim > 2:
                a = np.squeeze(a)
            a = np.nan_to_num(a, nan=0.0, posinf=0.0, neginf=0.0)
            if a.dtype == np.bool_:
                a = a.astype(np.uint8) * 255
            elif np.issubdtype(a.dtype, np.floating):
                max_val = float(np.nanmax(a)) if a.size else 0.0
                min_val = float(np.nanmin(a)) if a.size else 0.0
                if max_val <= 1.0 + 1e-6 and min_val >= -1e-6:
                    a = (np.clip(a, 0.0, 1.0) * 255.0).astype(np.uint8)
                else:
                    a = np.clip(a, 0.0, 255.0).astype(np.uint8)
            elif a.dtype != np.uint8:
                a = np.clip(a, 0, 255).astype(np.uint8)
            if mosaic_data is not None and a.shape[:2] != mosaic_data.shape[:2]:
                try:
                    import cv2

                    a = cv2.resize(
                        a,
                        (mosaic_data.shape[1], mosaic_data.shape[0]),
                        interpolation=cv2.INTER_NEAREST,
                    )
                except Exception:
                    if (
                        a.shape[0] >= mosaic_data.shape[0]
                        and a.shape[1] >= mosaic_data.shape[1]
                    ):
                        a = a[: mosaic_data.shape[0], : mosaic_data.shape[1]]
                    else:
                        raise ValueError(
                            f"alpha_union shape {a.shape} mismatch with mosaic {mosaic_data.shape}"
                        )
            threshold_u8 = int(ALPHA_OPACITY_THRESHOLD * 255)
            if threshold_u8 > 0:
                a = np.where(a >= threshold_u8, 255, 0).astype(np.uint8, copy=False)
            alpha_final = np.ascontiguousarray(a, dtype=np.uint8)
            logger.info(
                "phase6: alpha_union received, propagating to alpha_final (uint8 %s)",
                alpha_final.shape,
            )
    except Exception as e_alpha_norm:
        alpha_final = None
        logger.warning("phase6: alpha propagation failed: %s", e_alpha_norm)

    # Defer memmap cleanup to Phase 6 after final save

    _log_memory_usage(progress_callback, "Fin assemble_final_mosaic_reproject_coadd")
    _pcb(
        "assemble_info_finished_reproject_coadd",
        prog=None,
        lvl="INFO",
        shape=mosaic_data.shape if mosaic_data is not None else "N/A",
    )

    if mosaic_data is not None:
        if mosaic_data.ndim != 3:
            raise ValueError(f"Expected final mosaic in HWC order, got {mosaic_data.shape}")
        logger.debug("Mosaic shape (HWC): %s", mosaic_data.shape)
        if isinstance(coverage, np.ndarray) and logger.isEnabledFor(logging.DEBUG):
            mask = coverage > 0
            if np.any(mask):
                stats = []
                for c in range(mosaic_data.shape[-1]):
                    vals = mosaic_data[..., c][mask]
                    if vals.size:
                        stats.append((float(np.mean(vals)), float(np.std(vals))))
                    else:
                        stats.append((float("nan"), float("nan")))
                logger.debug(
                    "Reproject overlap stats (mean/std per channel): %s",
                    stats,
                )

    _update_eta(n_channels)

    return (
        mosaic_data.astype(np.float32),
        coverage.astype(np.float32),
        alpha_final,
    )

def _load_master_tiles_for_two_pass(
    master_tile_fits_with_wcs_list: list,
    *,
    apply_crop: bool,
    crop_percent: float,
    logger=None,
):
    """Load master tiles from disk for the coverage renormalization pass."""
    tiles: list[np.ndarray] = []
    tiles_wcs: list[Any] = []
    coverage_masks: list[np.ndarray | None] = []
    if not master_tile_fits_with_wcs_list:
        return tiles, tiles_wcs, coverage_masks
    for tile_path, tile_wcs in master_tile_fits_with_wcs_list:
        if not tile_path or not _path_exists(tile_path) or tile_wcs is None:
            continue
        try:
            with fits.open(tile_path, memmap=False) as hdul:
                data = hdul[0].data.astype(np.float32)
                alpha_arr = None
                if "ALPHA" in hdul and hdul["ALPHA"].data is not None:
                    alpha_arr = np.asarray(hdul["ALPHA"].data, dtype=np.float32, copy=False)
        except Exception as exc:
            if logger:
                logger.warning(
                    "[TwoPass] Failed to load master tile %s: %s",
                    _safe_basename(tile_path),
                    exc,
                )
            continue
        if data.ndim == 3 and data.shape[0] in (1, 3) and data.shape[-1] != data.shape[0]:
            data = np.moveaxis(data, 0, -1)
        if data.ndim == 2:
            data = data[..., np.newaxis]
        coverage_map = None
        if alpha_arr is not None:
            alpha_arr = np.squeeze(alpha_arr)
            if alpha_arr.ndim > 2 and alpha_arr.shape[0] == 1:
                alpha_arr = alpha_arr[0]
            max_alpha = float(np.nanmax(alpha_arr)) if alpha_arr.size else 0.0
            if max_alpha > 1.0:
                alpha_arr = alpha_arr / max_alpha
            coverage_map = np.clip(alpha_arr, 0.0, 1.0).astype(np.float32, copy=False)
        else:
            arr_np = np.asarray(data, dtype=np.float32, copy=False)
            mask_valid = np.any(np.isfinite(arr_np), axis=-1) if arr_np.ndim == 3 else np.isfinite(arr_np)
            coverage_map = mask_valid.astype(np.float32)
        current_wcs = tile_wcs
        if (
            apply_crop
            and crop_percent > 1e-3
            and ZEMOSAIC_UTILS_AVAILABLE
            and hasattr(zemosaic_utils, "crop_image_and_wcs")
        ):
            try:
                cropped_img, cropped_wcs = zemosaic_utils.crop_image_and_wcs(
                    data,
                    copy.deepcopy(tile_wcs),
                    float(crop_percent) / 100.0,
                    progress_callback=None,
                )
                if cropped_img is not None and cropped_wcs is not None:
                    data = cropped_img
                    current_wcs = cropped_wcs
                    if coverage_map is not None:
                        try:
                            coverage_img = coverage_map[..., np.newaxis]
                            cropped_cov, _ = zemosaic_utils.crop_image_and_wcs(
                                coverage_img,
                                copy.deepcopy(tile_wcs),
                                float(crop_percent) / 100.0,
                                progress_callback=None,
                            )
                            if cropped_cov is not None:
                                coverage_map = np.squeeze(cropped_cov)
                        except Exception:
                            coverage_map = coverage_map[: data.shape[0], : data.shape[1]]
            except Exception as exc:
                if logger:
                    logger.debug(
                        "[TwoPass] Crop failed for %s: %s",
                        _safe_basename(tile_path),
                        exc,
                    )
        tiles.append(np.asarray(data, dtype=np.float32))
        tiles_wcs.append(current_wcs)
        coverage_masks.append(np.asarray(coverage_map, dtype=np.float32) if coverage_map is not None else None)
        if logger and len(tiles) <= 5:
            logger.debug(
                "[TwoPass] Loaded tile %s with shape=%s", _safe_basename(tile_path), np.asarray(data).shape
            )
    return tiles, tiles_wcs, coverage_masks


def _derive_final_alpha_mask(
    alpha_source: np.ndarray | None,
    mosaic_data: np.ndarray | None,
    coverage_data: np.ndarray | None,
    logger: logging.Logger,
) -> np.ndarray | None:
    """Normalize any available alpha signals into an 8-bit mask for Phase 6."""
    alpha_final: np.ndarray | None = None
    try:
        if isinstance(alpha_source, np.ndarray):
            alpha_candidate = np.asarray(alpha_source)
            if alpha_candidate.ndim == 3 and alpha_candidate.shape[-1] == 1:
                alpha_candidate = alpha_candidate[..., 0]
            elif alpha_candidate.ndim > 2:
                alpha_candidate = np.squeeze(alpha_candidate)
            alpha_candidate = np.nan_to_num(alpha_candidate, nan=0.0, posinf=0.0, neginf=0.0)
            if alpha_candidate.dtype == np.bool_:
                alpha_candidate = alpha_candidate.astype(np.uint8) * 255
            elif np.issubdtype(alpha_candidate.dtype, np.floating):
                max_val = float(np.nanmax(alpha_candidate)) if alpha_candidate.size else 0.0
                min_val = float(np.nanmin(alpha_candidate)) if alpha_candidate.size else 0.0
                if max_val <= 1.0 + 1e-6 and min_val >= -1e-6:
                    alpha_candidate = (np.clip(alpha_candidate, 0.0, 1.0) * 255.0).astype(np.uint8)
                else:
                    alpha_candidate = np.clip(alpha_candidate, 0.0, 255.0).astype(np.uint8)
            elif alpha_candidate.dtype != np.uint8:
                alpha_candidate = np.clip(alpha_candidate, 0, 255).astype(np.uint8)
            if isinstance(mosaic_data, np.ndarray) and alpha_candidate.shape[:2] != mosaic_data.shape[:2]:
                try:
                    import cv2  # type: ignore

                    alpha_candidate = cv2.resize(
                        alpha_candidate,
                        (mosaic_data.shape[1], mosaic_data.shape[0]),
                        interpolation=cv2.INTER_NEAREST,
                    )
                except Exception:
                    if alpha_candidate.shape[0] >= mosaic_data.shape[0] and alpha_candidate.shape[1] >= mosaic_data.shape[1]:
                        alpha_candidate = alpha_candidate[: mosaic_data.shape[0], : mosaic_data.shape[1]]
                    else:
                        alpha_candidate = None
            if alpha_candidate is not None:
                alpha_final = np.ascontiguousarray(alpha_candidate, dtype=np.uint8)
        elif isinstance(mosaic_data, np.ndarray):
            if coverage_data is not None:
                cov = np.nan_to_num(coverage_data, nan=0.0, neginf=0.0, posinf=0.0).astype(np.float32, copy=False)
                max_cov = float(np.nanmax(cov)) if np.any(cov) else 0.0
                if max_cov > 0:
                    alpha_final = (np.clip(cov / max_cov, 0.0, 1.0) * 255.0).astype(np.uint8)
                else:
                    mask_valid = (
                        np.any(np.isfinite(mosaic_data), axis=-1) if mosaic_data.ndim == 3 else np.isfinite(mosaic_data)
                    )
                    alpha_final = (mask_valid.astype(np.float32) * 255.0).astype(np.uint8)
            else:
                mask_valid = (
                    np.any(np.isfinite(mosaic_data), axis=-1) if mosaic_data.ndim == 3 else np.isfinite(mosaic_data)
                )
                alpha_final = (mask_valid.astype(np.float32) * 255.0).astype(np.uint8)
    except Exception as exc_alpha_final:
        logger.warning("phase6: alpha propagation fallback engaged: %s", exc_alpha_final)
        alpha_final = None
    return alpha_final


def _iter_row_chunks(total_rows: int, rows_per_chunk: int) -> Iterable[tuple[int, int]]:
    """Yield contiguous row slices with a fixed height."""
    if total_rows <= 0:
        return
    step = max(1, rows_per_chunk)
    for start in range(0, total_rows, step):
        end = min(total_rows, start + step)
        yield start, end


def _rows_from_chunk_bytes(
    width: int,
    itemsize: int,
    chunk_bytes: int | None,
    height: int,
    *,
    channels: int = 1,
) -> int | None:
    """Return an estimated rows_per_chunk derived from a byte budget."""
    if chunk_bytes is None or chunk_bytes <= 0 or width <= 0 or itemsize <= 0:
        return None
    bytes_per_row = max(1, width * max(1, channels) * itemsize)
    rows = max(1, chunk_bytes // bytes_per_row)
    return int(min(height, rows))


def _chunked_gaussian_blur(
    coverage: np.ndarray,
    sigma_px: int,
    *,
    use_gpu: bool,
    rows_per_chunk: int | None,
    chunk_bytes: int | None,
    progress_hook=None,
    logger=None,
) -> tuple[np.ndarray, str]:
    """Apply a Gaussian blur in row chunks to control RAM/VRAM usage."""
    h, w = coverage.shape
    if sigma_px <= 0 or h == 0 or w == 0:
        return coverage.copy(), "identity"
    margin = max(1, int(math.ceil(float(sigma_px) * 4.0)))
    if rows_per_chunk is None or rows_per_chunk <= 0:
        rows_per_chunk = _rows_from_chunk_bytes(w, coverage.itemsize, chunk_bytes, h) or h
    rows_per_chunk = max(rows_per_chunk, margin * 2)
    rows_per_chunk = min(rows_per_chunk, h)

    def _emit(idx: int, total: int) -> None:
        if progress_hook is None:
            return
        try:
            progress_hook(idx, total, "blur")
        except Exception:
            pass

    chunk_total = max(1, math.ceil(h / rows_per_chunk))
    if use_gpu and CUPY_AVAILABLE:
        try:
            import cupy as cp  # type: ignore
            from cupyx.scipy.ndimage import gaussian_filter as gpu_gaussian_filter  # type: ignore

            output = np.empty_like(coverage, dtype=np.float32)
            chunk_idx = 0
            for y0, y1 in _iter_row_chunks(h, rows_per_chunk):
                chunk_idx += 1
                pad_start = max(0, y0 - margin)
                pad_end = min(h, y1 + margin)
                cov_chunk_gpu = cp.asarray(coverage[pad_start:pad_end], dtype=cp.float32)
                blurred_gpu = gpu_gaussian_filter(cov_chunk_gpu, float(sigma_px), truncate=4.0)
                central = blurred_gpu[(y0 - pad_start) : (y1 - pad_start)]
                output[y0:y1] = cp.asnumpy(central)
                _emit(chunk_idx, chunk_total)
            return output.astype(np.float32, copy=False), "cupy_chunk"
        except Exception as exc_gpu:
            if logger:
                logger.debug("[TwoPass] cupy chunked blur failed: %s", exc_gpu)

    try:
        from scipy.ndimage import gaussian_filter  # type: ignore

        output = np.empty_like(coverage, dtype=np.float32)
        chunk_idx = 0
        for y0, y1 in _iter_row_chunks(h, rows_per_chunk):
            chunk_idx += 1
            pad_start = max(0, y0 - margin)
            pad_end = min(h, y1 + margin)
            blurred = gaussian_filter(
                coverage[pad_start:pad_end],
                sigma=float(sigma_px),
                truncate=4.0,
            )
            output[y0:y1] = blurred[(y0 - pad_start) : (y1 - pad_start)].astype(np.float32, copy=False)
            _emit(chunk_idx, chunk_total)
        return output.astype(np.float32, copy=False), "scipy_chunk"
    except Exception as exc_scipy:
        if logger:
            logger.debug("[TwoPass] scipy chunked blur failed: %s", exc_scipy)

    try:
        import cv2  # type: ignore

        output = np.empty_like(coverage, dtype=np.float32)
        k = max(3, int(2 * round(float(sigma_px) * 1.5) + 1))
        chunk_idx = 0
        for y0, y1 in _iter_row_chunks(h, rows_per_chunk):
            chunk_idx += 1
            pad_start = max(0, y0 - margin)
            pad_end = min(h, y1 + margin)
            blurred = cv2.GaussianBlur(
                coverage[pad_start:pad_end],
                (k, k),
                sigmaX=float(sigma_px),
            )
            output[y0:y1] = blurred[(y0 - pad_start) : (y1 - pad_start)].astype(np.float32, copy=False)
            _emit(chunk_idx, chunk_total)
        return output.astype(np.float32, copy=False), "cv2_chunk"
    except Exception as exc_cv:
        if logger:
            logger.debug("[TwoPass] Coverage blur fallback failed: %s", exc_cv)

    return coverage.copy(), "identity"


def _compute_tile_gain_task(args: tuple) -> tuple[int, float]:
    """
    Worker for per-tile gain computation. Arguments are packed to ease pickling.
    """
    (
        idx,
        tile,
        tile_wcs,
        final_wcs,
        coverage_shape,
        coverage_arr,
        scale_map,
        gain_min,
        gain_max,
        coverage_src,
    ) = args
    if tile is None or tile_wcs is None:
        return idx, 1.0
    shape = np.asarray(tile).shape
    if shape[0] <= 0 or shape[1] <= 0:
        return idx, 1.0
    cov_arr = None
    if coverage_src is not None:
        cov_arr = np.asarray(coverage_src, dtype=np.float32, copy=False)
        if cov_arr.ndim > 2:
            cov_arr = np.squeeze(cov_arr)
    if cov_arr is None:
        mask = np.ones(shape[:2], dtype=np.float32)
    else:
        try:
            mask = np.nan_to_num(cov_arr, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32, copy=False)
        except Exception:
            mask = np.ones(shape[:2], dtype=np.float32)
        if mask.shape != tuple(shape[:2]):
            try:
                mask = mask.reshape(shape[:2])
            except Exception:
                mask = np.ones(shape[:2], dtype=np.float32)
    try:
        reproj_mask, _ = reproject_interp(
            (mask, tile_wcs),
            final_wcs,
            shape_out=coverage_shape,
        )
    except Exception:
        return idx, 1.0
    valid = (reproj_mask > 1e-3) & (coverage_arr > 0.0)
    if not np.any(valid):
        return idx, 1.0
    med_gain = float(np.median(scale_map[valid]))
    return idx, float(np.clip(med_gain, gain_min, gain_max))


def compute_per_tile_gains_from_coverage(
    tiles: list[np.ndarray],
    tiles_wcs: list[Any],
    final_wcs: Any,
    coverage_p1: np.ndarray,
    *,
    sigma_px: int,
    gain_clip: tuple[float, float],
    logger=None,
    use_gpu: bool = False,
    tiles_coverage: list[np.ndarray | None] | None = None,
    parallel_plan: ParallelPlan | dict | None = None,
    cpu_workers: int | None = None,
    row_chunk_size: int | None = None,
    chunk_bytes_limit: int | None = None,
    progress_emitter=None,
) -> list[float]:
    """Compute multiplicative gains for each tile using the blurred coverage map."""
    tiles_total = len(tiles) if tiles else 0

    def _emit_progress(
        tiles_done: int,
        *,
        chunk_index: int | None = None,
        chunk_total: int | None = None,
        stage: str | None = None,
        force: bool = False,
    ) -> None:
        if progress_emitter is None:
            return
        try:
            progress_emitter(
                tiles_done,
                chunk_index=chunk_index,
                chunk_total=chunk_total,
                stage=stage,
                force=force,
            )
        except Exception:
            pass

    if logger:
        logger.debug(
            "[TwoPass] compute_per_tile_gains_from_coverage start: tiles=%d, coverage_shape=%s, sigma=%s, clip=%s",
            tiles_total,
            getattr(coverage_p1, "shape", None),
            sigma_px,
            gain_clip,
        )
    if coverage_p1 is None or coverage_p1.ndim != 2:
        raise ValueError("Coverage map must be 2D for gain estimation")
    coverage = np.asarray(coverage_p1, dtype=np.float32)
    if coverage.size == 0:
        raise ValueError("Coverage map is empty")
    if logger:
        logger.debug(
            "[TwoPass] Coverage stats before blur: shape=%s, min=%.4f, max=%.4f, mean=%.4f",
            coverage.shape,
            float(np.nanmin(coverage)),
            float(np.nanmax(coverage)),
            float(np.nanmean(coverage)),
        )
    sigma_px = int(max(0, sigma_px))
    gain_min, gain_max = map(float, gain_clip)
    if gain_min > gain_max:
        gain_min, gain_max = gain_max, gain_min
    height, width = coverage.shape
    plan_rows_hint = None
    plan_chunk_hint = None
    plan_workers_hint = None
    plan_ref = parallel_plan
    if plan_ref is not None:
        def _plan_value(key: str):
            try:
                return getattr(plan_ref, key)
            except Exception:
                if isinstance(plan_ref, dict):
                    return plan_ref.get(key)
            return None

        plan_rows_hint = _plan_value("gpu_rows_per_chunk" if use_gpu else "rows_per_chunk")
        plan_chunk_hint = _plan_value("gpu_max_chunk_bytes" if use_gpu else "max_chunk_bytes")
        plan_workers_hint = _plan_value("cpu_workers")
    if cpu_workers is None and plan_workers_hint:
        try:
            cpu_workers = int(plan_workers_hint)
        except Exception:
            cpu_workers = None
    if chunk_bytes_limit is None and plan_chunk_hint:
        try:
            chunk_bytes_limit = int(plan_chunk_hint)
        except Exception:
            chunk_bytes_limit = None
    if chunk_bytes_limit is not None and chunk_bytes_limit <= 0:
        chunk_bytes_limit = None
    if row_chunk_size is None and plan_rows_hint:
        try:
            row_chunk_size = int(plan_rows_hint)
        except Exception:
            row_chunk_size = None
    if row_chunk_size is None or row_chunk_size <= 0:
        row_chunk_size = _rows_from_chunk_bytes(width, coverage.itemsize, chunk_bytes_limit, height)
    if row_chunk_size is None or row_chunk_size <= 0:
        row_chunk_size = min(height, 512 if use_gpu else 1024)
    row_chunk_size = int(max(1, min(height, row_chunk_size)))

    def _blur_progress(chunk_idx: int, chunk_total: int, stage: str) -> None:
        _emit_progress(0, chunk_index=chunk_idx, chunk_total=chunk_total, stage=stage, force=False)

    cov_blur, blur_source = _chunked_gaussian_blur(
        coverage,
        sigma_px,
        use_gpu=use_gpu,
        rows_per_chunk=row_chunk_size,
        chunk_bytes=chunk_bytes_limit,
        progress_hook=_blur_progress,
        logger=logger,
    )
    if logger:
        logger.debug(
            "[TwoPass] Coverage blur applied with sigma=%d using %s (rows_per_chunk=%d, chunk_bytes=%s)",
            sigma_px,
            blur_source,
            row_chunk_size,
            chunk_bytes_limit if chunk_bytes_limit is not None else "-",
        )
    eps = np.finfo(np.float32).eps
    scale_map = cov_blur / np.maximum(coverage, eps)
    scale_map = np.clip(scale_map, 0.5, 2.0)
    if logger:
        logger.debug(
            "[TwoPass] Scale map stats: min=%.4f, max=%.4f, mean=%.4f",
            float(np.nanmin(scale_map)),
            float(np.nanmax(scale_map)),
            float(np.nanmean(scale_map)),
        )
    if not tiles or not tiles_wcs or len(tiles) != len(tiles_wcs):
        raise ValueError("Tile data and WCS lists must be aligned and non-empty")
    if tiles_total <= 0:
        return []

    if cpu_workers is None or cpu_workers <= 0:
        try:
            cpu_workers = multiprocessing.cpu_count() or os.cpu_count()
        except Exception:
            cpu_workers = os.cpu_count()
    cpu_workers = int(max(1, cpu_workers or 1))
    max_workers = max(1, min(cpu_workers, tiles_total))
    if chunk_bytes_limit and coverage.size > 0:
        est_tile_bytes = max(coverage.nbytes, coverage.size * np.dtype(np.float32).itemsize)
        est_tile_bytes = max(est_tile_bytes, coverage.size * np.dtype(np.float64).itemsize)
        est_tile_bytes *= 2
        max_by_mem = max(1, int(chunk_bytes_limit // max(1, est_tile_bytes)))
        max_workers = max(1, min(max_workers, max_by_mem))
    executor_cls = ThreadPoolExecutor if use_gpu else ProcessPoolExecutor
    progress_interval = max(1, tiles_total // max(1, max_workers * 2))

    def _execute(executor_type):
        gains_result: list[float] = [1.0] * tiles_total
        done = 0
        _emit_progress(0, chunk_index=0, chunk_total=tiles_total, stage="gains", force=True)
        future_to_idx: dict[Any, int] = {}
        with executor_type(max_workers=max_workers) as pool:
            for idx, (tile, tile_wcs) in enumerate(zip(tiles, tiles_wcs)):
                coverage_src = None
                if tiles_coverage and idx < len(tiles_coverage or []):
                    coverage_src = tiles_coverage[idx]
                fut = pool.submit(
                    _compute_tile_gain_task,
                    (
                        idx,
                        tile,
                        tile_wcs,
                        final_wcs,
                        coverage.shape,
                        coverage,
                        scale_map,
                        gain_min,
                        gain_max,
                        coverage_src,
                    ),
                )
                future_to_idx[fut] = idx
            for fut in as_completed(future_to_idx):
                idx_value = future_to_idx.get(fut, None)
                gain_value: float = 1.0
                try:
                    result_idx, gain_value = fut.result()
                    idx_value = result_idx
                except Exception as exc_fut:
                    if logger and idx_value is not None:
                        logger.warning("[TwoPass] Gain task failed for tile %d: %s", idx_value, exc_fut)
                if idx_value is not None:
                    gains_result[idx_value] = float(gain_value)
                done += 1
                if done % progress_interval == 0 or done == tiles_total:
                    _emit_progress(done, chunk_index=done, chunk_total=tiles_total, stage="gains", force=False)
        _emit_progress(tiles_total, chunk_index=tiles_total, chunk_total=tiles_total, stage="gains", force=True)
        return gains_result

    try:
        gains = _execute(executor_cls)
    except Exception as exc_pool:
        if not use_gpu and executor_cls is ProcessPoolExecutor:
            if logger:
                logger.warning("[TwoPass] Process pool failed, retrying gain computation with threads: %s", exc_pool)
            gains = _execute(ThreadPoolExecutor)
        else:
            raise
    return gains


def run_second_pass_coverage_renorm(
    tiles: list[np.ndarray],
    tiles_wcs: list[Any],
    final_wcs_p1: Any,
    coverage_p1: np.ndarray,
    shape_out: tuple[int, int],
    *,
    sigma_px: int,
    gain_clip: tuple[float, float],
    logger=None,
    use_gpu_two_pass: bool | None = None,
    tiles_coverage: list[np.ndarray | None] | None = None,
    parallel_plan: ParallelPlan | None = None,
    telemetry_ctrl: ResourceTelemetryController | None = None,
) -> tuple[np.ndarray, np.ndarray] | None:
    """Apply coverage-based gains to tiles and reproject them for a second pass."""
    if logger:
        logger.debug(
            "[TwoPass] run_second_pass_coverage_renorm start: tiles=%d, wcs=%d, coverage_shape=%s, sigma=%s, clip=%s",
            len(tiles) if tiles else 0,
            len(tiles_wcs) if tiles_wcs else 0,
            getattr(coverage_p1, "shape", None),
            sigma_px,
            gain_clip,
        )
    if not tiles or not tiles_wcs or coverage_p1 is None:
        if logger:
            logger.warning(
                "[TwoPass] Missing inputs for second pass (tiles=%s, wcs=%s, coverage=%s)",
                bool(tiles),
                bool(tiles_wcs),
                coverage_p1 is not None,
            )
        return None
    if not (REPROJECT_AVAILABLE and reproject_and_coadd and reproject_interp):
        if logger:
            logger.warning("[TwoPass] Reproject dependencies unavailable; skipping second pass")
        return None
    if not (ZEMOSAIC_UTILS_AVAILABLE and zemosaic_utils):
        if logger:
            logger.warning("[TwoPass] zemosaic_utils unavailable; skipping second pass")
        return None
    use_gpu = bool(use_gpu_two_pass)
    cpu_workers_hint: int | None = None
    plan_rows_cpu_hint: int | None = None
    plan_rows_gpu_hint: int | None = None
    plan_chunk_cpu_hint: int | None = None
    plan_chunk_gpu_hint: int | None = None
    use_memmap_flag: bool | None = None
    if parallel_plan is not None:
        def _plan_value(key: str):
            try:
                return getattr(parallel_plan, key)
            except Exception:
                if isinstance(parallel_plan, dict):
                    return parallel_plan.get(key)
            return None

        try:
            cpu_workers_hint = int(_plan_value("cpu_workers") or 0)
        except Exception:
            cpu_workers_hint = None
        if cpu_workers_hint is not None and cpu_workers_hint <= 0:
            cpu_workers_hint = None
        try:
            use_memmap_flag = bool(_plan_value("use_memmap") or False)
        except Exception:
            use_memmap_flag = None
        try:
            plan_rows_cpu_hint = int(_plan_value("rows_per_chunk") or 0) or None
        except Exception:
            plan_rows_cpu_hint = None
        if plan_rows_cpu_hint is not None and plan_rows_cpu_hint <= 0:
            plan_rows_cpu_hint = None
        try:
            plan_rows_gpu_hint = int(_plan_value("gpu_rows_per_chunk") or 0) or None
        except Exception:
            plan_rows_gpu_hint = None
        if plan_rows_gpu_hint is not None and plan_rows_gpu_hint <= 0:
            plan_rows_gpu_hint = None
        try:
            plan_chunk_cpu_hint = int(_plan_value("max_chunk_bytes") or 0) or None
        except Exception:
            plan_chunk_cpu_hint = None
        if plan_chunk_cpu_hint is not None and plan_chunk_cpu_hint <= 0:
            plan_chunk_cpu_hint = None
        try:
            plan_chunk_gpu_hint = int(_plan_value("gpu_max_chunk_bytes") or 0) or None
        except Exception:
            plan_chunk_gpu_hint = None
        if plan_chunk_gpu_hint is not None and plan_chunk_gpu_hint <= 0:
            plan_chunk_gpu_hint = None

    cov_shape = np.asarray(coverage_p1).shape if coverage_p1 is not None else tuple()
    cov_h = int(cov_shape[0]) if len(cov_shape) >= 1 and cov_shape[0] else (int(shape_out[0]) if shape_out else 0)
    cov_w = int(cov_shape[1]) if len(cov_shape) >= 2 and cov_shape[1] else (int(shape_out[1]) if shape_out else 0)
    bytes_per_sample = np.dtype(np.float32).itemsize
    if plan_rows_cpu_hint is None:
        plan_rows_cpu_hint = _rows_from_chunk_bytes(cov_w, bytes_per_sample, plan_chunk_cpu_hint, cov_h)
    if plan_rows_gpu_hint is None:
        plan_rows_gpu_hint = _rows_from_chunk_bytes(cov_w, bytes_per_sample, plan_chunk_gpu_hint, cov_h)
    if plan_rows_cpu_hint is None and cov_h > 0:
        plan_rows_cpu_hint = min(cov_h, 1024)
    if plan_rows_gpu_hint is None and cov_h > 0:
        plan_rows_gpu_hint = min(cov_h, 512 if use_gpu else cov_h)
    if plan_rows_cpu_hint is None:
        plan_rows_cpu_hint = 1024
    if plan_rows_gpu_hint is None:
        plan_rows_gpu_hint = 512 if use_gpu else 1024
    effective_row_chunk = plan_rows_gpu_hint if use_gpu else plan_rows_cpu_hint
    chunk_bytes_effective = plan_chunk_gpu_hint if use_gpu else plan_chunk_cpu_hint
    tiles_total = len(tiles) if tiles else 0

    def _emit_two_pass_stats(
        tiles_done: int,
        *,
        chunk_index: int | None = None,
        chunk_total: int | None = None,
        force: bool = False,
        stage: str | None = None,
    ) -> None:
        if telemetry_ctrl is None:
            return
        try:
            ctx = {
                "phase_index": 5,
                "phase_name": "Phase 5: Two-Pass Coverage Renorm",
                "tiles_done": int(max(0, tiles_done)),
                "tiles_total": int(max(0, tiles_total)),
                "cpu_workers": cpu_workers_hint or 0,
                "use_gpu": bool(use_gpu),
                "use_gpu_phase5": bool(use_gpu_two_pass),
            }
            if stage:
                ctx["stage"] = stage
            if plan_rows_cpu_hint is not None:
                ctx["rows_per_chunk"] = int(plan_rows_cpu_hint)
            if plan_rows_gpu_hint is not None:
                ctx["gpu_rows_per_chunk"] = int(plan_rows_gpu_hint)
            if plan_chunk_cpu_hint is not None:
                ctx["max_chunk_bytes"] = int(plan_chunk_cpu_hint)
            if plan_chunk_gpu_hint is not None:
                ctx["gpu_max_chunk_bytes"] = int(plan_chunk_gpu_hint)
            _ = telemetry_ctrl.emit_stats(ctx, force=force)
        except Exception:
            pass

    def _gains_progress(
        tiles_done: int,
        *,
        chunk_index: int | None = None,
        chunk_total: int | None = None,
        stage: str | None = None,
        force: bool = False,
    ) -> None:
        _emit_two_pass_stats(
            tiles_done,
            chunk_index=chunk_index,
            chunk_total=chunk_total,
            force=force,
            stage=stage or "gains",
        )

    _emit_two_pass_stats(0, chunk_index=0, chunk_total=max(1, tiles_total), force=True, stage="start")
    effective_row_chunk = int(max(1, effective_row_chunk or 1))
    try:
        gains = compute_per_tile_gains_from_coverage(
            tiles,
            tiles_wcs,
            final_wcs_p1,
            coverage_p1,
            sigma_px=sigma_px,
            gain_clip=gain_clip,
            logger=logger,
            use_gpu=use_gpu,
            tiles_coverage=tiles_coverage,
            parallel_plan=parallel_plan,
            cpu_workers=cpu_workers_hint,
            row_chunk_size=effective_row_chunk,
            chunk_bytes_limit=chunk_bytes_effective,
            progress_emitter=_gains_progress,
        )
    except Exception as exc:
        if logger:
            logger.warning("[TwoPass] Gain computation failed: %s", exc, exc_info=True)
        return None
    if logger:
        finite_gains = [g for g in gains if np.isfinite(g)]
        logger.debug(
            "[TwoPass] Computed gains count=%d, finite=%d, min=%.4f, max=%.4f",
            len(gains),
            len(finite_gains),
            float(np.min(finite_gains)) if finite_gains else float("nan"),
            float(np.max(finite_gains)) if finite_gains else float("nan"),
        )
    _emit_two_pass_stats(
        tiles_total,
        chunk_index=tiles_total,
        chunk_total=max(1, tiles_total),
        force=False,
        stage="gains_done",
    )
    corrected_tiles: list[np.ndarray] = []
    for arr, gain in zip(tiles, gains):
        tile_arr = np.asarray(arr, dtype=np.float32)
        if tile_arr.ndim == 2:
            tile_arr = tile_arr[..., np.newaxis]
        corrected_tiles.append(tile_arr * float(gain))
    try:
        output_projection = (
            final_wcs_p1.to_header(relax=True)
            if hasattr(final_wcs_p1, "to_header")
            else final_wcs_p1
        )
    except Exception:
        output_projection = final_wcs_p1

    reproj_kwargs: dict[str, Any] = {
        "output_projection": output_projection,
        "reproject_function": reproject_interp,
        "combine_function": "mean",
    }
    memmap_pref = bool(use_memmap_flag) if use_memmap_flag is not None else False
    try:
        sig = inspect.signature(reproject_and_coadd)
    except Exception:
        sig = None
    if sig:
        if "match_background" in sig.parameters:
            reproj_kwargs["match_background"] = True
        elif "match_bg" in sig.parameters:
            reproj_kwargs["match_bg"] = True
        if "process_workers" in sig.parameters and cpu_workers_hint:
            reproj_kwargs["process_workers"] = int(cpu_workers_hint)
        if "use_memmap" in sig.parameters:
            reproj_kwargs["use_memmap"] = memmap_pref
        elif "intermediate_memmap" in sig.parameters:
            reproj_kwargs["intermediate_memmap"] = memmap_pref
    else:
        reproj_kwargs["match_background"] = True
        if cpu_workers_hint:
            reproj_kwargs["process_workers"] = int(cpu_workers_hint)

    row_hint = plan_rows_gpu_hint if use_gpu else plan_rows_cpu_hint
    chunk_hint = chunk_bytes_effective
    if chunk_hint is None and use_gpu and plan_chunk_cpu_hint:
        chunk_hint = plan_chunk_cpu_hint
    if row_hint:
        reproj_kwargs["rows_per_chunk"] = int(max(1, row_hint))
    if chunk_hint:
        reproj_kwargs["max_chunk_bytes"] = int(max(1, chunk_hint))
    if logger:
        try:
            logger.info(
                "[TwoPass] Reprojection plan: gpu=%s cpu_workers=%s rows(cpu/gpu)=%s/%s chunk_mb(cpu/gpu)=%.2f/%.2f memmap=%s",
                use_gpu,
                cpu_workers_hint or "-",
                plan_rows_cpu_hint or 0,
                plan_rows_gpu_hint or 0,
                (plan_chunk_cpu_hint or 0) / (1024 ** 2),
                (plan_chunk_gpu_hint or 0) / (1024 ** 2),
                bool(use_memmap_flag),
            )
        except Exception:
            pass

    n_channels = corrected_tiles[0].shape[-1] if corrected_tiles[0].ndim == 3 else 1
    mosaic_channels: list[np.ndarray | None] = [None] * n_channels
    coverage_channels: list[np.ndarray | None] = [None] * n_channels
    shape_out_hw = tuple(map(int, shape_out))

    def _process_channel(ch_idx: int, use_gpu_flag: bool) -> tuple[int, np.ndarray, np.ndarray]:
        if logger:
            logger.debug(
                "[TwoPass] Reproject channel %d/%d with %d tiles (shape_out=%s, gpu=%s)",
                ch_idx + 1,
                n_channels,
                len(corrected_tiles),
                shape_out_hw,
                use_gpu_flag,
            )
        data_list = [tile[..., ch_idx] if tile.ndim == 3 else tile[..., 0] for tile in corrected_tiles]

        def _invoke_reproj(use_gpu_local: bool, local_kwargs: dict[str, Any]):
            return zemosaic_utils.reproject_and_coadd_wrapper(
                data_list=data_list,
                wcs_list=tiles_wcs,
                shape_out=shape_out_hw,
                use_gpu=use_gpu_local,
                cpu_func=reproject_and_coadd,
                **local_kwargs,
            )

        local_kwargs = dict(reproj_kwargs)
        try:
            chan_mosaic, chan_cov = _invoke_reproj(use_gpu_flag, local_kwargs)
        except wcs_module.NoConvergence as conv_exc:
            if logger:
                logger.warning(
                    "[TwoPass] WCS convergence failed on channel %d, creating empty patch: %s",
                    ch_idx,
                    conv_exc,
                    exc_info=False,
                )
            chan_mosaic = np.full(shape_out_hw, np.nan, dtype=np.float32)
            chan_cov = np.zeros(shape_out_hw, dtype=np.float32)
        except TypeError as type_err:
            if logger:
                logger.warning("[TwoPass] GPU reprojection TypeError, attempting recovery: %s", type_err)
            chan_mosaic = chan_cov = None
            if use_gpu_flag:
                retry_kwargs = dict(local_kwargs)
                removed = []
                err_msg = str(type_err)
                for key in list(retry_kwargs.keys()):
                    if f"'{key}'" in err_msg:
                        removed.append(key)
                        retry_kwargs.pop(key, None)
                if removed:
                    try:
                        chan_mosaic, chan_cov = _invoke_reproj(True, retry_kwargs)
                    except Exception as retry_exc:
                        if logger:
                            logger.warning(
                                "[TwoPass] GPU retry failed, switching to CPU: %s (removed=%s)",
                                retry_exc,
                                ",".join(removed),
                            )
            if chan_mosaic is None or chan_cov is None:
                try:
                    chan_mosaic, chan_cov = _invoke_reproj(False, local_kwargs)
                except wcs_module.NoConvergence as conv_exc_cpu:
                    if logger:
                        logger.warning(
                            "[TwoPass] WCS convergence failed on channel %d (CPU fallback), creating empty patch: %s",
                            ch_idx,
                            conv_exc_cpu,
                            exc_info=False,
                        )
                    chan_mosaic = np.full(shape_out_hw, np.nan, dtype=np.float32)
                    chan_cov = np.zeros(shape_out_hw, dtype=np.float32)
                except Exception as cpu_exc:
                    if logger:
                        logger.warning(
                            "[TwoPass] CPU fallback after GPU TypeError failed on channel %d: %s",
                            ch_idx,
                            cpu_exc,
                            exc_info=True,
                        )
                    raise
        except Exception as exc:
            if use_gpu_flag:
                try:
                    chan_mosaic, chan_cov = _invoke_reproj(False, local_kwargs)
                except wcs_module.NoConvergence as conv_exc_cpu2:
                    if logger:
                        logger.warning(
                            "[TwoPass] WCS convergence failed on channel %d (CPU fallback), creating empty patch: %s",
                            ch_idx,
                            conv_exc_cpu2,
                            exc_info=False,
                        )
                    chan_mosaic = np.full(shape_out_hw, np.nan, dtype=np.float32)
                    chan_cov = np.zeros(shape_out_hw, dtype=np.float32)
                except Exception as cpu_exc:
                    if logger:
                        logger.warning(
                            "[TwoPass] Reprojection failed on channel %d (GPU+CPU): %s / %s",
                            ch_idx,
                            exc,
                            cpu_exc,
                            exc_info=True,
                        )
                    raise
            else:
                if logger:
                    logger.warning(
                        "[TwoPass] Reprojection failed on channel %d: %s",
                        ch_idx,
                        exc,
                        exc_info=True,
                    )
                raise
        chan_mosaic_np = np.asarray(chan_mosaic, dtype=np.float32)
        chan_cov_np = np.asarray(chan_cov, dtype=np.float32)
        if logger:
            logger.debug(
                "[TwoPass] Channel %d done: mosaic_shape=%s, coverage_shape=%s, cov_stats=(min=%.4f, max=%.4f)",
                ch_idx + 1,
                getattr(chan_mosaic_np, "shape", None),
                getattr(chan_cov_np, "shape", None),
                float(np.nanmin(chan_cov_np)) if chan_cov_np is not None else float("nan"),
                float(np.nanmax(chan_cov_np)) if chan_cov_np is not None else float("nan"),
            )
        return ch_idx, chan_mosaic_np, chan_cov_np

    reproj_chunk_total = n_channels
    if n_channels == 1:
        try:
            ch_idx, chan_mosaic_np, chan_cov_np = _process_channel(0, use_gpu)
        except Exception:
            return None
        mosaic_channels[0] = chan_mosaic_np
        coverage_channels[0] = chan_cov_np
        _emit_two_pass_stats(
            tiles_total,
            chunk_index=1,
            chunk_total=reproj_chunk_total,
            force=False,
            stage="reproject",
        )
    else:
        channel_tasks: list[tuple[int, bool]] = []
        gpu_assigned = False
        for ch in range(n_channels):
            use_gpu_flag = bool(use_gpu and not gpu_assigned)
            if use_gpu_flag:
                gpu_assigned = True
            channel_tasks.append((ch, use_gpu_flag))
        max_channel_workers = max(1, min(n_channels, cpu_workers_hint or n_channels))
        reproj_failure = False
        done_channels = 0
        with ThreadPoolExecutor(max_workers=max_channel_workers) as executor:
            future_map = {
                executor.submit(_process_channel, ch_idx, use_gpu_flag): ch_idx for ch_idx, use_gpu_flag in channel_tasks
            }
            for fut in as_completed(future_map):
                try:
                    ch_idx, chan_mosaic_np, chan_cov_np = fut.result()
                    mosaic_channels[ch_idx] = chan_mosaic_np
                    coverage_channels[ch_idx] = chan_cov_np
                except Exception as exc:
                    reproj_failure = True
                    if logger:
                        logger.warning("[TwoPass] Channel %d reprojection failed: %s", future_map.get(fut, -1), exc)
                    break
                done_channels += 1
                _emit_two_pass_stats(
                    tiles_total,
                    chunk_index=done_channels,
                    chunk_total=reproj_chunk_total,
                    force=False,
                    stage="reproject",
                )
        if reproj_failure or any(m is None for m in mosaic_channels):
            return None

    coverage_result: np.ndarray | None = None
    for cov in coverage_channels:
        if cov is not None:
            coverage_result = cov.astype(np.float32, copy=False)
            break
    if coverage_result is None:
        return None
    stacked_mosaic = [np.asarray(mc, dtype=np.float32) for mc in mosaic_channels if mc is not None]
    mosaic = (
        stacked_mosaic[0][..., np.newaxis]
        if n_channels == 1
        else np.stack(stacked_mosaic, axis=-1)
    )
    _emit_two_pass_stats(
        tiles_total,
        chunk_index=reproj_chunk_total,
        chunk_total=reproj_chunk_total,
        force=True,
        stage="reproject_done",
    )
    return mosaic.astype(np.float32), coverage_result.astype(np.float32)


# Backwards compatibility alias expected by tests
assemble_final_mosaic_with_reproject_coadd = assemble_final_mosaic_reproject_coadd


def prepare_tiles_and_calc_grid(
    tiles_with_wcs: list,
    crop_percent: float = 0.0,
    re_solve_cropped_tiles: bool = False,
    solver_settings: dict | None = None,
    solver_instance=None,
    drizzle_scale_factor: float = 1.0,
    progress_callback: Callable | None = None,
):
    wcs_list = []
    shape_list = []
    for path, w in tiles_with_wcs:
        current_wcs = w
        if re_solve_cropped_tiles and solver_instance is not None:
            try:
                solved = solver_instance.solve(path, w.to_header(), solver_settings or {}, update_header_with_solution=True)
                if solved:
                    current_wcs = solved
            except Exception:
                pass
        wcs_list.append(current_wcs)
        if hasattr(current_wcs, "pixel_shape"):
            shape_list.append((current_wcs.pixel_shape[1], current_wcs.pixel_shape[0]))
        else:
            shape_list.append((0, 0))
    return _calculate_final_mosaic_grid(wcs_list, shape_list, drizzle_scale_factor, progress_callback)




def run_hierarchical_mosaic_classic_legacy(
    input_folder: str,
    output_folder: str,
    astap_exe_path: str,
    astap_data_dir_param: str,
    astap_search_radius_config: float,
    astap_downsample_config: int,
    astap_sensitivity_config: int,
    cluster_threshold_config: float,
    cluster_target_groups_config: int,
    cluster_orientation_split_deg_config: float,
    progress_callback: callable,
    stack_ram_budget_gb_config: float,
    stack_norm_method: str,
    stack_weight_method: str,
    stack_reject_algo: str,
    stack_kappa_low: float,
    stack_kappa_high: float,
    parsed_winsor_limits: tuple[float, float],
    stack_final_combine: str,
    poststack_equalize_rgb_config: bool,
    apply_radial_weight_config: bool,
    radial_feather_fraction_config: float,
    radial_shape_power_config: float,
    min_radial_weight_floor_config: float,
    final_assembly_method_config: str,
    inter_master_merge_enable_config: bool,
    inter_master_overlap_threshold_config: float,
    inter_master_min_group_size_config: int,
    inter_master_stack_method_config: str,
    inter_master_memmap_policy_config: str,
    inter_master_local_scale_config: str,
    inter_master_max_group_config: int,
    num_base_workers_config: int,
    # --- ARGUMENTS POUR LE ROGNAGE ---
    apply_master_tile_crop_config: bool,
    master_tile_crop_percent_config: float,
    quality_crop_enabled_config: bool,
    quality_crop_band_px_config: int,
    quality_crop_k_sigma_config: float,
    quality_crop_margin_px_config: int,
    quality_crop_min_run_config: int,
    altaz_cleanup_enabled_config: bool,
    altaz_margin_percent_config: float,
    altaz_decay_config: float,
    altaz_nanize_config: bool,
    quality_gate_enabled_config: bool,
    quality_gate_threshold_config: float,
    quality_gate_edge_band_px_config: int,
    quality_gate_k_sigma_config: float,
    quality_gate_erode_px_config: int,
    quality_gate_move_rejects_config: bool,
    save_final_as_uint16_config: bool,
    legacy_rgb_cube_config: bool,

    coadd_use_memmap_config: bool,
    coadd_memmap_dir_config: str,
    coadd_cleanup_memmap_config: bool,
    assembly_process_workers_config: int,
    auto_limit_frames_per_master_tile_config: bool,
    winsor_max_frames_per_pass_config: int,
    winsor_worker_limit_config: int,
    max_raw_per_master_tile_config: int,
    intertile_photometric_match_config: bool = True,
    intertile_preview_size_config: int = 512,
    intertile_overlap_min_config: float = 0.05,
    intertile_sky_percentile_config: tuple[float, float] | list[float] = (30.0, 70.0),
    intertile_robust_clip_sigma_config: float = 2.5,
    intertile_global_recenter_config: bool = True,
    intertile_recenter_clip_config: tuple[float, float] | list[float] = (0.85, 1.18),
    use_auto_intertile_config: bool = False,
    match_background_for_final_config: bool = True,
    incremental_feather_parity_config: bool = False,
    two_pass_coverage_renorm_config: bool = False,
    two_pass_cov_sigma_px_config: int = 50,
    two_pass_cov_gain_clip_config: tuple[float, float] | list[float] = (0.85, 1.18),
    center_out_normalization_p3_config: bool = True,
    p3_center_sky_percentile_config: tuple[float, float] | list[float] = (25.0, 60.0),
    p3_center_robust_clip_sigma_config: float = 2.5,
    p3_center_preview_size_config: int = 256,
    p3_center_min_overlap_fraction_config: float = 0.03,
    center_out_anchor_mode_config: str = "auto_central_quality",
    anchor_quality_probe_limit_config: int = 12,
    anchor_quality_span_range_config: tuple[float, float] | list[float] = (0.02, 6.0),
    anchor_quality_median_clip_sigma_config: float = 2.5,
    enable_poststack_anchor_review_config: bool = True,
    poststack_anchor_probe_limit_config: int = 8,
    poststack_anchor_span_range_config: tuple[float, float] | list[float] = (0.004, 10.0),
    poststack_anchor_median_clip_sigma_config: float = 3.5,
    poststack_anchor_min_improvement_config: float = 0.12,
    poststack_anchor_use_overlap_affine_config: bool = True,
    use_gpu_phase5: bool = False,
    gpu_id_phase5: int | None = None,
    enable_tile_weighting_config: bool | None = None,
    tile_weight_mode_config: str = "n_frames",
    logging_level_config: str = "INFO",
    solver_settings: dict | None = None,
    skip_filter_ui: bool = False,
    # New optional integration points when filter ran in GUI
    filter_invoked: bool = False,
    filter_overrides: dict | None = None,
    filtered_header_items: list[dict] | None = None,
    early_filter_enabled: bool | None = None,
    final_mosaic_rgb_equalize_enabled_config: bool | None = None,
    final_mosaic_black_point_equalize_enabled: bool | None = None,
    final_mosaic_black_point_percentile: float | None = None,
):
    """
    Legacy classic (non-grid, non-SDS) hierarchical mosaic pipeline.

    Direct port of the classic pipeline from `zemosaic_worker_non grid_ok.py`,
    including two-pass coverage renorm.
    """
    worker_config_cache: dict[str, Any] = {}
    if ZEMOSAIC_CONFIG_AVAILABLE and zemosaic_config:
        try:
            worker_config_cache = zemosaic_config.load_config() or {}
        except Exception:
            worker_config_cache = {}

    try:
        zconfig = SimpleNamespace(**worker_config_cache)
    except Exception:
        zconfig = SimpleNamespace()

    final_mosaic_black_point_equalize_enabled = _coerce_bool_flag(
        final_mosaic_black_point_equalize_enabled
    )
    if final_mosaic_black_point_equalize_enabled is None:
        final_mosaic_black_point_equalize_enabled = _coerce_bool_flag(
            getattr(zconfig, "final_mosaic_black_point_equalize_enabled", None)
        )
    if final_mosaic_black_point_equalize_enabled is None:
        final_mosaic_black_point_equalize_enabled = True
    final_mosaic_black_point_equalize_enabled = bool(
        final_mosaic_black_point_equalize_enabled
    )

    try:
        final_mosaic_black_point_percentile = float(final_mosaic_black_point_percentile)
    except (TypeError, ValueError):
        final_mosaic_black_point_percentile = getattr(
            zconfig, "final_mosaic_black_point_percentile", 0.1
        )

    if not (
        isinstance(final_mosaic_black_point_percentile, (int, float))
        and math.isfinite(final_mosaic_black_point_percentile)
    ):
        final_mosaic_black_point_percentile = 0.1
    final_mosaic_black_point_percentile = float(
        max(0.0, min(100.0, final_mosaic_black_point_percentile))
    )

    def pcb(msg_key, prog=None, lvl="INFO", **kwargs):
        """Shortcut to emit log+callback events with the current progress callback."""
        _log_and_callback(msg_key, prog, lvl, callback=progress_callback, **kwargs)

    global_wcs_plan = _prepare_global_wcs_plan(
        output_folder,
        worker_config_cache,
        filter_overrides,
    )

    config_sds_default = _coerce_bool_flag(worker_config_cache.get("sds_mode_default"))
    override_flag = None
    override_defined = False
    plan_override_flag = None
    plan_override_defined = False
    plan_override = None
    if isinstance(filter_overrides, dict):
        if "sds_mode" in filter_overrides:
            override_flag = _coerce_bool_flag(filter_overrides.get("sds_mode"))
            override_defined = override_flag is not None
        plan_override = filter_overrides.get("global_wcs_plan_override")
    if isinstance(plan_override, dict) and "sds_mode" in plan_override:
        plan_override_flag = _coerce_bool_flag(plan_override.get("sds_mode"))
        plan_override_defined = plan_override_flag is not None
    if override_defined:
        sds_mode_flag = override_flag
    elif plan_override_defined:
        sds_mode_flag = plan_override_flag
    elif config_sds_default is not None:
        sds_mode_flag = config_sds_default
    else:
        sds_mode_flag = False
    global_wcs_plan["sds_mode"] = bool(sds_mode_flag)

    try:
        sds_coverage_threshold_config = float(worker_config_cache.get("sds_coverage_threshold", 0.92))
    except Exception:
        sds_coverage_threshold_config = 0.92
    if not math.isfinite(sds_coverage_threshold_config):
        sds_coverage_threshold_config = 0.92
    sds_coverage_threshold_config = max(0.10, min(0.99, sds_coverage_threshold_config))
    try:
        sds_min_batch_size_config = int(worker_config_cache.get("sds_min_batch_size", 5) or 5)
    except Exception:
        sds_min_batch_size_config = 5
    if sds_min_batch_size_config <= 0:
        sds_min_batch_size_config = 1
    try:
        sds_target_batch_size_config = int(worker_config_cache.get("sds_target_batch_size", 10) or 10)
    except Exception:
        sds_target_batch_size_config = 10
    if sds_target_batch_size_config < sds_min_batch_size_config:
        sds_target_batch_size_config = sds_min_batch_size_config
    try:
        sds_min_coverage_keep_config = float(worker_config_cache.get("sds_min_coverage_keep", 0.4))
    except Exception:
        sds_min_coverage_keep_config = 0.4
    if not math.isfinite(sds_min_coverage_keep_config):
        sds_min_coverage_keep_config = 0.4
    sds_min_coverage_keep_config = max(0.0, min(1.0, sds_min_coverage_keep_config))

    global_wcs_autocrop_enabled_config = bool(worker_config_cache.get("global_wcs_autocrop_enabled"))
    try:
        global_wcs_autocrop_margin_px_config = int(worker_config_cache.get("global_wcs_autocrop_margin_px", 64) or 0)
    except Exception:
        global_wcs_autocrop_margin_px_config = 0
    if global_wcs_autocrop_margin_px_config < 0:
        global_wcs_autocrop_margin_px_config = 0

    if global_wcs_plan.get("enabled"):
        logger.info(
            "[Worker] Global WCS descriptor ready (%s)",
            global_wcs_plan.get("fits_path"),
        )
    else:
        requested_mode = ""
        if isinstance(filter_overrides, dict):
            requested_mode = str(filter_overrides.get("mode") or "").strip().lower()
        if requested_mode == "seestar":
            pcb(
                "global_coadd_error_descriptor_missing",
                prog=None,
                lvl="WARN",
                path=str(filter_overrides.get("global_wcs_path") or ""),
            )

    # Cache retention policy (Phase 1 preprocessed cache cleanup)
    cache_retention_mode = "run_end"
    allowed_cache_modes = {"run_end", "per_tile", "keep"}
    if worker_config_cache:
        try:
            cache_retention_mode = str(worker_config_cache.get("cache_retention", "run_end")).strip().lower()
        except Exception:
            cache_retention_mode = "run_end"
    if cache_retention_mode not in allowed_cache_modes:
        cache_retention_mode = "run_end"
    logger.info("Cache retention mode: %s", cache_retention_mode)
    try:
        pcb("run_info_cache_retention_mode", prog=None, lvl="INFO_DETAIL", mode=cache_retention_mode)
    except Exception:
        pass

    cleanup_temp_artifacts_value = (worker_config_cache or {}).get("cleanup_temp_artifacts")
    cleanup_temp_artifacts_config = _coerce_bool_flag(cleanup_temp_artifacts_value)
    if cleanup_temp_artifacts_config is None:
        cleanup_temp_artifacts_config = True

    _configure_worker_logging(logging_level_config, source_hint="qt_gui_config")

    # --- Harmoniser les méthodes de pondération issues du GUI / CLI / fallback config ---
    requested_stack_weight_method = stack_weight_method
    stack_weight_method_normalized = str(stack_weight_method or "").lower().strip()
    if not stack_weight_method_normalized:
        stack_weight_method_normalized = ""
        if ZEMOSAIC_CONFIG_AVAILABLE and zemosaic_config:
            try:
                cfg_weight = zemosaic_config.load_config() or {}
                stack_weight_method_normalized = str(
                    cfg_weight.get("stacking_weighting_method", "")
                ).lower().strip()
            except Exception:
                stack_weight_method_normalized = ""
    if not stack_weight_method_normalized:
        stack_weight_method_normalized = "none"
    if stack_weight_method_normalized not in {"none", "noise_variance", "noise_fwhm"}:
        stack_weight_method_normalized = "none"
    if str(requested_stack_weight_method or "").lower().strip() != stack_weight_method_normalized:
        _log_and_callback(
            f"[Worker] stack_weight_method fallback -> '{stack_weight_method_normalized}'",
            lvl="INFO",
            callback=progress_callback,
        )
    stack_weight_method = stack_weight_method_normalized

    # Reset alignment warning counters at start of run
    for k in ALIGN_WARNING_COUNTS:
        ALIGN_WARNING_COUNTS[k] = 0
    
    def update_gui_eta(eta_seconds_total):
        if progress_callback and callable(progress_callback):
            eta_str = "--:--:--"
            if eta_seconds_total is not None and eta_seconds_total >= 0:
                h, rem = divmod(int(eta_seconds_total), 3600); m, s = divmod(rem, 60)
                eta_str = f"{h:02d}:{m:02d}:{s:02d}"
            pcb(f"ETA_UPDATE:{eta_str}", prog=None, lvl="ETA_LEVEL")


    try:
        telemetry_enabled = bool(getattr(zconfig, "enable_resource_telemetry", False))
    except Exception:
        telemetry_enabled = False
    try:
        telemetry_interval = float(getattr(zconfig, "resource_telemetry_interval_sec", 1.5) or 1.5)
    except Exception:
        telemetry_interval = 1.5
    try:
        telemetry_log_to_csv = bool(getattr(zconfig, "resource_telemetry_log_to_csv", True))
    except Exception:
        telemetry_log_to_csv = True
    telemetry_csv_path = None
    if telemetry_log_to_csv and output_folder:
        try:
            telemetry_csv_path = os.path.join(output_folder, "resource_telemetry.csv")
        except Exception:
            telemetry_csv_path = None
    telemetry = ResourceTelemetryController(
        enabled=telemetry_enabled,
        interval_sec=telemetry_interval,
        callback=progress_callback,
        csv_path=telemetry_csv_path,
        log_and_callback=_log_and_callback,
    )

    def _telemetry_context(extra: dict | None = None) -> dict:
        ctx: dict[str, Any] = {}
        plan_candidate = None
        try:
            plan_candidate = getattr(zconfig, "parallel_plan", None)
        except Exception:
            plan_candidate = None
        if plan_candidate is None:
            try:
                plan_candidate = worker_config_cache.get("parallel_plan")
            except Exception:
                plan_candidate = None
        if plan_candidate is not None:
            for attr in (
                "cpu_workers",
                "rows_per_chunk",
                "gpu_rows_per_chunk",
                "max_chunk_bytes",
                "gpu_max_chunk_bytes",
                "use_gpu",
                "use_gpu_phase5",
            ):
                value = None
                if hasattr(plan_candidate, attr):
                    try:
                        value = getattr(plan_candidate, attr)
                    except Exception:
                        value = None
                elif isinstance(plan_candidate, dict):
                    value = plan_candidate.get(attr)
                if value is not None:
                    ctx[attr] = value
        try:
            ctx.setdefault("use_gpu_phase5", bool(use_gpu_phase5))
        except Exception:
            pass
        if extra:
            ctx.update(extra)
        return ctx

    telemetry.maybe_emit_stats(_telemetry_context({"phase_name": "Init", "phase_index": 0}))


    resource_probe_info = _probe_system_resources(
        output_folder,
        two_pass_enabled=two_pass_coverage_renorm_config,
        two_pass_sigma_px=two_pass_cov_sigma_px_config,
        two_pass_gain_clip=two_pass_cov_gain_clip_config,
    )
    _emit_gpu_info_summary(progress_callback, resource_probe_info)
    two_pass_enabled = bool(resource_probe_info.get("two_pass_enabled", False))
    try:
        two_pass_sigma_px = int(resource_probe_info.get("two_pass_sigma_px", 50) or 50)
    except (TypeError, ValueError):
        two_pass_sigma_px = 50
    gain_clip_raw = resource_probe_info.get("two_pass_gain_clip")
    gain_clip_tuple: tuple[float, float]
    if isinstance(gain_clip_raw, (list, tuple)) and len(gain_clip_raw) >= 2:
        try:
            low = float(gain_clip_raw[0])
            high = float(gain_clip_raw[1])
            if low > high:
                low, high = high, low
            gain_clip_tuple = (low, high)
        except (TypeError, ValueError):
            gain_clip_tuple = (0.85, 1.18)
    else:
        gain_clip_tuple = (0.85, 1.18)
    try:
        if (
            isinstance(intertile_recenter_clip_config, (list, tuple))
            and len(intertile_recenter_clip_config) >= 2
        ):
            clip_low = float(intertile_recenter_clip_config[0])
            clip_high = float(intertile_recenter_clip_config[1])
            if clip_low > clip_high:
                clip_low, clip_high = clip_high, clip_low
            intertile_recenter_clip_tuple = (clip_low, clip_high)
        else:
            intertile_recenter_clip_tuple = (0.85, 1.18)
    except Exception:
        intertile_recenter_clip_tuple = (0.85, 1.18)
    auto_caps_info: dict | None = None
    auto_resource_strategy: dict = {}
    phase0_header_items: list[dict] = []
    phase0_lookup: dict[str, dict] = {}
    preplan_groups_override_paths: list[list[str]] | None = None
    intertile_match_flag = bool(intertile_photometric_match_config)
    match_background_flag = (
        True
        if match_background_for_final_config is None
        else bool(match_background_for_final_config)
    )
    feather_parity_flag = bool(incremental_feather_parity_config)
    tile_weighting_enabled_config = _coerce_bool_flag(enable_tile_weighting_config)
    if tile_weighting_enabled_config is None:
        tile_weighting_enabled_config = _coerce_bool_flag(worker_config_cache.get("enable_tile_weighting"))
    if tile_weighting_enabled_config is None:
        tile_weighting_enabled_config = True
    weight_mode_value = tile_weight_mode_config or worker_config_cache.get("tile_weight_mode") or "n_frames"
    tile_weight_mode_config = str(weight_mode_value).strip().lower() or "n_frames"
    if sds_mode_flag:
        tile_weighting_enabled_config = False

    try:
        if isinstance(intertile_sky_percentile_config, (list, tuple)) and len(intertile_sky_percentile_config) >= 2:
            intertile_sky_percentile_tuple = (
                float(intertile_sky_percentile_config[0]),
                float(intertile_sky_percentile_config[1]),
            )
        else:
            intertile_sky_percentile_tuple = (30.0, 70.0)
    except Exception:
        intertile_sky_percentile_tuple = (30.0, 70.0)

    def _normalize_path_for_matching(path_value: str | None) -> str | None:
        if not path_value:
            return None
        normalized = _normcase_path(Path(path_value).expanduser())
        return normalized or None


    # Seuil de clustering : valeur de repli à 0.05° si l'option est absente ou non positive
    try:
        cluster_threshold = float(cluster_threshold_config or 0)
    except (TypeError, ValueError):
        cluster_threshold = 0
    SEESTAR_STACK_CLUSTERING_THRESHOLD_DEG = (
        cluster_threshold if cluster_threshold > 0 else 0.05

    )
    # Orientation split threshold (degrees). 0 disables orientation filtering
    try:
        orientation_split_thr = float(cluster_orientation_split_deg_config or 0)
    except (TypeError, ValueError):
        orientation_split_thr = 0.0
    ORIENTATION_SPLIT_THRESHOLD_DEG = orientation_split_thr if orientation_split_thr > 0 else 0.0
    try:
        stack_ram_budget_gb = float(stack_ram_budget_gb_config or 0.0)
    except (TypeError, ValueError):
        stack_ram_budget_gb = 0.0
    STACK_RAM_BUDGET_BYTES = int(stack_ram_budget_gb * (1024 ** 3)) if stack_ram_budget_gb > 0 else 0
    PROGRESS_WEIGHT_PHASE1_RAW_SCAN = 30; PROGRESS_WEIGHT_PHASE2_CLUSTERING = 5
    PROGRESS_WEIGHT_PHASE3_MASTER_TILES = 35; PROGRESS_WEIGHT_PHASE4_GRID_CALC = 5
    PROGRESS_WEIGHT_PHASE4_5_INTER_MASTER = 6
    PROGRESS_WEIGHT_PHASE5_ASSEMBLY = 9; PROGRESS_WEIGHT_PHASE6_SAVE = 8
    PROGRESS_WEIGHT_PHASE7_CLEANUP = 2

    DEFAULT_PHASE_WORKER_RATIO = 1.0
    ALIGNMENT_PHASE_WORKER_RATIO = 1.0  # Phase 3 targets ~90% of logical cores while keeping one free

    if use_gpu_phase5 and gpu_id_phase5 is not None and CUPY_AVAILABLE:
        os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
        os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id_phase5)
        try:
            import cupy
            cupy.cuda.Device(0).use()
        except Exception as e:
            pcb(
                "run_error_gpu_init_failed",
                prog=None,
                lvl="ERROR",
                error=str(e),
            )
            use_gpu_phase5 = False
    else:
        for v in ("CUDA_VISIBLE_DEVICES", "CUDA_DEVICE_ORDER"):
            os.environ.pop(v, None)

    # Determine final GPU usage flag only if a valid NVIDIA GPU is selected
    use_gpu_phase5_flag = (
        use_gpu_phase5
        and gpu_id_phase5 is not None
        and CUPY_AVAILABLE
        and gpu_is_available()
    )
    if use_gpu_phase5_flag and ZEMOSAIC_UTILS_AVAILABLE and zemosaic_utils:
        try:
            # Initialize CuPy memory pools on the selected device (index 0 under the mask)
            if hasattr(zemosaic_utils, 'ensure_cupy_pool_initialized'):
                zemosaic_utils.ensure_cupy_pool_initialized(0)
        except Exception:
            pass
    if use_gpu_phase5_flag:
        _log_and_callback("phase5_using_gpu", callback=progress_callback, lvl="INFO")
    else:
        _log_and_callback("phase5_using_cpu", callback=progress_callback, lvl="INFO")
    def _cleanup_per_tile_cache(cache_paths: Iterable[str]) -> tuple[int, int]:
        """Remove preprocessed cache files for a completed master tile."""

        removed_count = 0
        removed_bytes = 0
        seen_paths: set[str] = set()

        for path in cache_paths or ():
            if path is None:
                continue
            try:
                path_obj = Path(os.fspath(path)).expanduser()
                try:
                    resolved = path_obj.resolve(strict=False)
                except Exception:
                    resolved = path_obj
                norm_path = str(resolved)
            except Exception:
                norm_path = None
            if not norm_path or norm_path in seen_paths:
                continue
            seen_paths.add(norm_path)
            if not norm_path.lower().endswith(".npy"):
                continue
            cache_path_obj = Path(norm_path)
            if not cache_path_obj.is_file():
                continue

            file_size = 0
            try:
                file_size = cache_path_obj.stat().st_size
            except OSError:
                file_size = 0

            try:
                cache_path_obj.unlink()
                removed_count += 1
                removed_bytes += file_size
                logger.debug("Removed per-tile cache file: %s", cache_path_obj)
            except FileNotFoundError:
                continue
            except OSError as exc_remove:
                logger.warning("Failed to remove per-tile cache file %s: %s", cache_path_obj, exc_remove)

        return removed_count, removed_bytes

    def _compute_phase_workers(base_workers: int, num_tasks: int, ratio: float = DEFAULT_PHASE_WORKER_RATIO) -> int:
        workers = max(1, int(base_workers * ratio))
        if num_tasks > 0:
            workers = min(workers, num_tasks)
        return max(1, workers)
    current_global_progress = 0
    
    error_messages_deps = []
    if not (ASTROPY_AVAILABLE and WCS and SkyCoord and Angle and fits and u): error_messages_deps.append("Astropy")
    if not (REPROJECT_AVAILABLE and find_optimal_celestial_wcs and reproject_and_coadd and reproject_interp): error_messages_deps.append("Reproject")
    if not (ZEMOSAIC_UTILS_AVAILABLE and zemosaic_utils): error_messages_deps.append("zemosaic_utils")
    if not (ZEMOSAIC_ASTROMETRY_AVAILABLE and zemosaic_astrometry): error_messages_deps.append("zemosaic_astrometry")
    if not (ZEMOSAIC_ALIGN_STACK_AVAILABLE and zemosaic_align_stack): error_messages_deps.append("zemosaic_align_stack")
    try: import psutil
    except ImportError: error_messages_deps.append("psutil")
    if error_messages_deps:
        pcb("run_error_critical_deps_missing", prog=None, lvl="ERROR", modules=", ".join(error_messages_deps)); return

    start_time_total_run = time.monotonic()
    pcb("CHRONO_START_REQUEST", prog=None, lvl="CHRONO_LEVEL")
    _log_memory_usage(progress_callback, "Début Run Hierarchical Mosaic")
    pcb("run_info_processing_started", prog=current_global_progress, lvl="INFO")
    _log_and_callback(
        (
            f"Options Stacking (Master Tiles): Norm='{stack_norm_method}', "
            f"Weight='{stack_weight_method}', Reject='{stack_reject_algo}', "
            f"Combine='{stack_final_combine}'"
        ),
        lvl="INFO",
        callback=progress_callback,
    )
    pcb(
        f"  Config ASTAP: Exe='{_safe_basename(astap_exe_path) if astap_exe_path else 'N/A'}', "
        f"Data='{_safe_basename(astap_data_dir_param) if astap_data_dir_param else 'N/A'}', "
        f"Radius={astap_search_radius_config}deg, Downsample={astap_downsample_config}, Sens={astap_sensitivity_config}",
        prog=None,
        lvl="DEBUG_DETAIL",
    )
    pcb(f"  Config Workers (GUI): Base demandé='{num_base_workers_config}' (0=auto)", prog=None, lvl="DEBUG_DETAIL")
    pcb(
        f"  Options Stacking (Master Tuiles): Norm='{stack_norm_method}', Weight='{stack_weight_method}', Reject='{stack_reject_algo}', "
        f"Combine='{stack_final_combine}', RGBEqualize={poststack_equalize_rgb_config}, RadialWeight={apply_radial_weight_config} "
        f"(Feather={radial_feather_fraction_config if apply_radial_weight_config else 'N/A'}, "
        f"Power={radial_shape_power_config if apply_radial_weight_config else 'N/A'}, "
        f"Floor={min_radial_weight_floor_config if apply_radial_weight_config else 'N/A'})",
        prog=None,
        lvl="DEBUG_DETAIL",
    )
    pcb(f"  Options Assemblage Final: Méthode='{final_assembly_method_config}'", prog=None, lvl="DEBUG_DETAIL")

    time_per_raw_file_wcs = None; time_per_master_tile_creation = None
    cache_dir_name = ".zemosaic_img_cache"
    temp_image_cache_dir = str(Path(output_folder).expanduser() / cache_dir_name)
    temp_master_tile_storage_dir: str | None = None
    try:
        if _path_exists(temp_image_cache_dir): shutil.rmtree(temp_image_cache_dir)
        os.makedirs(temp_image_cache_dir, exist_ok=True)
    except OSError as e_mkdir_cache:
        pcb("run_error_cache_dir_creation_failed", prog=None, lvl="ERROR", directory=temp_image_cache_dir, error=str(e_mkdir_cache)); return
    try:
        cache_probe = _probe_system_resources(
            temp_image_cache_dir,
            two_pass_enabled=two_pass_coverage_renorm_config,
            two_pass_sigma_px=two_pass_cov_sigma_px_config,
            two_pass_gain_clip=two_pass_cov_gain_clip_config,
        )
        for key, value in cache_probe.items():
            if value is not None:
                resource_probe_info[key] = value
    except Exception:
        pass

# --- Phase 1 (Prétraitement et WCS) ---
    base_progress_phase1 = current_global_progress
    _log_memory_usage(progress_callback, "Début Phase 1 (Prétraitement)")
    pcb("run_info_phase1_started_cache", prog=base_progress_phase1, lvl="INFO")
    pcb("PHASE_UPDATE:1", prog=None, lvl="ETA_LEVEL")
    
    fits_file_paths = []
    # Prépare des exclusions supplémentaires: dossier de sortie et WCS global
    try:
        _output_abs_path = Path(output_folder).expanduser().resolve() if output_folder else None
        _output_abs_norm = _normcase_path(_output_abs_path) if _output_abs_path else None
    except Exception:
        _output_abs_path = None
        _output_abs_norm = None
    try:
        _wcs_out_base = _safe_basename((worker_config_cache or {}).get("global_wcs_output_path", "global_mosaic_wcs.fits")).strip().lower()
    except Exception:
        _wcs_out_base = "global_mosaic_wcs.fits"
    # Scan des fichiers FITS dans le dossier d'entrée et ses sous-dossiers
    for root_dir_iter, dirnames_iter, files_in_dir_iter in os.walk(input_folder):
        root_path = Path(root_dir_iter)
        # Exclure les dossiers interdits dès la descente
        try:
            filtered_dirs = []
            for d in dirnames_iter:
                child = root_path / d
                # Exclusion via règle standard
                if is_path_excluded(child, EXCLUDED_DIRS):
                    continue
                # Exclure aussi le sous-arbre du dossier de sortie
                try:
                    if _output_abs_norm:
                        try:
                            child_norm = _normcase_path(child.resolve())
                        except Exception:
                            child_norm = _normcase_path(child)
                        if child_norm.startswith(_output_abs_norm):
                            continue
                except Exception:
                    pass
                filtered_dirs.append(d)
            dirnames_iter[:] = filtered_dirs
        except Exception:
            dirnames_iter[:] = [
                d
                for d in dirnames_iter
                if UNALIGNED_DIRNAME not in (root_path / d).parts
            ]

        # Assurer un ordre déterministe quelle que soit la plateforme/FS
        try:
            files_in_dir_iter = sorted(files_in_dir_iter, key=lambda s: s.lower())
        except Exception:
            files_in_dir_iter = list(files_in_dir_iter)

        for file_name_iter in files_in_dir_iter:
            if file_name_iter.lower().endswith((".fit", ".fits")):
                full_path = root_path / file_name_iter
                full_path_str = str(full_path)
                try:
                    if is_path_excluded(full_path, EXCLUDED_DIRS):
                        continue
                except Exception:
                    if UNALIGNED_DIRNAME in full_path.parts:
                        continue
                # Ignorer l'artefact du WCS global si présent dans l'arbre d'entrée
                try:
                    if _wcs_out_base and file_name_iter.strip().lower() == _wcs_out_base:
                        continue
                except Exception:
                    pass
                fits_file_paths.append(full_path_str)
    # Tri global déterministe
    try:
        fits_file_paths.sort(key=lambda p: p.lower())
    except Exception:
        fits_file_paths.sort()
    
    if not fits_file_paths: 
        pcb("run_error_no_fits_found_input", prog=current_global_progress, lvl="ERROR")
        return # Sortie anticipée si aucun fichier FITS n'est trouvé
    
    num_total_raw_files = len(fits_file_paths)
    pcb("run_info_found_potential_fits", prog=base_progress_phase1, lvl="INFO_DETAIL", num_files=num_total_raw_files)

    telemetry.maybe_emit_stats(
        _telemetry_context(
            {
                "phase_name": "Phase 1: Preprocessing",
                "phase_index": 1,
                "files_total": num_total_raw_files,
            }
        )
    )

    # --- Parallel auto-tune plan (optional) ---
    parallel_caps: ParallelCapabilities | None = None  # type: ignore[assignment]
    global_parallel_plan: ParallelPlan | None = None  # type: ignore[assignment]
    if PARALLEL_HELPERS_AVAILABLE:
        try:
            parallel_caps = detect_parallel_capabilities()
        except Exception as exc_parallel_caps:
            parallel_caps = None
            logger.warning("Parallel capability detection failed: %s", exc_parallel_caps)
        frame_h = 0
        frame_w = 0
        if isinstance(global_wcs_plan, dict):
            try:
                frame_h = int(global_wcs_plan.get("height") or 0)
            except Exception:
                frame_h = 0
            try:
                frame_w = int(global_wcs_plan.get("width") or 0)
            except Exception:
                frame_w = 0
        frame_shape = (frame_h, frame_w) if (frame_h > 0 and frame_w > 0) else (frame_h, frame_w)
        try:
            global_parallel_plan = auto_tune_parallel_plan(
                kind="global",
                frame_shape=frame_shape,
                n_frames=max(1, num_total_raw_files),
                bytes_per_pixel=4,
                config=worker_config_cache,
                caps=parallel_caps,
            )
            worker_config_cache["parallel_plan"] = global_parallel_plan
            setattr(zconfig, "parallel_plan", global_parallel_plan)
            if parallel_caps is not None:
                worker_config_cache["parallel_capabilities"] = parallel_caps
                setattr(zconfig, "parallel_capabilities", parallel_caps)
            try:
                pcb(
                    "parallel_plan_summary",
                    prog=None,
                    lvl="INFO_DETAIL",
                    cpu_workers=int(getattr(global_parallel_plan, "cpu_workers", 0)),
                    use_gpu=bool(getattr(global_parallel_plan, "use_gpu", False)),
                    rows=int(getattr(global_parallel_plan, "rows_per_chunk", 0) or 0),
                    gpu_rows=int(getattr(global_parallel_plan, "gpu_rows_per_chunk", 0) or 0),
                    memmap=bool(getattr(global_parallel_plan, "use_memmap", False)),
                    chunk_mb=float(
                        getattr(global_parallel_plan, "max_chunk_bytes", 0) / (1024 ** 2)
                        if getattr(global_parallel_plan, "max_chunk_bytes", 0)
                        else 0.0
                    ),
                )
            except Exception:
                pass
        except Exception as exc_parallel_plan:
            logger.warning("Parallel auto-tune failed: %s", exc_parallel_plan)
            global_parallel_plan = None

    # Kick off a stage progress stream so the GUI progress bar animates
    try:
        if progress_callback and callable(progress_callback):
            progress_callback("phase1_scan", 0, int(num_total_raw_files))
        # Also update a dedicated raw files counter in the GUI
        pcb(f"RAW_FILE_COUNT_UPDATE:0/{num_total_raw_files}", prog=None, lvl="ETA_LEVEL")
    except Exception:
        pass

    # --- Phase 0 (Header-only scan + early filter) ---
    # Preserve GUI-provided filter context arguments
    filter_invoked_arg = filter_invoked
    filter_overrides_arg = filter_overrides
    filtered_header_items_arg = filtered_header_items

    skip_filter_ui = bool(skip_filter_ui)
    # Resolve early filter enable policy: explicit argument takes precedence,
    # otherwise load from config, then apply skip_filter_ui override.
    if early_filter_enabled is None:
        early_filter_enabled = True
        try:
            if ZEMOSAIC_CONFIG_AVAILABLE and zemosaic_config:
                cfg0 = zemosaic_config.load_config() or {}
                early_filter_enabled = bool(cfg0.get("enable_early_filter", True))
        except Exception:
            early_filter_enabled = True
    else:
        early_filter_enabled = bool(early_filter_enabled)

    if skip_filter_ui:
        early_filter_enabled = False
        pcb("log_filter_ui_skipped", prog=None, lvl="INFO_DETAIL")

    if ASTROPY_AVAILABLE and fits is not None:
        header_items_for_filter: list[dict] = []
        filtered_items: list[dict] | None = None
        # If caller provided overrides or prior filter state, adopt them
        filter_overrides = filter_overrides_arg if isinstance(filter_overrides_arg, dict) else None
        filter_accepted = False
        filter_invoked = bool(filter_invoked_arg)
        streaming_filter_success = False

        launch_filter_interface_fn = None
        if early_filter_enabled:
            try:
                from zemosaic_filter_gui import launch_filter_interface as launch_filter_interface_fn  # type: ignore
            except ImportError:
                launch_filter_interface_fn = None
                pcb("Phase 0: filter GUI not available", prog=None, lvl="DEBUG_DETAIL")

        def _parse_filter_result(ret_obj):
            filt_items = None
            accepted_flag = False
            overrides_obj = None
            if isinstance(ret_obj, tuple) and len(ret_obj) >= 1:
                filt_items = ret_obj[0]
                if len(ret_obj) >= 2:
                    try:
                        accepted_flag = bool(ret_obj[1])
                    except Exception:
                        accepted_flag = False
                if len(ret_obj) >= 3:
                    overrides_obj = ret_obj[2]
            elif isinstance(ret_obj, list):
                filt_items = ret_obj
                accepted_flag = True
            return filt_items, accepted_flag, overrides_obj

        initial_filter_overrides = None
        try:
            initial_filter_overrides = {
                "cluster_panel_threshold": float(cluster_threshold_config),
                "cluster_target_groups": int(cluster_target_groups_config),
                "cluster_orientation_split_deg": float(cluster_orientation_split_deg_config),
            }
        except Exception:
            initial_filter_overrides = None

        # If the GUI already provided a filtered list, adopt it directly and
        # mark the streaming path as successful to avoid relaunching the UI.
        if isinstance(filtered_header_items_arg, list) and filtered_header_items_arg:
            try:
                header_items_for_filter = filtered_header_items_arg
            except Exception:
                header_items_for_filter = list(filtered_header_items_arg)
            filter_invoked = True
            filter_accepted = True
            streaming_filter_success = True
            try:
                filtered_items = list(header_items_for_filter)
            except Exception:
                filtered_items = header_items_for_filter

        solver_payload_for_filter = solver_settings if isinstance(solver_settings, dict) else None
        config_payload_for_filter = {
            "astap_executable_path": astap_exe_path,
            "astap_data_directory_path": astap_data_dir_param,
            "astap_default_search_radius": astap_search_radius_config,
            "astap_default_downsample": astap_downsample_config,
            "astap_default_sensitivity": astap_sensitivity_config,
        }

        if launch_filter_interface_fn is not None:
            try:
                filter_invoked = True
                filter_ret = launch_filter_interface_fn(
                    input_folder,
                    initial_filter_overrides,
                    stream_scan=True,
                    scan_recursive=True,
                    batch_size=200,
                    preview_cap=1500,
                    solver_settings_dict=solver_payload_for_filter,
                    config_overrides=config_payload_for_filter,
                )
                filtered_items, filter_accepted, filter_overrides = _parse_filter_result(filter_ret)
                # If the user cancelled from the filter UI, abort the run cleanly
                if isinstance(filter_overrides, dict) and filter_overrides.get("filter_cancelled"):
                    pcb("run_warn_phase0_filter_cancelled", prog=None, lvl="WARN")
                    pcb("log_key_processing_cancelled", prog=None, lvl="WARN")
                    return
                # In streaming mode the UI returns the final filtered list, not
                # the header pre-scan items. Consider the streaming path a success
                # whenever the UI was invoked without raising.
                streaming_filter_success = True
                if isinstance(filtered_items, list):
                    header_items_for_filter = filtered_items
                pcb(
                    "Phase 0: streaming filter UI completed",
                    prog=None,
                    lvl="INFO_DETAIL",
                )
            except Exception as e_filter:
                # If we fail to invoke the streaming UI, fall back to header scan.
                filter_invoked = False
                header_items_for_filter = []
                filtered_items = None
                filter_overrides = None
                filter_accepted = False
                pcb(f"Phase 0 streaming filter failed: {e_filter}", prog=None, lvl="WARN")

        if not streaming_filter_success:
            pcb("Phase 0: header scan start", prog=None, lvl="INFO_DETAIL")
            t0_hscan = time.monotonic()
            header_items_for_filter = []
            num_scanned = 0
            for idx_file, fpath in enumerate(fits_file_paths):
                hdr = None
                wcs0 = None
                shp_hw = None
                center_sc = None
                try:
                    hdr = fits.getheader(fpath, 0)
                    try:
                        nax1 = int(hdr.get("NAXIS1", 0))
                        nax2 = int(hdr.get("NAXIS2", 0))
                        if nax1 > 0 and nax2 > 0:
                            shp_hw = (nax2, nax1)
                    except Exception:
                        shp_hw = None
                    try:
                        w = WCS(hdr, naxis=2, relax=True) if WCS is not None else None
                        if w and getattr(w, "is_celestial", False):
                            wcs0 = w
                    except Exception:
                        wcs0 = None
                    if wcs0 is None:
                        try:
                            if (
                                ZEMOSAIC_ASTROMETRY_AVAILABLE
                                and zemosaic_astrometry
                                and hasattr(zemosaic_astrometry, "extract_center_from_header")
                            ):
                                center_sc = zemosaic_astrometry.extract_center_from_header(hdr)
                        except Exception:
                            center_sc = None
                    item = {
                        "path": fpath,
                        "header": hdr,
                        "index": idx_file,
                    }
                    if shp_hw:
                        item["shape"] = shp_hw
                    if wcs0 is not None:
                        item["wcs"] = wcs0
                    if center_sc is not None:
                        item["center"] = center_sc
                    header_items_for_filter.append(item)
                    num_scanned += 1
                except Exception:
                    header_items_for_filter.append({"path": fpath, "index": idx_file})
                    num_scanned += 1
            t1_hscan = time.monotonic()
            avg_t = (t1_hscan - t0_hscan) / max(1, num_scanned)
            pcb(
                f"Phase 0: header scan finished — files={num_scanned}, avg={avg_t:.4f}s/header",
                prog=None,
                lvl="DEBUG",
            )

            if launch_filter_interface_fn is not None and not filter_invoked:
                try:
                    filter_invoked = True
                    filter_ret = launch_filter_interface_fn(header_items_for_filter, initial_filter_overrides)
                    filtered_items, filter_accepted, filter_overrides = _parse_filter_result(filter_ret)
                    if isinstance(filter_overrides, dict) and filter_overrides.get("filter_cancelled"):
                        pcb("run_warn_phase0_filter_cancelled", prog=None, lvl="WARN")
                        pcb("log_key_processing_cancelled", prog=None, lvl="WARN")
                        return
                except Exception as e_filter:
                    filter_invoked = False
                    filtered_items = None
                    filter_overrides = None
                    filter_accepted = False
                    pcb(f"Phase 0 filter UI failed: {e_filter}", prog=None, lvl="WARN")
            elif not early_filter_enabled:
                pcb("Phase 0: header scan completed (filter UI disabled)", prog=None, lvl="DEBUG_DETAIL")

        phase0_header_items = header_items_for_filter

        if filter_invoked:
            if filter_overrides:
                try:
                    if "cluster_panel_threshold" in filter_overrides:
                        cluster_threshold_config = filter_overrides["cluster_panel_threshold"]
                        pcb(
                            "clusterstacks_info_override_threshold",
                            prog=None,
                            lvl="INFO_DETAIL",
                            value=cluster_threshold_config,
                        )
                    if "cluster_target_groups" in filter_overrides:
                        cluster_target_groups_config = filter_overrides["cluster_target_groups"]
                        pcb(
                            "clusterstacks_info_override_target_groups",
                            prog=None,
                            lvl="INFO_DETAIL",
                            value=cluster_target_groups_config,
                        )
                    if "cluster_orientation_split_deg" in filter_overrides:
                        cluster_orientation_split_deg_config = filter_overrides["cluster_orientation_split_deg"]
                        pcb(
                            "clusterstacks_info_override_orientation_split",
                            prog=None,
                            lvl="INFO_DETAIL",
                            value=cluster_orientation_split_deg_config,
                        )
                except Exception:
                    pass
                try:
                    raw_groups_override = (
                        filter_overrides.get("preplan_master_groups")
                        if isinstance(filter_overrides, dict)
                        else None
                    )
                    if isinstance(raw_groups_override, list):
                        mapped_groups: list[list[str]] = []
                        for group in raw_groups_override:
                            if not isinstance(group, (list, tuple)):
                                continue
                            normalized_group: list[str] = []
                            for item in group:
                                path_val = None
                                if isinstance(item, dict):
                                    path_val = item.get("path") or item.get("path_raw")
                                elif isinstance(item, str):
                                    path_val = item
                                norm_path = _normalize_path_for_matching(path_val)
                                if norm_path:
                                    normalized_group.append(norm_path)
                            if normalized_group:
                                mapped_groups.append(normalized_group)
                        if mapped_groups:
                            preplan_groups_override_paths = mapped_groups
                            pcb(
                                f"Phase 0 filter provided {len(mapped_groups)} preplanned group(s).",
                                prog=None,
                                lvl="INFO_DETAIL",
                            )
                except Exception as e_preplan:
                    pcb(
                        f"Phase 0 filter preplan override failed: {e_preplan}",
                        prog=None,
                        lvl="DEBUG_DETAIL",
                    )

            if not filter_accepted:
                pcb("run_warn_phase0_filter_cancelled", prog=None, lvl="WARN")
                pcb("Phase 0: filter cancelled -> proceeding with all files", prog=None, lvl="INFO_DETAIL")
            if filter_accepted and isinstance(filtered_items, list):
                new_paths = [
                    item.get("path")
                    for item in filtered_items
                    if isinstance(item, dict) and item.get("path")
                ]
                filtered_paths: list[str] = []
                for candidate_path in new_paths:
                    try:
                        if is_path_excluded(candidate_path, EXCLUDED_DIRS):
                            continue
                    except Exception:
                        if UNALIGNED_DIRNAME in _normpath_parts(candidate_path):
                            continue
                    filtered_paths.append(candidate_path)

                fits_file_paths = filtered_paths
                pcb(
                    f"Phase 0: selection after filter = {len(fits_file_paths)} files",
                    prog=None,
                    lvl="INFO_DETAIL",
                )
                if fits_file_paths:
                    try:
                        fits_file_paths.sort(key=lambda p: p.lower())
                    except Exception:
                        fits_file_paths.sort()
            elif filter_accepted and not filtered_items:
                pcb("Phase 0: filter returned no items", prog=None, lvl="WARN")
    else:
        phase0_header_items = []
        pcb("Phase 0: header scan unavailable (Astropy missing)", prog=None, lvl="WARN")

    phase0_lookup = {item["path"]: item for item in phase0_header_items if isinstance(item, dict) and item.get("path")}
    per_frame_info = _estimate_per_frame_cost_mb(phase0_header_items)
    auto_caps_info = _compute_auto_tile_caps(
        resource_probe_info,
        per_frame_info,
        policy_max=0,
        policy_min=8,
        user_max_override=int(max_raw_per_master_tile_config) if max_raw_per_master_tile_config else None,
    )
    try:
        msg = (
            "AutoCaps: per_frame≈{pf:.1f} MB, RAM_free≈{rf:.0f} MB → "
            "frames_by_ram={fbr}, cap={cap}, memmap={mm}, GPUHint={gpu}, parallel={par}".format(
                pf=auto_caps_info.get("per_frame_mb", 0.0),
                rf=resource_probe_info.get("ram_available_mb", 0.0) or 0.0,
                fbr=auto_caps_info.get("frames_by_ram", 0),
                cap=auto_caps_info.get("cap"),
                mm="on" if auto_caps_info.get("memmap") else "off",
                gpu=auto_caps_info.get("gpu_batch_hint") or "n/a",
                par=auto_caps_info.get("parallel_groups", 1),
            )
        )
        _log_and_callback(msg, prog=None, lvl="INFO_DETAIL", callback=progress_callback)
    except Exception:
        pass
    auto_resource_strategy = {
        "cap": auto_caps_info.get("cap"),
        "min_cap": auto_caps_info.get("min_cap"),
        "memmap": auto_caps_info.get("memmap"),
        "memmap_budget_mb": auto_caps_info.get("memmap_budget_mb"),
        "gpu_batch_hint": auto_caps_info.get("gpu_batch_hint"),
        "parallel_groups": auto_caps_info.get("parallel_groups"),
        "per_frame_mb": auto_caps_info.get("per_frame_mb"),
    }

    
    # --- Détermination du nombre de workers de BASE ---
    effective_base_workers = 0
    num_logical_processors = os.cpu_count() or 1 
    
    if num_base_workers_config <= 0: # Mode automatique (0 de la GUI)
        desired_auto_ratio = 0.75
        effective_base_workers = max(1, int(np.ceil(num_logical_processors * desired_auto_ratio)))
        pcb(f"WORKERS_CONFIG: Mode Auto. Base de workers calculée: {effective_base_workers} ({desired_auto_ratio*100:.0f}% de {num_logical_processors} processeurs logiques)", prog=None, lvl="INFO_DETAIL")
    else: # Mode manuel
        effective_base_workers = min(num_base_workers_config, num_logical_processors)
        if effective_base_workers < num_base_workers_config:
             pcb(f"WORKERS_CONFIG: Demande GUI ({num_base_workers_config}) limitée à {effective_base_workers} (total processeurs logiques: {num_logical_processors}).", prog=None, lvl="WARN")
        pcb(f"WORKERS_CONFIG: Mode Manuel. Base de workers: {effective_base_workers}", prog=None, lvl="INFO_DETAIL")
    
    if effective_base_workers <= 0: # Fallback
        effective_base_workers = 1
        pcb(f"WORKERS_CONFIG: AVERT - effective_base_workers était <= 0, forcé à 1.", prog=None, lvl="WARN")

    # Calcul du nombre de workers pour la Phase 1
    actual_num_workers_ph1 = _compute_phase_workers(
        effective_base_workers,
        num_total_raw_files,
        DEFAULT_PHASE_WORKER_RATIO,
    )
    pcb(
        f"WORKERS_PHASE1: Utilisation de {actual_num_workers_ph1} worker(s). (Base: {effective_base_workers}, Fichiers: {num_total_raw_files})",
        prog=None,
        lvl="INFO",
    )  # Log mis à jour pour plus de clarté
    
    start_time_phase1 = time.monotonic()
    all_raw_files_processed_info_dict = {} # Pour stocker les infos des fichiers traités avec succès
    files_processed_count_ph1 = 0      # Compteur pour les fichiers soumis au ThreadPoolExecutor

    with ThreadPoolExecutor(max_workers=actual_num_workers_ph1, thread_name_prefix="ZeMosaic_Ph1_") as executor_ph1:
        batch_size = 200
        for i in range(0, len(fits_file_paths), batch_size):
            batch = fits_file_paths[i:i+batch_size]
            future_to_filepath_ph1 = {
                executor_ph1.submit(
                    get_wcs_and_pretreat_raw_file,
                    f_path,
                    astap_exe_path,
                    astap_data_dir_param,
                    astap_search_radius_config,
                    astap_downsample_config,
                    astap_sensitivity_config,
                    180,
                    progress_callback,
                    temp_image_cache_dir,
                    solver_settings
                ): f_path for f_path in batch
            }

            for future in as_completed(future_to_filepath_ph1):
                file_path_original = future_to_filepath_ph1[future]
                files_processed_count_ph1 += 1  # Incrémenter pour chaque future terminée

                # Update GUI stage progress (files read / total)
                try:
                    if progress_callback and callable(progress_callback):
                        progress_callback("phase1_scan", int(files_processed_count_ph1), int(num_total_raw_files))
                    # Mirror the count so the GUI can show X/N files
                    pcb(f"RAW_FILE_COUNT_UPDATE:{files_processed_count_ph1}/{num_total_raw_files}", prog=None, lvl="ETA_LEVEL")
                except Exception:
                    pass

                telemetry.maybe_emit_stats(
                    _telemetry_context(
                        {
                            "phase_name": "Phase 1: Preprocessing",
                            "phase_index": 1,
                            "files_done": files_processed_count_ph1,
                            "files_total": num_total_raw_files,
                        }
                    )
                )

                prog_step_phase1 = base_progress_phase1 + int(
                    PROGRESS_WEIGHT_PHASE1_RAW_SCAN * (files_processed_count_ph1 / max(1, num_total_raw_files))
                )

                try:
                    # Récupérer le résultat de la tâche
                    img_data_adu, wcs_obj_solved, header_obj_updated, hp_mask_path = future.result()

                    # Si la tâche a réussi (ne retourne pas que des None)
                    if (
                        img_data_adu is not None
                        and wcs_obj_solved is not None
                        and header_obj_updated is not None
                    ):
                        # Sauvegarder les données prétraitées en .npy
                        cache_file_basename = f"preprocessed_{Path(file_path_original).stem}_{files_processed_count_ph1}.npy"
                        cached_image_path = Path(temp_image_cache_dir) / cache_file_basename
                        try:
                            np.save(str(cached_image_path), img_data_adu)
                        except Exception as e_save_npy:
                            pcb(
                                "run_error_phase1_save_npy_failed",
                                prog=prog_step_phase1,
                                lvl="ERROR",
                                filename=_safe_basename(file_path_original),
                                error=str(e_save_npy),
                            )
                            logger.error(f"Erreur sauvegarde NPY pour {file_path_original}:", exc_info=True)
                        else:
                            # Stocker les informations pour les phases suivantes
                            entry = {
                                'path_raw': file_path_original,
                                'path_preprocessed_cache': str(cached_image_path),
                                'path_hotpix_mask': hp_mask_path,
                                'wcs': wcs_obj_solved,
                                'header': header_obj_updated,
                                'preprocessed_shape': tuple(int(dim) for dim in getattr(img_data_adu, 'shape', []) or ()),
                            }
                            meta = phase0_lookup.get(file_path_original)
                            if isinstance(meta, dict):
                                if 'index' in meta:
                                    entry['phase0_index'] = meta.get('index')
                                if 'center' in meta:
                                    entry['phase0_center'] = meta.get('center')
                                if 'shape' in meta:
                                    entry['phase0_shape'] = meta.get('shape')
                                if 'wcs' in meta and 'wcs' not in entry:
                                    entry['phase0_wcs'] = meta.get('wcs')
                            all_raw_files_processed_info_dict[file_path_original] = entry
                        finally:
                            # Libérer la mémoire des données image dès que possible
                            del img_data_adu
                            gc.collect()
                    else:
                        # Le fichier a échoué (ex: WCS non résolu et déplacé)
                        # get_wcs_and_pretreat_raw_file a déjà loggué l'échec spécifique.
                        pcb(
                            "run_warn_phase1_wcs_pretreat_failed_or_skipped_thread",
                            prog=prog_step_phase1,
                            lvl="WARN",
                            filename=_safe_basename(file_path_original),
                        )
                        if img_data_adu is not None:
                            del img_data_adu
                            gc.collect()

                except Exception as exc_thread:
                    # Erreur imprévue dans la future elle-même
                    pcb(
                        "run_error_phase1_thread_exception",
                        prog=prog_step_phase1,
                        lvl="ERROR",
                        filename=_safe_basename(file_path_original),
                        error=str(exc_thread),
                    )
                    logger.error(
                        f"Exception non gérée dans le thread Phase 1 pour {file_path_original}:",
                        exc_info=True,
                    )

                # Log de mémoire et ETA
                if (
                    files_processed_count_ph1 % max(1, num_total_raw_files // 10) == 0
                    or files_processed_count_ph1 == num_total_raw_files
                ):
                    _log_memory_usage(
                        progress_callback,
                        f"Phase 1 - Traité {files_processed_count_ph1}/{num_total_raw_files}",
                    )

                elapsed_phase1 = time.monotonic() - start_time_phase1
                if files_processed_count_ph1 > 0:
                    time_per_raw_file_wcs = elapsed_phase1 / files_processed_count_ph1
                    eta_phase1_sec = (num_total_raw_files - files_processed_count_ph1) * time_per_raw_file_wcs
                    current_progress_in_run_percent = base_progress_phase1 + (
                        files_processed_count_ph1 / max(1, num_total_raw_files)
                    ) * PROGRESS_WEIGHT_PHASE1_RAW_SCAN
                    time_per_percent_point_global = (
                        (time.monotonic() - start_time_total_run) / max(1, current_progress_in_run_percent)
                        if current_progress_in_run_percent > 0
                        else (time.monotonic() - start_time_total_run)
                    )
                    total_eta_sec = eta_phase1_sec + (
                        100 - current_progress_in_run_percent
                    ) * time_per_percent_point_global
                    update_gui_eta(total_eta_sec)

    # Construire la liste finale des informations des fichiers traités avec succès
    all_raw_files_processed_info = [
        all_raw_files_processed_info_dict[fp] 
        for fp in fits_file_paths 
        if fp in all_raw_files_processed_info_dict
    ]
    
    if not all_raw_files_processed_info: 
        pcb("run_error_phase1_no_valid_raws_after_cache", prog=(base_progress_phase1 + PROGRESS_WEIGHT_PHASE1_RAW_SCAN), lvl="ERROR")
        return # Sortie anticipée si aucun fichier n'a pu être traité avec succès

    current_global_progress = base_progress_phase1 + PROGRESS_WEIGHT_PHASE1_RAW_SCAN
    _log_memory_usage(progress_callback, "Fin Phase 1 (Prétraitement)")
    pcb("run_info_phase1_finished_cache", prog=current_global_progress, lvl="INFO", num_valid_raws=len(all_raw_files_processed_info))
    # --- Optional interactive filtering between Phase 1 and Phase 2 ---
    try:
        raw_files_with_wcs = all_raw_files_processed_info
        try:
            raw_files_with_wcs = raw_files_with_wcs
            # Keep the same variable name used by subsequent phases
            all_raw_files_processed_info = raw_files_with_wcs
        except ImportError:
            # Optional module not present: silently skip
            pass
        except Exception as e_opt:
            logger.warning(f"Filtrage facultatif désactivé suite à une erreur : {e_opt}")
    except Exception as e_hook:
        # Any unexpected issue in the hook wrapper: continue unchanged
        logger.warning(f"Filtrage facultatif non appliqué: {e_hook}")
    if time_per_raw_file_wcs: 
        pcb(f"    Temps moyen/brute (P1): {time_per_raw_file_wcs:.2f}s", prog=None, lvl="DEBUG")

    # --- Phase 2 (Clustering) ---
    base_progress_phase2 = current_global_progress
    _log_memory_usage(progress_callback, "Début Phase 2 (Clustering)")
    pcb("run_info_phase2_started", prog=base_progress_phase2, lvl="INFO")
    pcb("PHASE_UPDATE:2", prog=None, lvl="ETA_LEVEL")
    # Use order-invariant connected-components clustering for robustness
    preplan_groups_active = False
    if preplan_groups_override_paths:
        try:
            path_lookup = {
                _normalize_path_for_matching(info.get("path_raw") or info.get("path")): info
                for info in all_raw_files_processed_info
                if isinstance(info, dict)
            }
            used_paths: set[str] = set()
            mapped_info_groups: list[list[dict]] = []
            missing_preplan: list[str] = []
            for group_paths in preplan_groups_override_paths:
                current_group: list[dict] = []
                for path_norm in group_paths:
                    if not path_norm:
                        continue
                    info = path_lookup.get(path_norm)
                    if info is not None:
                        current_group.append(info)
                        used_paths.add(path_norm)
                    else:
                        missing_preplan.append(path_norm)
                if current_group:
                    mapped_info_groups.append(current_group)
            if mapped_info_groups:
                leftovers = [
                    info
                    for info in all_raw_files_processed_info
                    if _normalize_path_for_matching(info.get("path_raw") or info.get("path")) not in used_paths
                ]
                if leftovers:
                    mapped_info_groups.append(leftovers)
                seestar_stack_groups = mapped_info_groups
                preplan_groups_active = True
                _log_and_callback(
                    f"Phase 2: using {len(mapped_info_groups)} preplanned group(s) from filter UI.",
                    prog=None,
                    lvl="INFO_DETAIL",
                    callback=progress_callback,
                )
                if missing_preplan:
                    try:
                        preview = ", ".join(_safe_basename(p) for p in missing_preplan[:5] if p)
                    except Exception:
                        preview = ""
                    _log_and_callback(
                        "Phase 2: some preplanned paths were not found after preprocessing: "
                        + (preview if preview else str(len(missing_preplan))),
                        prog=None,
                        lvl="WARN",
                        callback=progress_callback,
                    )
        except Exception as e_preplan_map:
            _log_and_callback(
                f"Phase 2: failed to map preplanned groups ({e_preplan_map}). Falling back to clustering.",
                prog=None,
                lvl="WARN",
                callback=progress_callback,
            )
            preplan_groups_active = False

    if not preplan_groups_active:
        seestar_stack_groups = cluster_seestar_stacks_connected(
            all_raw_files_processed_info,
            SEESTAR_STACK_CLUSTERING_THRESHOLD_DEG,
            progress_callback,
            orientation_split_threshold_deg=ORIENTATION_SPLIT_THRESHOLD_DEG,
        )
        if STACK_RAM_BUDGET_BYTES > 0 and seestar_stack_groups:
            seestar_stack_groups, ram_budget_adjustments = _apply_ram_budget_to_groups(
                seestar_stack_groups,
                STACK_RAM_BUDGET_BYTES,
                float(SEESTAR_STACK_CLUSTERING_THRESHOLD_DEG),
                float(ORIENTATION_SPLIT_THRESHOLD_DEG),
            )
            for adj in ram_budget_adjustments:
                method = adj.get("method")
                if method == "recluster":
                    _log_and_callback(
                        "clusterstacks_warn_ram_budget_recluster",
                        prog=None,
                        lvl="WARN",
                        callback=progress_callback,
                        group_index=adj.get("group_index"),
                        original_frames=adj.get("original_frames"),
                        num_subgroups=adj.get("num_subgroups"),
                        new_threshold_deg=adj.get("new_threshold_deg"),
                        attempts=adj.get("attempts"),
                        estimated_mb=adj.get("estimated_mb"),
                        budget_mb=adj.get("budget_mb"),
                    )
                elif method == "split":
                    _log_and_callback(
                        "clusterstacks_warn_ram_budget_split",
                        prog=None,
                        lvl="WARN",
                        callback=progress_callback,
                        group_index=adj.get("group_index"),
                        original_frames=adj.get("original_frames"),
                        num_subgroups=adj.get("num_subgroups"),
                        segment_size=adj.get("segment_size"),
                        estimated_mb=adj.get("estimated_mb"),
                        budget_mb=adj.get("budget_mb"),
                    )
                    if adj.get("still_over_budget"):
                        _log_and_callback(
                            "clusterstacks_warn_ram_budget_split_still_over",
                            prog=None,
                            lvl="WARN",
                            callback=progress_callback,
                            group_index=adj.get("group_index"),
                            segment_size=adj.get("segment_size"),
                            budget_mb=adj.get("budget_mb"),
                        )
                elif method == "single_over_budget":
                    _log_and_callback(
                        "clusterstacks_warn_ram_budget_single_over",
                        prog=None,
                        lvl="WARN",
                        callback=progress_callback,
                        group_index=adj.get("group_index"),
                        estimated_mb=adj.get("estimated_mb"),
                        budget_mb=adj.get("budget_mb"),
                    )
    # Diagnostic: nearest-neighbor separation percentiles to help tune eps
    try:
        panel_centers_sky_dbg = []
        for info in all_raw_files_processed_info:
            wcs_obj = info.get("wcs")
            if not (wcs_obj and getattr(wcs_obj, "is_celestial", False)):
                continue
            try:
                if getattr(wcs_obj, "pixel_shape", None):
                    cx = wcs_obj.pixel_shape[0] / 2.0
                    cy = wcs_obj.pixel_shape[1] / 2.0
                    center_world = wcs_obj.pixel_to_world(cx, cy)
                elif hasattr(wcs_obj, "wcs") and hasattr(wcs_obj.wcs, "crval"):
                    center_world = SkyCoord(
                        ra=float(wcs_obj.wcs.crval[0]) * u.deg,
                        dec=float(wcs_obj.wcs.crval[1]) * u.deg,
                        frame="icrs",
                    )
                else:
                    continue
                panel_centers_sky_dbg.append(center_world)
            except Exception:
                continue
        if len(panel_centers_sky_dbg) >= 2:
            coords_dbg = SkyCoord(ra=[c.ra for c in panel_centers_sky_dbg], dec=[c.dec for c in panel_centers_sky_dbg], frame="icrs")
            try:
                _, sep_nn, _ = coords_dbg.match_to_catalog_sky(coords_dbg, nthneighbor=1)
                nn = np.asarray(sep_nn.deg, dtype=float)
                p10 = float(np.nanpercentile(nn, 10.0))
                p50 = float(np.nanpercentile(nn, 50.0))
                p90 = float(np.nanpercentile(nn, 90.0))
                _log_and_callback(
                    f"Cluster NN stats (deg): P10={p10:.4f}, P50={p50:.4f}, P90={p90:.4f}",
                    prog=None,
                    lvl="DEBUG_DETAIL",
                    callback=progress_callback,
                )
            except Exception:
                pass
    except Exception:
        pass
    # If clustering is pathologically conservative (almost one group per image),
    # auto-relax the threshold based on nearest-neighbor distances to avoid
    # producing hundreds of master tiles for tightly-dithered panels.
    try:
        total_inputs_for_cluster = len(all_raw_files_processed_info)
        groups_initial = len(seestar_stack_groups)
        if total_inputs_for_cluster > 2 and groups_initial >= max(3, int(0.9 * total_inputs_for_cluster)):
            # Compute a robust suggested threshold from the 90th percentile of
            # nearest-neighbor separations between panel centers.
            # Rebuild centers the same way as clustering helpers do.
            panel_centers_sky = []
            for info in all_raw_files_processed_info:
                wcs_obj = info.get("wcs")
                if not (wcs_obj and getattr(wcs_obj, "is_celestial", False)):
                    continue
                try:
                    if getattr(wcs_obj, "pixel_shape", None):
                        cx = wcs_obj.pixel_shape[0] / 2.0
                        cy = wcs_obj.pixel_shape[1] / 2.0
                        center_world = wcs_obj.pixel_to_world(cx, cy)
                    elif hasattr(wcs_obj, "wcs") and hasattr(wcs_obj.wcs, "crval"):
                        center_world = SkyCoord(
                            ra=float(wcs_obj.wcs.crval[0]) * u.deg,
                            dec=float(wcs_obj.wcs.crval[1]) * u.deg,
                            frame="icrs",
                        )
                    else:
                        continue
                    panel_centers_sky.append(center_world)
                except Exception:
                    continue

            if len(panel_centers_sky) >= 2:
                coords = SkyCoord(
                    ra=[c.ra for c in panel_centers_sky],
                    dec=[c.dec for c in panel_centers_sky],
                    frame="icrs",
                )
                try:
                    # Nearest neighbor (excluding self). Astropy handles wrap.
                    _, sep2d, _ = coords.match_to_catalog_sky(coords, nthneighbor=1)
                    nn_deg = np.asarray(sep2d.deg, dtype=float)
                    # Robust high-quantile of dithers; add a small headroom.
                    p90 = float(np.nanpercentile(nn_deg, 90.0)) if nn_deg.size else 0.0
                    # Propose a relaxed threshold within sane bounds.
                    thr_initial = float(SEESTAR_STACK_CLUSTERING_THRESHOLD_DEG)
                    thr_candidate = max(thr_initial, p90 * 1.2)
                    thr_candidate = float(min(max(thr_candidate, 0.01), 1.0))  # clamp 0.01°..1.0°

                    if thr_candidate > thr_initial:
                        _log_and_callback(
                            f"Cluster AUTO: threshold {thr_initial:.3f}° too conservative -> {groups_initial}/{total_inputs_for_cluster} groups.",
                            prog=None,
                            lvl="INFO_DETAIL",
                            callback=progress_callback,
                        )
                        _log_and_callback(
                            f"Cluster AUTO: relaxing to {thr_candidate:.3f}° (≈1.2×P90 NN={p90:.3f}°) and re-clustering...",
                            prog=None,
                            lvl="INFO_DETAIL",
                            callback=progress_callback,
                        )
                        seestar_stack_groups = cluster_seestar_stacks_connected(
                            all_raw_files_processed_info, thr_candidate, progress_callback
                        )
                        groups_after = len(seestar_stack_groups)
                        _log_and_callback(
                            f"Cluster AUTO: re-clustered into {groups_after} groups (was {groups_initial}).",
                            prog=None,
                            lvl="INFO_DETAIL",
                            callback=progress_callback,
                        )
                except Exception as e_auto_relax:
                    _log_and_callback(
                        f"Cluster AUTO: failed to compute NN-based relax: {e_auto_relax}",
                        prog=None,
                        lvl="DEBUG_DETAIL",
                        callback=progress_callback,
                    )
    except Exception as e_cluster_guard:
        _log_and_callback(
            f"Cluster AUTO: guard exception: {e_cluster_guard}", prog=None, lvl="DEBUG_DETAIL", callback=progress_callback
        )

    # Optional: drive clustering to a target number of groups by relaxing
    # the threshold via a bounded search. Disabled when target <= 0.
    try:
        target_groups = int(cluster_target_groups_config or 0)
    except Exception:
        target_groups = 0
    if (not preplan_groups_active) and target_groups > 0 and len(seestar_stack_groups) != target_groups:
        try:
            # Build coordinates array
            panel_centers_sky = []
            for info in all_raw_files_processed_info:
                wcs_obj = info.get("wcs")
                if not (wcs_obj and getattr(wcs_obj, "is_celestial", False)):
                    continue
                try:
                    if getattr(wcs_obj, "pixel_shape", None):
                        cx = wcs_obj.pixel_shape[0] / 2.0
                        cy = wcs_obj.pixel_shape[1] / 2.0
                        center_world = wcs_obj.pixel_to_world(cx, cy)
                    elif hasattr(wcs_obj, "wcs") and hasattr(wcs_obj.wcs, "crval"):
                        center_world = SkyCoord(
                            ra=float(wcs_obj.wcs.crval[0]) * u.deg,
                            dec=float(wcs_obj.wcs.crval[1]) * u.deg,
                            frame="icrs",
                        )
                    else:
                        continue
                    panel_centers_sky.append(center_world)
                except Exception:
                    continue

            if len(panel_centers_sky) >= 2:
                coords = SkyCoord(
                    ra=[c.ra for c in panel_centers_sky],
                    dec=[c.dec for c in panel_centers_sky],
                    frame="icrs",
                )
                # Establish an upper bound big enough that all panels connect
                # (max pairwise separation). Clamp to 5 degrees to avoid
                # pathological values.
                try:
                    sep_mat_deg = coords.separation(coords).deg
                    max_pair_deg = float(np.nanmax(sep_mat_deg)) if np.size(sep_mat_deg) else 0.5
                except Exception:
                    max_pair_deg = 0.5
                thr_current = float(SEESTAR_STACK_CLUSTERING_THRESHOLD_DEG)
                def _count_groups(thr: float) -> tuple[int, list]:
                    g = cluster_seestar_stacks_connected(
                        all_raw_files_processed_info,
                        float(thr),
                        None,
                        orientation_split_threshold_deg=ORIENTATION_SPLIT_THRESHOLD_DEG,
                    )
                    return len(g), g
                cnt_cur = len(seestar_stack_groups)
                # Direction: if too many groups, increase threshold; if too few, decrease.
                if cnt_cur > target_groups:
                    lo = thr_current
                    hi = float(min(max(max_pair_deg, lo * 2.0, 0.05), 5.0))
                    cnt_hi, groups_hi = _count_groups(hi)
                    # Expand hi until we get <= target (fewer groups) or cap
                    expand_iter = 0
                    while cnt_hi > target_groups and hi < 5.0 and expand_iter < 8:
                        hi = min(hi * 1.5 + 1e-6, 5.0)
                        cnt_hi, groups_hi = _count_groups(hi)
                        expand_iter += 1
                    best_thr = hi
                    best_groups = groups_hi
                    for _ in range(14):
                        mid = 0.5 * (lo + hi)
                        cnt_mid, groups_mid = _count_groups(mid)
                        if cnt_mid > target_groups:
                            lo = mid
                        else:
                            hi = mid
                            best_thr = mid
                            best_groups = groups_mid
                else:
                    # Need more groups ⇒ lower the threshold
                    hi = thr_current
                    lo = max(1e-6, hi / 2.0)
                    cnt_lo, groups_lo = _count_groups(lo)
                    shrink_iter = 0
                    while cnt_lo < target_groups and lo > 1e-6 and shrink_iter < 12:
                        hi = lo
                        lo = max(1e-6, lo / 1.5)
                        cnt_lo, groups_lo = _count_groups(lo)
                        shrink_iter += 1
                    best_thr = lo
                    best_groups = groups_lo
                    # Binary search upward to approach target from the high side (more stable)
                    for _ in range(14):
                        mid = 0.5 * (lo + hi)
                        cnt_mid, groups_mid = _count_groups(mid)
                        if cnt_mid < target_groups:
                            # still too few groups ⇒ lower threshold more
                            hi = mid
                        else:
                            lo = mid
                            best_thr = mid
                            best_groups = groups_mid
                _log_and_callback(
                    f"Cluster AUTO Target: threshold -> {best_thr:.4f}° for ≈{len(best_groups)} groups (target {target_groups}).",
                    prog=None,
                    lvl="INFO_DETAIL",
                    callback=progress_callback,
                )
                seestar_stack_groups = best_groups
        except Exception as e_target:
            _log_and_callback(
                f"Cluster AUTO Target: search failed: {e_target}", prog=None, lvl="DEBUG_DETAIL", callback=progress_callback
            )
    if not seestar_stack_groups:
        pcb("run_error_phase2_no_groups", prog=(base_progress_phase2 + PROGRESS_WEIGHT_PHASE2_CLUSTERING), lvl="ERROR")
        return
    if (not preplan_groups_active) and auto_caps_info and seestar_stack_groups:
        try:
            cap_value = int(auto_caps_info.get("cap", 0))
            min_value = int(auto_caps_info.get("min_cap", 8))
        except Exception:
            cap_value = 0
            min_value = 8
        if cap_value > 0:
            original_count = len(seestar_stack_groups)
            seestar_stack_groups = _auto_split_groups(
                seestar_stack_groups,
                cap_value,
                min_value,
                progress_callback=progress_callback,
            )
            if len(seestar_stack_groups) != original_count:
                try:
                    _log_and_callback(
                        f"AutoSplit summary: {original_count} -> {len(seestar_stack_groups)} subgroup(s) (cap={cap_value})",
                        prog=None,
                        lvl="INFO_DETAIL",
                        callback=progress_callback,
                    )
                except Exception:
                    pass
            if min_value > 0:
                seestar_stack_groups = _merge_small_groups(
                    seestar_stack_groups,
                    min_size=min_value,
                    cap=cap_value,
                )

    # Do not subdivide groups if a target group count is set; respect clustering first.
    if (
        not preplan_groups_active
        and (cluster_target_groups_config is None or int(cluster_target_groups_config) <= 0)
        and max_raw_per_master_tile_config
        and max_raw_per_master_tile_config > 0
    ):
        new_groups = []
        for g in seestar_stack_groups:
            for i in range(0, len(g), max_raw_per_master_tile_config):
                new_groups.append(g[i:i + max_raw_per_master_tile_config])
        if len(new_groups) != len(seestar_stack_groups):
            pcb(
                "clusterstacks_info_groups_split_manual_limit",
                prog=None,
                lvl="INFO_DETAIL",
                original=len(seestar_stack_groups),
                new=len(new_groups),
                limit=max_raw_per_master_tile_config,
            )
        seestar_stack_groups = new_groups
    cpu_total = os.cpu_count() or 1
    winsor_worker_limit = max(1, min(int(winsor_worker_limit_config), cpu_total))
    winsor_max_frames_per_pass = max(0, int(winsor_max_frames_per_pass_config))
    global_wcs_plan["winsor_worker_limit"] = int(winsor_worker_limit)
    global_wcs_plan["winsor_max_frames_per_pass"] = int(winsor_max_frames_per_pass)
    global_wcs_plan["use_align_helpers"] = True
    global_wcs_plan["prefer_gpu_helpers"] = bool(use_gpu_phase5_flag)
    pcb(
        f"Winsor worker limit set to {winsor_worker_limit}" + (
            " (ProcessPoolExecutor enabled)" if winsor_worker_limit > 1 else ""
        ),
        prog=None,
        lvl="INFO",
    )
    if winsor_max_frames_per_pass > 0:
        pcb(
            f"Winsor streaming limit set to {winsor_max_frames_per_pass} frame(s) per pass",
            prog=None,
            lvl="INFO_DETAIL",
        )
    sds_stack_params = {
        "stack_reject_algo": stack_reject_algo,
        "stack_weight_method": stack_weight_method,
        "stack_norm_method": stack_norm_method,
        "stack_kappa_low": stack_kappa_low,
        "stack_kappa_high": stack_kappa_high,
        "stack_final_combine": stack_final_combine,
        "parsed_winsor_limits": parsed_winsor_limits,
        "winsor_worker_limit": winsor_worker_limit,
        "winsor_max_frames_per_pass": winsor_max_frames_per_pass,
        "apply_radial_weight": apply_radial_weight_config,
        "radial_feather_fraction": radial_feather_fraction_config,
        "radial_shape_power": radial_shape_power_config,
        "poststack_equalize_rgb": poststack_equalize_rgb_config,
    }
    manual_limit = max_raw_per_master_tile_config
    memmap_streaming_enabled = bool(auto_caps_info and auto_caps_info.get("memmap"))
    allow_auto_limit = (
        auto_limit_frames_per_master_tile_config
        and (cluster_target_groups_config is None or int(cluster_target_groups_config) <= 0)
        and not memmap_streaming_enabled
        and not preplan_groups_active
    )
    if allow_auto_limit and seestar_stack_groups:
        try:
            sample_path = None
            for group in seestar_stack_groups:
                if group:
                    sample_path = group[0].get('path_preprocessed_cache')
                    if sample_path:
                        break
            if sample_path is None:
                raise RuntimeError("auto-limit sample path unavailable")
            sample_arr = np.load(sample_path, mmap_mode='r')
            bytes_per_frame = sample_arr.nbytes
            sample_shape = sample_arr.shape
            sample_arr = None
            available_bytes = psutil.virtual_memory().available
            expected_workers = max(1, int(effective_base_workers * ALIGNMENT_PHASE_WORKER_RATIO))
            # Be more conservative: align/stack create extra buffers; use a larger safety factor
            limit = max(
                1,
                int(
                    available_bytes // (expected_workers * bytes_per_frame * 12)
                ),
            )
            # Clamp to a reasonable upper bound if no manual cap is set
            if manual_limit <= 0:
                limit = min(limit, 100)
            if manual_limit > 0:
                limit = min(limit, manual_limit)
            winsor_worker_limit = min(winsor_worker_limit, limit)
            new_groups = []
            for g in seestar_stack_groups:
                for i in range(0, len(g), limit):
                    new_groups.append(g[i:i+limit])
            if len(new_groups) != len(seestar_stack_groups):
                pcb(
                    "clusterstacks_info_groups_split_auto_limit",
                    prog=None,
                    lvl="INFO_DETAIL",
                    original=len(seestar_stack_groups),
                    new=len(new_groups),
                    limit=limit,
                    shape=str(sample_shape),
                )
            seestar_stack_groups = new_groups
            if manual_limit > 0 and limit != manual_limit:
                logger.info(
                    "Manual frame limit (%d) is lower than auto limit, using manual value.",
                    manual_limit,
                )
        except Exception as e_auto:
            pcb("clusterstacks_warn_auto_limit_failed", prog=None, lvl="WARN", error=str(e_auto))
    current_global_progress = base_progress_phase2 + PROGRESS_WEIGHT_PHASE2_CLUSTERING
    num_seestar_stacks_to_process = len(seestar_stack_groups)
    telemetry.maybe_emit_stats(
        _telemetry_context(
            {
                "phase_name": "Phase 2: Clustering",
                "phase_index": 2,
                "files_total": num_total_raw_files,
                "tiles_total": num_seestar_stacks_to_process,
            }
        )
    )
    _log_memory_usage(progress_callback, "Fin Phase 2"); pcb("run_info_phase2_finished", prog=current_global_progress, lvl="INFO", num_groups=num_seestar_stacks_to_process)


    # --- IO-aware adaptation (bench read speed on cache + write speed on output) ---
    io_read_mbps, io_write_mbps = None, None
    io_read_cat, io_write_cat = "unknown", "unknown"
    try:
        sample_cache_for_read = None
        # Try to pick a representative cached image path from the first group
        if seestar_stack_groups and seestar_stack_groups[0]:
            sample_cache_for_read = seestar_stack_groups[0][0].get('path_preprocessed_cache')
        if sample_cache_for_read and _path_exists(sample_cache_for_read):
            io_read_mbps = _measure_sequential_read_mbps(sample_cache_for_read)
            io_read_cat = _categorize_io_speed(io_read_mbps)
        # Write speed on output folder
        if output_folder and _path_isdir(output_folder):
            io_write_mbps = _measure_sequential_write_mbps(output_folder)
            io_write_cat = _categorize_io_speed(io_write_mbps)
        pcb(
            f"IO_BENCH: read {io_read_mbps:.1f} MB/s ({io_read_cat}), write {io_write_mbps:.1f} MB/s ({io_write_cat})"
            if (io_read_mbps is not None and io_write_mbps is not None)
            else f"IO_BENCH: read={io_read_mbps}, write={io_write_mbps}"
            ,
            prog=None,
            lvl="DEBUG",
        )
    except Exception as e_io_bench:
        pcb(f"IO_BENCH: failed ({e_io_bench})", prog=None, lvl="WARN")

    # Derive conservative caps from read speed (dominant in Phase 3) on Windows/slow disks
    io_ph3_cap = None
    io_cache_read_slots = None
    new_winsor_limit = winsor_worker_limit
    if os.name == 'nt':
        if io_read_cat == "very_slow":
            io_ph3_cap = 1
            io_cache_read_slots = 1
            new_winsor_limit = min(new_winsor_limit, 1)
        elif io_read_cat == "slow":
            io_ph3_cap = 2
            io_cache_read_slots = 1
            new_winsor_limit = min(new_winsor_limit, 1)
        elif io_read_cat == "medium":
            io_ph3_cap = 3
            io_cache_read_slots = 2
            new_winsor_limit = min(new_winsor_limit, 2)
        elif io_read_cat == "fast":
            io_ph3_cap = 4
            io_cache_read_slots = 2
            # Keep winsor limit as computed
        # Apply winsor limit adjustment if changed
        if new_winsor_limit != winsor_worker_limit:
            pcb(
                f"IO_ADAPT: winsor_worker_limit reduced {winsor_worker_limit} -> {new_winsor_limit} due to IO ({io_read_cat})",
                prog=None,
                lvl="INFO_DETAIL",
            )
            winsor_worker_limit = new_winsor_limit
        # Adjust cache IO semaphore (controls concurrent npy reads)
        try:
            if io_cache_read_slots and io_cache_read_slots > 0:
                global _CACHE_IO_SEMAPHORE
                _CACHE_IO_SEMAPHORE = threading.Semaphore(int(io_cache_read_slots))
                pcb(
                    f"IO_ADAPT: cache read slots set to {io_cache_read_slots}",
                    prog=None,
                    lvl="INFO_DETAIL",
                )
        except Exception:
            pass


    final_output_wcs = None
    final_output_shape_hw = None
    final_mosaic_data_HWC = None
    final_mosaic_coverage_HW = None
    final_alpha_map = None
    sds_fallback_logged = False
    alpha_final: np.ndarray | None = None
    master_tiles_results_list: list[tuple[str, Any]] = []
    final_quality_pipeline_cfg = {
        "quality_crop_enabled": bool(quality_crop_enabled_config),
        "quality_crop_band_px": int(quality_crop_band_px_config),
        "quality_crop_k_sigma": float(quality_crop_k_sigma_config),
        "quality_crop_margin_px": int(quality_crop_margin_px_config),
        "quality_crop_min_run": int(quality_crop_min_run_config),
        "altaz_cleanup_enabled": bool(altaz_cleanup_enabled_config),
        "altaz_margin_percent": float(altaz_margin_percent_config),
        "altaz_decay": float(altaz_decay_config),
        "altaz_nanize": bool(altaz_nanize_config),
    }

    global_anchor_shift: tuple[float, float] | None = None
    sds_runtime_tile_dir: str | None = None

    def _build_phase45_options_dict(base_progress: float) -> dict[str, Any]:
        stack_cfg_phase45 = {
            "kappa_low": float(stack_kappa_low),
            "kappa_high": float(stack_kappa_high),
            "winsor_limits": parsed_winsor_limits,
            "winsor_max_frames_per_pass": winsor_max_frames_per_pass_config,
            "winsor_worker_limit": winsor_worker_limit_config,
            "normalize_method": stack_norm_method,
            "stacking_normalize_method": stack_norm_method,
            "weight_method": stack_weight_method,
            "reject_algo": stack_reject_algo,
            "final_combine": stack_final_combine,
            "stack_norm_method": stack_norm_method,
            "stack_weight_method": stack_weight_method,
            "stack_reject_algo": stack_reject_algo,
            "stack_final_combine": stack_final_combine,
            "intertile_sky_percentile": intertile_sky_percentile_tuple,
            "quality_crop_enabled": bool(quality_crop_enabled_config),
            "quality_crop_band_px": int(quality_crop_band_px_config),
            "quality_crop_k_sigma": float(quality_crop_k_sigma_config),
            "quality_crop_margin_px": int(quality_crop_margin_px_config),
            "quality_crop_min_run": int(quality_crop_min_run_config),
            "altaz_cleanup_enabled": bool(altaz_cleanup_enabled_config),
            "altaz_margin_percent": float(altaz_margin_percent_config),
            "altaz_decay": float(altaz_decay_config),
            "altaz_nanize": bool(altaz_nanize_config),
        }
        return {
            "enable": bool(inter_master_merge_enable_config),
            "base_progress": base_progress,
            "progress_weight": PROGRESS_WEIGHT_PHASE4_5_INTER_MASTER,
            "overlap_threshold": inter_master_overlap_threshold_config,
            "min_group_size": inter_master_min_group_size_config,
            "stack_method": inter_master_stack_method_config,
            "memmap_policy": inter_master_memmap_policy_config,
            "local_scale": inter_master_local_scale_config,
            "max_group_size": inter_master_max_group_config,
            "worker_config": worker_config_cache,
            "stack_cfg": stack_cfg_phase45,
        }

    def _build_phase5_options_dict(base_progress: float, *, final_method: str | None = None) -> dict[str, Any]:
        current_parallel_plan = getattr(zconfig, "parallel_plan", worker_config_cache.get("parallel_plan"))
        target_method = final_method or final_assembly_method_config
        tile_weighting_allowed = bool(tile_weighting_enabled_config)
        if str(target_method or "").lower().strip() != "reproject_coadd":
            tile_weighting_allowed = False
        caps_candidate = None
        try:
            caps_candidate = getattr(zconfig, "parallel_capabilities")
        except Exception:
            caps_candidate = None
        if caps_candidate is None:
            try:
                caps_candidate = worker_config_cache.get("parallel_capabilities")
            except Exception:
                caps_candidate = None
        return {
            "base_progress": base_progress,
            "progress_weight": PROGRESS_WEIGHT_PHASE5_ASSEMBLY,
            "final_assembly_method": target_method,
            "apply_master_tile_crop": apply_master_tile_crop_config,
            "quality_crop_enabled": quality_crop_enabled_config,
            "master_tile_crop_percent": master_tile_crop_percent_config,
            "intertile_match_flag": intertile_match_flag,
            "match_background_flag": match_background_flag,
            "feather_parity_flag": feather_parity_flag,
            "two_pass_enabled": two_pass_enabled,
            "two_pass_sigma_px": two_pass_sigma_px,
            "two_pass_gain_clip": gain_clip_tuple,
            "two_pass_coverage_renorm": two_pass_coverage_renorm_config,
            "use_gpu_phase5": use_gpu_phase5_flag,
            "assembly_process_workers": assembly_process_workers_config,
            "intertile_preview_size": intertile_preview_size_config,
            "intertile_overlap_min": intertile_overlap_min_config,
            "intertile_sky_percentile": intertile_sky_percentile_tuple,
            "intertile_robust_clip_sigma": intertile_robust_clip_sigma_config,
            "intertile_global_recenter": intertile_global_recenter_config,
            "intertile_recenter_clip": intertile_recenter_clip_tuple,
            "use_auto_intertile": use_auto_intertile_config,
            "coadd_use_memmap": coadd_use_memmap_config,
            "coadd_memmap_dir": coadd_memmap_dir_config,
            "global_anchor_shift": global_anchor_shift,
            "parallel_plan": current_parallel_plan,
            "parallel_capabilities": caps_candidate,
            "telemetry": telemetry,
            "sds_mode": bool(sds_mode_flag),
            "tile_weighting_enabled": tile_weighting_allowed,
            "tile_weight_mode": tile_weight_mode_config,
        }

    def _ensure_plan_descriptor_loaded(plan: dict[str, Any]) -> None:
        if not plan.get("enabled"):
            return
        if plan.get("wcs") is not None and plan.get("width") and plan.get("height"):
            return
        fits_path = plan.get("fits_path")
        json_path = plan.get("json_path")
        descriptor_refresh = _load_global_wcs_descriptor_safe(fits_path, json_path)
        if descriptor_refresh:
            plan["descriptor"] = descriptor_refresh
            plan["wcs"] = descriptor_refresh.get("wcs")
            plan["width"] = descriptor_refresh.get("width")
            plan["height"] = descriptor_refresh.get("height")
            if not plan.get("meta"):
                meta_payload = descriptor_refresh.get("metadata")
                if isinstance(meta_payload, dict):
                    plan["meta"] = _coerce_to_builtin(meta_payload)

    def _plan_has_descriptor_fields(plan: dict[str, Any]) -> bool:
        if not plan.get("enabled"):
            return False
        if plan.get("wcs") is None:
            return False
        try:
            width_ok = int(plan.get("width") or 0) > 0
            height_ok = int(plan.get("height") or 0) > 0
        except Exception:
            width_ok = height_ok = False
        return width_ok and height_ok

    def _disable_invalid_plan(reason: str, *, emit_warn: bool) -> None:
        if not global_wcs_plan.get("enabled"):
            return
        if emit_warn:
            pcb("sds_warn_runtime_wcs_failed", prog=None, lvl="WARN", error=reason)
        global_wcs_plan["enabled"] = False
        global_wcs_plan["wcs"] = None
        global_wcs_plan["width"] = None
        global_wcs_plan["height"] = None

    sds_post_context: dict[str, Any] = {}
    plan_mode_value = str(global_wcs_plan.get("mode") or "").strip().lower()
    need_global_plan = bool(plan_mode_value == "seestar")

    _ensure_plan_descriptor_loaded(global_wcs_plan)
    if global_wcs_plan.get("enabled") and not _plan_has_descriptor_fields(global_wcs_plan):
        _disable_invalid_plan("global WCS descriptor missing metadata", emit_warn=(sds_mode_flag or need_global_plan))

    if (sds_mode_flag or need_global_plan) and not global_wcs_plan.get("enabled"):
        built = _runtime_build_global_wcs_plan(
            seestar_groups=seestar_stack_groups,
            global_plan=global_wcs_plan,
            worker_config=worker_config_cache,
            output_dir=output_folder,
            pcb=pcb,
        )
        if built:
            _ensure_plan_descriptor_loaded(global_wcs_plan)
        if global_wcs_plan.get("enabled") and not _plan_has_descriptor_fields(global_wcs_plan):
            _disable_invalid_plan("runtime WCS descriptor incomplete", emit_warn=True)

    plan_width: int | None = None
    plan_height: int | None = None
    if global_wcs_plan.get("enabled"):
        try:
            plan_width = int(global_wcs_plan.get("width") or 0)
            plan_height = int(global_wcs_plan.get("height") or 0)
        except Exception:
            plan_width = plan_height = None
        if not sds_mode_flag:
            # SDS is OFF: skip mega-tile flows and force the classic master-tile pipeline.
            final_mosaic_data_HWC = None
            final_mosaic_coverage_HW = None
            final_alpha_map = None
            pcb("sds_off_classic_mastertile_pipeline", prog=None, lvl="INFO")
        else:
            pcb("sds_on_mega_tile_pipeline", prog=None, lvl="INFO")
            (
                sds_mosaic_data_HWC,
                sds_coverage_HW,
                sds_alpha_map,
            ) = assemble_global_mosaic_sds(
                seestar_stack_groups,
                global_plan=global_wcs_plan,
                progress_callback=progress_callback,
                match_background=match_background_flag,
                base_progress_phase=base_progress_phase2 + PROGRESS_WEIGHT_PHASE2_CLUSTERING,
                progress_weight_phase=(
                    PROGRESS_WEIGHT_PHASE3_MASTER_TILES
                    + PROGRESS_WEIGHT_PHASE4_GRID_CALC
                    + PROGRESS_WEIGHT_PHASE4_5_INTER_MASTER
                    + PROGRESS_WEIGHT_PHASE5_ASSEMBLY
                ),
                start_time_total_run=start_time_total_run,
                cache_root=output_folder,
                stack_params=sds_stack_params,
                coverage_threshold=sds_coverage_threshold_config,
                min_batch_size=sds_min_batch_size_config,
                target_batch_size=sds_target_batch_size_config,
                preplan_path_groups=preplan_groups_override_paths,
                postprocess_context=sds_post_context,
                parallel_plan=getattr(zconfig, "parallel_plan", worker_config_cache.get("parallel_plan")),
            )
            sds_tile_records = list(sds_post_context.pop("sds_tile_records", []) or [])
            temp_dir_candidate = sds_post_context.pop("sds_tile_temp_dir", None)
            if temp_dir_candidate:
                sds_runtime_tile_dir = temp_dir_candidate
            sds_two_pass_tile_pairs = sds_post_context.pop("two_pass_tile_pairs", None)

            sds_polish_succeeded = False
            if sds_mosaic_data_HWC is not None:
                final_output_wcs = global_wcs_plan.get("wcs")
                target_hw = (
                    (sds_mosaic_data_HWC.shape[0], sds_mosaic_data_HWC.shape[1])
                    if hasattr(sds_mosaic_data_HWC, "shape")
                    else None
                )
                pcb("phase5_sds_polish_start", prog=None, lvl="INFO")
                if logger:
                    logger.info("[SDS] Phase 5 polish on global SDS mosaic (skipping master tiles)")
                final_mosaic_data_HWC, final_mosaic_coverage_HW, alpha_final = _finalize_sds_global_mosaic(
                    sds_mosaic_data_HWC,
                    sds_coverage_HW,
                    zconfig=zconfig,
                    pcb=pcb,
                    sds_config={"min_coverage_keep": sds_min_coverage_keep_config},
                    collected_tiles=sds_two_pass_tile_pairs,
                    final_output_wcs=final_output_wcs,
                    final_output_shape_hw=target_hw,
                    pipeline_cfg=final_quality_pipeline_cfg,
                    enable_lecropper_pipeline=bool(
                        final_quality_pipeline_cfg.get("quality_crop_enabled")
                        or final_quality_pipeline_cfg.get("altaz_cleanup_enabled")
                    ),
                    enable_master_tile_crop=bool(apply_master_tile_crop_config and not quality_crop_enabled_config),
                    master_tile_crop_percent=float(master_tile_crop_percent_config),
                    two_pass_enabled=bool(two_pass_enabled),
                    two_pass_sigma_px=int(two_pass_sigma_px),
                    two_pass_gain_clip=gain_clip_tuple,
                    use_gpu_two_pass=use_gpu_phase5_flag,
                    enable_autocrop=bool(global_wcs_autocrop_enabled_config),
                    autocrop_margin_px=global_wcs_autocrop_margin_px_config,
                    global_plan=global_wcs_plan,
                    fallback_two_pass_loader=None,
                    parallel_plan=getattr(zconfig, "parallel_plan", worker_config_cache.get("parallel_plan")),
                )
                final_alpha_map = alpha_final
                if final_mosaic_data_HWC is not None:
                    final_output_shape_hw = (final_mosaic_data_HWC.shape[0], final_mosaic_data_HWC.shape[1])
                    final_output_wcs = global_wcs_plan.get("wcs")
                    current_global_progress = min(
                        100.0,
                        base_progress_phase2
                        + PROGRESS_WEIGHT_PHASE2_CLUSTERING
                        + PROGRESS_WEIGHT_PHASE3_MASTER_TILES
                        + PROGRESS_WEIGHT_PHASE4_GRID_CALC
                        + PROGRESS_WEIGHT_PHASE4_5_INTER_MASTER
                        + PROGRESS_WEIGHT_PHASE5_ASSEMBLY,
                    )
                    seestar_stack_groups = []
                    sds_polish_succeeded = True
                    pcb("sds_global_finalize_done", prog=None, lvl="INFO", has_alpha=alpha_final is not None)
                else:
                    final_mosaic_coverage_HW = None
                    final_alpha_map = None
            if not sds_polish_succeeded:
                if not sds_tile_records:
                    pcb("sds_failed_fallback_mosaic_first", prog=None, lvl="WARN")
                    mosaic_result = assemble_global_mosaic_first(
                        seestar_stack_groups,
                        global_plan=global_wcs_plan,
                        progress_callback=progress_callback,
                        match_background=match_background_flag,
                        base_progress_phase=base_progress_phase2 + PROGRESS_WEIGHT_PHASE2_CLUSTERING,
                        progress_weight_phase=(
                            PROGRESS_WEIGHT_PHASE3_MASTER_TILES
                            + PROGRESS_WEIGHT_PHASE4_GRID_CALC
                            + PROGRESS_WEIGHT_PHASE4_5_INTER_MASTER
                            + PROGRESS_WEIGHT_PHASE5_ASSEMBLY
                        ),
                        start_time_total_run=start_time_total_run,
                        cache_root=output_folder,
                        parallel_plan=getattr(zconfig, "parallel_plan", worker_config_cache.get("parallel_plan")),
                    ) or (None, None, None)
                    final_mosaic_data_HWC, final_mosaic_coverage_HW, final_alpha_map = mosaic_result
                    if final_mosaic_data_HWC is not None:
                        final_output_wcs = global_wcs_plan.get("wcs")
                        target_hw = (
                            (final_mosaic_data_HWC.shape[0], final_mosaic_data_HWC.shape[1])
                            if hasattr(final_mosaic_data_HWC, "shape")
                            else None
                        )
                        final_mosaic_data_HWC, final_mosaic_coverage_HW, alpha_final = _finalize_sds_global_mosaic(
                            final_mosaic_data_HWC,
                            final_mosaic_coverage_HW,
                            zconfig=zconfig,
                            pcb=pcb,
                            sds_config={"min_coverage_keep": sds_min_coverage_keep_config},
                            collected_tiles=None,
                            final_output_wcs=final_output_wcs,
                            final_output_shape_hw=target_hw,
                            pipeline_cfg=final_quality_pipeline_cfg,
                            enable_lecropper_pipeline=bool(
                                final_quality_pipeline_cfg.get("quality_crop_enabled")
                                or final_quality_pipeline_cfg.get("altaz_cleanup_enabled")
                            ),
                            enable_master_tile_crop=bool(apply_master_tile_crop_config and not quality_crop_enabled_config),
                            master_tile_crop_percent=float(master_tile_crop_percent_config),
                            two_pass_enabled=bool(two_pass_enabled),
                            two_pass_sigma_px=int(two_pass_sigma_px),
                            two_pass_gain_clip=gain_clip_tuple,
                            use_gpu_two_pass=use_gpu_phase5_flag,
                            enable_autocrop=bool(global_wcs_autocrop_enabled_config),
                            autocrop_margin_px=global_wcs_autocrop_margin_px_config,
                            global_plan=global_wcs_plan,
                            fallback_two_pass_loader=None,
                            parallel_plan=getattr(zconfig, "parallel_plan", worker_config_cache.get("parallel_plan")),
                        )
                        final_alpha_map = alpha_final
                        if final_mosaic_data_HWC is not None:
                            final_output_wcs = global_wcs_plan.get("wcs")
                            final_output_shape_hw = (
                                final_mosaic_data_HWC.shape[0],
                                final_mosaic_data_HWC.shape[1],
                            )
                            current_global_progress = min(
                                100.0,
                                base_progress_phase2
                                + PROGRESS_WEIGHT_PHASE2_CLUSTERING
                                + PROGRESS_WEIGHT_PHASE3_MASTER_TILES
                                + PROGRESS_WEIGHT_PHASE4_GRID_CALC
                                + PROGRESS_WEIGHT_PHASE4_5_INTER_MASTER
                                + PROGRESS_WEIGHT_PHASE5_ASSEMBLY,
                            )
                            seestar_stack_groups = []
                            sds_polish_succeeded = True
                            pcb("sds_global_finalize_done", prog=None, lvl="INFO", has_alpha=alpha_final is not None)
                    if not sds_polish_succeeded:
                        if not sds_fallback_logged:
                            pcb("sds_and_mosaic_first_failed_fallback_mastertiles", prog=None, lvl="WARN")
                            sds_fallback_logged = True
                        pcb("global_coadd_error_failed_fallback", prog=None, lvl="WARN")
                        global_wcs_plan["enabled"] = False
                else:
                    target_height = plan_height if isinstance(plan_height, int) and plan_height > 0 else None
                    target_width = plan_width if isinstance(plan_width, int) and plan_width > 0 else None
                    if (target_height is None or target_width is None) and sds_tile_records:
                        probe_path = sds_tile_records[0][0]
                        try:
                            with fits.open(probe_path, memmap=False) as hdul_probe:
                                data_shape = hdul_probe[0].shape if hdul_probe and hdul_probe[0] is not None else None
                                if data_shape and len(data_shape) >= 2:
                                    target_height = int(data_shape[0])
                                    target_width = int(data_shape[1])
                        except Exception:
                            pass
                    target_shape_hw = None
                    if target_height and target_width:
                        target_shape_hw = (target_height, target_width)
                    phase45_base = (
                        base_progress_phase2
                        + PROGRESS_WEIGHT_PHASE2_CLUSTERING
                        + PROGRESS_WEIGHT_PHASE3_MASTER_TILES
                        + PROGRESS_WEIGHT_PHASE4_GRID_CALC
                    )
                    sds_phase45_options = _build_phase45_options_dict(phase45_base)
                    sds_phase5_options = _build_phase5_options_dict(
                        phase45_base + PROGRESS_WEIGHT_PHASE4_5_INTER_MASTER
                    )
                    telemetry.maybe_emit_stats(
                        _telemetry_context(
                            {
                                "phase_name": "Phase 5: Assembly",
                                "phase_index": 5,
                                "tiles_total": len(sds_tile_records),
                            }
                        )
                    )
                    (
                        master_tiles_results_list,
                        final_mosaic_data_HWC,
                        final_mosaic_coverage_HW,
                        final_alpha_map,
                        alpha_final,
                        current_global_progress,
                    ) = _run_shared_phase45_phase5_pipeline(
                        sds_tile_records,
                        final_output_wcs=global_wcs_plan.get("wcs"),
                        final_output_shape_hw=target_shape_hw,
                        temp_master_tile_storage_dir=sds_runtime_tile_dir,
                        output_folder=output_folder,
                        cache_retention_mode=cache_retention_mode,
                        phase45_options=sds_phase45_options,
                        phase5_options=sds_phase5_options,
                        final_quality_pipeline_cfg=final_quality_pipeline_cfg,
                        start_time_total_run=start_time_total_run,
                        progress_callback=progress_callback,
                        pcb=pcb,
                        logger=logger,
                    )
                    master_tiles_results_list = list(sds_tile_records)
                    autocrop_meta = None
                    if final_mosaic_data_HWC is not None and global_wcs_autocrop_enabled_config:
                        final_mosaic_data_HWC, final_mosaic_coverage_HW, final_alpha_map, autocrop_meta = (
                            _auto_crop_global_mosaic_if_requested(
                                final_mosaic_data_HWC,
                                final_mosaic_coverage_HW,
                                final_alpha_map,
                                enable_autocrop=True,
                                margin_px=global_wcs_autocrop_margin_px_config,
                                pcb=pcb,
                            )
                        )
                        if autocrop_meta:
                            _apply_autocrop_to_global_plan(global_wcs_plan, autocrop_meta)
                    if final_mosaic_data_HWC is not None:
                        final_output_wcs = global_wcs_plan.get("wcs")
                        final_output_shape_hw = (final_mosaic_data_HWC.shape[0], final_mosaic_data_HWC.shape[1])
                        alpha_final = _derive_final_alpha_mask(
                            final_alpha_map,
                            final_mosaic_data_HWC,
                            final_mosaic_coverage_HW,
                            logger,
                        )
                        seestar_stack_groups = []
                    else:
                        if not sds_fallback_logged:
                            pcb("sds_and_mosaic_first_failed_fallback_mastertiles", prog=None, lvl="WARN")
                            sds_fallback_logged = True
                        pcb("global_coadd_error_failed_fallback", prog=None, lvl="WARN")
                        global_wcs_plan["enabled"] = False

    try:
        setattr(zconfig, "winsor_worker_limit", int(winsor_worker_limit))
    except Exception:
        pass
    try:
        setattr(zconfig, "winsor_max_frames_per_pass", int(winsor_max_frames_per_pass))
    except Exception:
        pass



    if final_mosaic_data_HWC is None:
            if sds_mode_flag and not sds_fallback_logged:
                pcb("sds_and_mosaic_first_failed_fallback_mastertiles", prog=None, lvl="WARN")
                sds_fallback_logged = True
            # --- Phase 3 (Création Master Tuiles) ---
            base_progress_phase3 = current_global_progress
            _log_memory_usage(progress_callback, "Début Phase 3 (Master Tuiles)")
            pcb("run_info_phase3_started_from_cache", prog=base_progress_phase3, lvl="INFO")
            pcb("PHASE_UPDATE:3", prog=None, lvl="ETA_LEVEL")
            telemetry.maybe_emit_stats(
                _telemetry_context(
                    {
                        "phase_name": "Phase 3: Master Tiles",
                        "phase_index": 3,
                        "tiles_total": num_seestar_stacks_to_process,
                    }
                )
            )
            temp_master_tile_storage_dir = str(Path(output_folder).expanduser() / "zemosaic_temp_master_tiles")
            try:
                if _path_exists(temp_master_tile_storage_dir): shutil.rmtree(temp_master_tile_storage_dir)
                os.makedirs(temp_master_tile_storage_dir, exist_ok=True)
            except OSError as e_mkdir_mt: 
                pcb("run_error_phase3_mkdir_failed", prog=current_global_progress, lvl="ERROR", directory=temp_master_tile_storage_dir, error=str(e_mkdir_mt)); return
                
            master_tiles_results_list_temp = {}
            start_time_phase3 = time.monotonic()

            tile_id_order = list(range(len(seestar_stack_groups)))
            center_out_context: CenterOutNormalizationContext | None = None
            global_anchor_shift = (1.0, 0.0)
            prestack_anchor_tile_id: int | None = None
            center_out_settings = {
                "enabled": bool(center_out_normalization_p3_config),
                "sky_percentile": tuple((p3_center_sky_percentile_config or (25.0, 60.0))[:2]) if isinstance(p3_center_sky_percentile_config, (list, tuple)) else (25.0, 60.0),
                "clip_sigma": float(p3_center_robust_clip_sigma_config),
                "preview_size": int(p3_center_preview_size_config),
                "min_overlap_fraction": float(p3_center_min_overlap_fraction_config),
            }
            anchor_mode_value = str(center_out_anchor_mode_config or "auto_central_quality").strip()
            anchor_mode_lower = anchor_mode_value.lower()
            anchor_quality_settings = {
                "probe_limit": anchor_quality_probe_limit_config,
                "span_range": anchor_quality_span_range_config,
                "median_clip_sigma": anchor_quality_median_clip_sigma_config,
            }
            try:
                anchor_crop_band = max(4, int(quality_crop_band_px_config))
            except Exception:
                anchor_crop_band = 32
            try:
                anchor_crop_margin = max(0, int(quality_crop_margin_px_config))
            except Exception:
                anchor_crop_margin = 8
            try:
                anchor_crop_sigma = float(quality_crop_k_sigma_config)
                if not math.isfinite(anchor_crop_sigma):
                    raise ValueError
            except Exception:
                anchor_crop_sigma = 2.0
            anchor_crop_sigma = max(0.1, min(anchor_crop_sigma, 10.0))
            anchor_crop_settings = {
                "enabled": bool(ANCHOR_AUTOCROP_AVAILABLE and anchor_mode_lower == "auto_central_quality"),
                "band_px": anchor_crop_band,
                "margin_px": anchor_crop_margin,
                "k_sigma": anchor_crop_sigma,
            }
            if center_out_settings["enabled"] and seestar_stack_groups:
                order_info = _compute_center_out_order(seestar_stack_groups)
                distances = {}
                global_center_coord = None
                if order_info:
                    ordered_indices, global_center_coord, distances = order_info
                    try:
                        seestar_stack_groups = [seestar_stack_groups[i] for i in ordered_indices]
                        tile_id_order = ordered_indices
                    except Exception:
                        tile_id_order = list(range(len(seestar_stack_groups)))
                else:
                    distances = {}
                    global_center_coord = None

                anchor_original_id: int | None = tile_id_order[0] if tile_id_order else None
                if tile_id_order and anchor_mode_lower == "auto_central_quality":
                    selected_anchor = _select_quality_anchor(
                        tile_id_order,
                        distances,
                        seestar_stack_groups,
                        anchor_quality_settings,
                        center_out_settings,
                        anchor_crop_settings,
                        progress_callback,
                    )
                    if selected_anchor is not None and selected_anchor in tile_id_order:
                        if selected_anchor != tile_id_order[0]:
                            try:
                                sel_index = tile_id_order.index(selected_anchor)
                                tile_id_order.insert(0, tile_id_order.pop(sel_index))
                                seestar_stack_groups.insert(0, seestar_stack_groups.pop(sel_index))
                            except Exception:
                                pass
                        anchor_original_id = int(selected_anchor)
                    else:
                        _log_and_callback(
                            "center_anchor_fallback_central_only",
                            lvl="WARN",
                            callback=progress_callback,
                        )
                elif tile_id_order:
                    anchor_original_id = int(tile_id_order[0])

                try:
                    pcb(
                        "phase3_center_out_plan",
                        prog=None,
                        lvl="INFO_DETAIL",
                        anchor=int(anchor_original_id) if anchor_original_id is not None else None,
                        center_ra=f"{global_center_coord.ra.deg:.6f}" if global_center_coord else None,
                        center_dec=f"{global_center_coord.dec.deg:.6f}" if global_center_coord else None,
                    )
                except Exception:
                    pass

                if tile_id_order:
                    anchor_id_final = int(anchor_original_id) if anchor_original_id is not None else int(tile_id_order[0])
                    center_out_context = CenterOutNormalizationContext(
                        anchor_tile_original_id=anchor_id_final,
                        ordered_tile_ids=tile_id_order,
                        tile_distances=distances,
                        settings=center_out_settings,
                        global_center=global_center_coord,
                        logger_instance=logger,
                    )
                    prestack_anchor_tile_id = anchor_id_final
            else:
                center_out_settings["enabled"] = False
            
            # Calcul des workers pour la Phase 3 (alignement/stacking des groupes)
            actual_num_workers_ph3 = _compute_phase_workers(
                effective_base_workers,
                num_seestar_stacks_to_process,
                ALIGNMENT_PHASE_WORKER_RATIO,
            )
            # Boost Phase 3 toward ~90% of logical cores while leaving one core free
            try:
                target_by_cpu = max(
                    1,
                    min(
                        num_logical_processors - 1,
                        int(math.ceil(num_logical_processors * 0.9)),
                    ),
                )
            except Exception:
                target_by_cpu = max(1, num_logical_processors - 1)
            target_ph3_workers = max(1, min(num_seestar_stacks_to_process, target_by_cpu))
            if target_ph3_workers > actual_num_workers_ph3:
                prev_workers = actual_num_workers_ph3
                actual_num_workers_ph3 = target_ph3_workers
                try:
                    pcb(
                        f"WORKERS_PHASE3: Boost {prev_workers} -> {actual_num_workers_ph3} (90% cores target, keeping one free)",
                        prog=None,
                        lvl="INFO_DETAIL",
                    )
                except Exception:
                    pass
            if auto_caps_info:
                try:
                    parallel_cap = int(auto_caps_info.get("parallel_groups", 0))
                except Exception:
                    parallel_cap = 0
                # Only apply auto-cap when it allows more than one worker; when
                # heuristics return 1 (e.g., memmap-enabled or missing probes),
                # let downstream RAM/IO guards decide to avoid pinning Phase 3
                # to a single worker.
                if parallel_cap > 1:
                    prev_workers = actual_num_workers_ph3
                    actual_num_workers_ph3 = max(1, min(actual_num_workers_ph3, parallel_cap))
                    if actual_num_workers_ph3 != prev_workers:
                        try:
                            _log_and_callback(
                                f"AutoCaps: Phase 3 worker cap {prev_workers} -> {actual_num_workers_ph3} (parallel limit)",
                                prog=None,
                                lvl="INFO_DETAIL",
                                callback=progress_callback,
                            )
                        except Exception:
                            pass
            # On Windows, still respect the reserve-one-core rule but avoid hard-capping at 4
            if os.name == 'nt':
                actual_num_workers_ph3 = max(1, min(actual_num_workers_ph3, max(1, num_logical_processors - 1)))
            # Apply IO-based cap if available
            try:
                if io_ph3_cap is not None:
                    prev_workers = actual_num_workers_ph3
                    actual_num_workers_ph3 = max(1, min(actual_num_workers_ph3, int(io_ph3_cap)))
                    if actual_num_workers_ph3 != prev_workers:
                        pcb(
                            f"IO_ADAPT: Phase 3 workers {prev_workers} -> {actual_num_workers_ph3} due to IO ({io_read_cat})",
                            prog=None,
                            lvl="INFO_DETAIL",
                        )
            except Exception:
                pass
            # RAM-aware cap for Phase 3: estimate per-job footprint and clamp concurrency
            try:
                avail_bytes = int(psutil.virtual_memory().available)
                # Determine per-frame bytes via a sample cached image when possible
                per_frame_bytes = None
                try:
                    sample_cache = None
                    if seestar_stack_groups and seestar_stack_groups[0]:
                        sample_cache = seestar_stack_groups[0][0].get('path_preprocessed_cache')
                    if sample_cache and _path_exists(sample_cache):
                        _arr = np.load(sample_cache, mmap_mode='r')
                        per_frame_bytes = int(_arr.size) * int(_arr.dtype.itemsize)
                        _arr = None
                except Exception:
                    per_frame_bytes = None
                if per_frame_bytes is None or per_frame_bytes <= 0:
                    # Conservative default (Seestar 1080x1920 RGB float32)
                    per_frame_bytes = 1080 * 1920 * 3 * 4
                frames_per_pass_cfg = int(winsor_max_frames_per_pass) if winsor_max_frames_per_pass and int(winsor_max_frames_per_pass) > 0 else 256
                try:
                    max_frames_in_group = max(len(g) for g in seestar_stack_groups if g) if seestar_stack_groups else 1
                except Exception:
                    max_frames_in_group = 1
                frames_per_pass = max(1, min(frames_per_pass_cfg, max_frames_in_group))
                fudge = 2.0
                per_job_bytes = int(max(1, frames_per_pass) * per_frame_bytes * fudge)
                allowed = int(avail_bytes * 0.6)
                max_by_ram = max(1, allowed // max(1, per_job_bytes))
                prev_workers = actual_num_workers_ph3
                actual_num_workers_ph3 = max(1, min(actual_num_workers_ph3, int(max_by_ram)))
                if actual_num_workers_ph3 != prev_workers:
                    try:
                        mb_per_job = per_job_bytes / (1024.0 * 1024.0)
                        pcb(
                            f"RAM_CAP: Phase 3 workers {prev_workers} -> {actual_num_workers_ph3} (frames/pass={frames_per_pass}, per-job~{mb_per_job:.1f}MB)",
                            prog=None,
                            lvl="INFO_DETAIL",
                        )
                    except Exception:
                        pass
            except Exception:
                pass
            pcb(
                f"WORKERS_PHASE3: Utilisation de {actual_num_workers_ph3} worker(s). (Base: {effective_base_workers}, Ratio {ALIGNMENT_PHASE_WORKER_RATIO*100:.0f}%, Groupes: {num_seestar_stacks_to_process})",
                prog=None,
                lvl="INFO",
            )  # Log mis à jour pour clarté

            # Initialize adaptive concurrency controls for Phase 3 (I/O + tasks)
            try:
                global _PH3_CONCURRENCY_SEMAPHORE
                _PH3_CONCURRENCY_SEMAPHORE = threading.Semaphore(int(actual_num_workers_ph3))
            except Exception:
                pass

            # Start a lightweight real-time monitor to adapt concurrency while Phase 3 runs
            monitor_stop_evt = threading.Event()

            def _rt_adapt_concurrency():
                try:
                    import psutil as _ps
                except Exception:
                    return  # psutil absent; skip runtime adaptation
                current_ph3_limit = int(actual_num_workers_ph3)
                current_cache_slots = None
                default_cache_slots = 2 if os.name == 'nt' else 3
                last_io = None
                last_t = None
                try:
                    last_io = _ps.disk_io_counters()
                    last_t = time.perf_counter()
                except Exception:
                    last_io, last_t = None, None
                while not monitor_stop_evt.is_set():
                    time.sleep(1.25)
                    # CPU snapshot
                    try:
                        cpu_pct = _ps.cpu_percent(interval=None)
                    except Exception:
                        cpu_pct = None
                    # Disk read throughput MB/s
                    read_mbps = None
                    try:
                        if last_io is not None:
                            now_io = _ps.disk_io_counters()
                            now_t = time.perf_counter()
                            dt = max(1e-3, (now_t - (last_t or now_t)))
                            read_mbps = (max(0, now_io.read_bytes - last_io.read_bytes) / dt) / (1024 * 1024)
                            last_io, last_t = now_io, now_t
                    except Exception:
                        pass

                    new_ph3_limit = current_ph3_limit
                    new_cache_slots = current_cache_slots if current_cache_slots is not None else default_cache_slots

                    if read_mbps is not None:
                        if os.name == 'nt':
                            if read_mbps >= 120:
                                new_ph3_limit = 1
                                new_cache_slots = 1
                            elif read_mbps >= 80:
                                new_ph3_limit = min(new_ph3_limit, 2)
                                new_cache_slots = 1
                            elif read_mbps >= 40:
                                new_cache_slots = 2
                            else:
                                new_cache_slots = default_cache_slots
                        else:
                            if read_mbps >= 200:
                                new_ph3_limit = max(1, min(new_ph3_limit, 2))
                                new_cache_slots = 2
                            elif read_mbps >= 120:
                                new_cache_slots = 2
                            else:
                                new_cache_slots = default_cache_slots

                    if cpu_pct is not None:
                        if cpu_pct >= 90:
                            new_ph3_limit = max(1, min(new_ph3_limit, 2 if os.name == 'nt' else 3))
                        elif cpu_pct <= 45:
                            new_ph3_limit = max(new_ph3_limit, min(int(actual_num_workers_ph3), 3 if os.name == 'nt' else int(actual_num_workers_ph3)))

                    new_ph3_limit = max(1, min(int(actual_num_workers_ph3), int(new_ph3_limit)))
                    new_cache_slots = max(1, int(new_cache_slots))

                    try:
                        if new_ph3_limit != current_ph3_limit:
                            current_ph3_limit = new_ph3_limit
                            try:
                                global _PH3_CONCURRENCY_SEMAPHORE
                                _PH3_CONCURRENCY_SEMAPHORE = threading.Semaphore(int(current_ph3_limit))
                                pcb(f"IO_ADAPT_RT: ph3_workers -> {current_ph3_limit}", prog=None, lvl="INFO_DETAIL")
                            except Exception:
                                pass
                        if (current_cache_slots is None) or (new_cache_slots != current_cache_slots):
                            current_cache_slots = new_cache_slots
                            try:
                                global _CACHE_IO_SEMAPHORE
                                _CACHE_IO_SEMAPHORE = threading.Semaphore(int(current_cache_slots))
                                pcb(f"IO_ADAPT_RT: cache_read_slots -> {current_cache_slots}", prog=None, lvl="INFO_DETAIL")
                            except Exception:
                                pass
                    except Exception:
                        pass

            monitor_thread = threading.Thread(target=_rt_adapt_concurrency, name="ZeMosaic_Ph3_RTAdapt", daemon=True)
            monitor_thread.start()

            dbg_tile_ids = _select_debug_tile_ids(tile_id_order)

            dbg_tile_ids = _select_debug_tile_ids(tile_id_order)

            tiles_processed_count_ph3 = 0
            # Envoyer l'info initiale avant la boucle
            if num_seestar_stacks_to_process > 0:
                pcb(f"MASTER_TILE_COUNT_UPDATE:{tiles_processed_count_ph3}/{num_seestar_stacks_to_process}", prog=None, lvl="ETA_LEVEL")
            
            executor_ph3 = ThreadPoolExecutor(max_workers=actual_num_workers_ph3, thread_name_prefix="ZeMosaic_Ph3_")

            future_to_tile_id: dict = {}
            tile_input_cache_paths: dict[int, list[str]] = {}
            pending_futures: set = set()
            next_dynamic_tile_id = num_seestar_stacks_to_process

            def _submit_master_tile_group(group_info_list: list[dict], assigned_tile_id: int, processing_rank: int | None = None) -> None:
                future = executor_ph3.submit(
                    create_master_tile,
                    group_info_list,
                    assigned_tile_id,
                    temp_master_tile_storage_dir,
                    stack_norm_method, stack_weight_method, stack_reject_algo,
                    stack_kappa_low, stack_kappa_high, parsed_winsor_limits,
                    stack_final_combine,
                    poststack_equalize_rgb_config,
                    apply_radial_weight_config, radial_feather_fraction_config,
                    radial_shape_power_config, min_radial_weight_floor_config,
                    quality_crop_enabled_config, quality_crop_band_px_config,
                    quality_crop_k_sigma_config, quality_crop_margin_px_config,
                    quality_crop_min_run_config,
                    altaz_cleanup_enabled_config,
                    altaz_margin_percent_config,
                    altaz_decay_config,
                    altaz_nanize_config,
                    quality_gate_enabled_config,
                    quality_gate_threshold_config,
                    quality_gate_edge_band_px_config,
                    quality_gate_k_sigma_config,
                    quality_gate_erode_px_config,
                    quality_gate_move_rejects_config,
                    astap_exe_path, astap_data_dir_param, astap_search_radius_config,
                    astap_downsample_config, astap_sensitivity_config, 180,
                    winsor_worker_limit,
                    winsor_max_frames_per_pass,
                    progress_callback,
                    resource_strategy=auto_resource_strategy,
                    center_out_context=center_out_context,
                    center_out_settings=center_out_settings if center_out_context else None,
                    center_out_rank=processing_rank,
                    parallel_plan=getattr(zconfig, "parallel_plan", worker_config_cache.get("parallel_plan")),
                    dbg_tile_ids=dbg_tile_ids,
                )
                future_to_tile_id[future] = assigned_tile_id
                pending_futures.add(future)
                if cache_retention_mode == "per_tile":
                    cache_paths: list[str] = []
                    for raw_entry in group_info_list or []:
                        if not isinstance(raw_entry, dict):
                            continue
                        cache_path = raw_entry.get('path_preprocessed_cache')
                        if isinstance(cache_path, str):
                            cache_paths.append(cache_path)
                    tile_input_cache_paths[assigned_tile_id] = cache_paths

            for proc_idx, sg_info_list in enumerate(seestar_stack_groups):
                assigned_tile_id = tile_id_order[proc_idx] if proc_idx < len(tile_id_order) else proc_idx
                rank = center_out_context.get_rank(assigned_tile_id) if center_out_context else proc_idx
                _submit_master_tile_group(sg_info_list, assigned_tile_id, rank)

            start_time_loop_ph3 = time.time()
            last_time_loop_ph3 = start_time_loop_ph3
            step_times_ph3 = []

            while pending_futures:
                done_futures, _ = wait(pending_futures, return_when=FIRST_COMPLETED)
                for future in done_futures:
                    pending_futures.discard(future)
                    tile_id_for_future = future_to_tile_id.pop(future, None)
                    if tile_id_for_future is None:
                        continue
                    tiles_processed_count_ph3 += 1
                    cache_paths_for_tile: list[str] = []
                    if cache_retention_mode == "per_tile":
                        cache_paths_for_tile = tile_input_cache_paths.pop(tile_id_for_future, [])

                    pcb(f"MASTER_TILE_COUNT_UPDATE:{tiles_processed_count_ph3}/{num_seestar_stacks_to_process}", prog=None, lvl="ETA_LEVEL")

                    prog_step_phase3 = base_progress_phase3 + int(
                        PROGRESS_WEIGHT_PHASE3_MASTER_TILES * (tiles_processed_count_ph3 / max(1, num_seestar_stacks_to_process))
                    )
                    if progress_callback:
                        try:
                            progress_callback("phase3_master_tiles", tiles_processed_count_ph3, num_seestar_stacks_to_process)
                        except Exception:
                            pass

                    telemetry.maybe_emit_stats(
                        _telemetry_context(
                            {
                                "phase_name": "Phase 3: Master Tiles",
                                "phase_index": 3,
                                "tiles_done": tiles_processed_count_ph3,
                                "tiles_total": num_seestar_stacks_to_process,
                            }
                        )
                    )

                    now = time.time()
                    step_times_ph3.append(now - last_time_loop_ph3)
                    last_time_loop_ph3 = now

                    try:
                        main_result, retry_groups = future.result()
                        mt_result_path, mt_result_wcs = (main_result or (None, None))
                        if mt_result_path and mt_result_wcs:
                            master_tiles_results_list_temp[tile_id_for_future] = (mt_result_path, mt_result_wcs)
                            if cache_retention_mode == "per_tile" and cache_paths_for_tile:
                                removed_count, removed_bytes = _cleanup_per_tile_cache(cache_paths_for_tile)
                                freed_mb = removed_bytes / (1024 * 1024) if removed_bytes else 0.0
                                logger.debug(
                                    "Per-tile cache cleanup for tile %s: removed %d file(s), freed %.3f MiB",
                                    tile_id_for_future,
                                    removed_count,
                                    freed_mb,
                                )
                                try:
                                    pcb(
                                        "run_debug_cache_per_tile_cleanup",
                                        prog=None,
                                        lvl="DEBUG_DETAIL",
                                        tile_id=int(tile_id_for_future),
                                        removed=int(removed_count),
                                        freed_mib=f"{freed_mb:.3f}",
                                    )
                                except Exception:
                                    pass
                        else:
                            pcb(
                                "run_warn_phase3_master_tile_creation_failed_thread",
                                prog=prog_step_phase3,
                                lvl="WARN",
                                stack_num=int(tile_id_for_future) + 1,
                            )
                        if retry_groups:
                            for retry_group in retry_groups:
                                if not retry_group:
                                    continue
                                filtered_retry_group: list[dict] = []
                                dropped_infos: list[dict] = []
                                for raw_info in retry_group:
                                    if isinstance(raw_info, dict):
                                        attempts = int(raw_info.get('retry_attempt', 0))
                                        if attempts > MAX_ALIGNMENT_RETRY_ATTEMPTS:
                                            dropped_infos.append(raw_info)
                                            continue
                                    filtered_retry_group.append(raw_info)
                                for dropped in dropped_infos:
                                    try:
                                        filename = _safe_basename(dropped.get('path_raw', 'UnknownRaw'))
                                    except Exception:
                                        filename = str(dropped)
                                    pcb(
                                        "run_warn_phase3_alignment_retry_abandoned",
                                        prog=None,
                                        lvl="WARN",
                                        tile_id=int(tile_id_for_future),
                                        filename=filename,
                                        attempts=int(dropped.get('retry_attempt', 0)) if isinstance(dropped, dict) else None,
                                    )
                                if not filtered_retry_group:
                                    continue
                                new_tile_id = next_dynamic_tile_id
                                next_dynamic_tile_id += 1
                                num_seestar_stacks_to_process += 1
                                pcb(
                                    "run_info_phase3_retry_submitted",
                                    prog=None,
                                    lvl="INFO_DETAIL",
                                    origin_tile=int(tile_id_for_future),
                                    new_tile=new_tile_id,
                                    frames=len(filtered_retry_group),
                                )
                                retry_rank = center_out_context.get_rank(new_tile_id) if center_out_context else None
                                _submit_master_tile_group(filtered_retry_group, new_tile_id, retry_rank)
                                pcb(
                                    f"MASTER_TILE_COUNT_UPDATE:{tiles_processed_count_ph3}/{num_seestar_stacks_to_process}",
                                    prog=None,
                                    lvl="ETA_LEVEL",
                                )
                                if progress_callback:
                                    try:
                                        progress_callback("phase3_master_tiles", tiles_processed_count_ph3, num_seestar_stacks_to_process)
                                    except Exception:
                                        pass
                    except Exception as exc_thread_ph3:
                        pcb(
                            "run_error_phase3_thread_exception",
                            prog=prog_step_phase3,
                            lvl="ERROR",
                            stack_num=int(tile_id_for_future) + 1,
                            error=str(exc_thread_ph3),
                        )
                        logger.error(f"Exception Phase 3 pour stack {int(tile_id_for_future) + 1}:", exc_info=True)
                    finally:
                        # Aggressively free CuPy memory pools between tiles to avoid device/pinned host growth
                        try:
                            if ZEMOSAIC_UTILS_AVAILABLE and zemosaic_utils and hasattr(zemosaic_utils, "free_cupy_memory_pools"):
                                zemosaic_utils.free_cupy_memory_pools()
                        except Exception:
                            pass
                        try:
                            gc.collect()
                        except Exception:
                            pass

                    if tiles_processed_count_ph3 % max(1, num_seestar_stacks_to_process // 5) == 0 or tiles_processed_count_ph3 == num_seestar_stacks_to_process:
                         _log_memory_usage(progress_callback, f"Phase 3 - Traité {tiles_processed_count_ph3}/{num_seestar_stacks_to_process} tuiles")

                    elapsed_phase3 = time.monotonic() - start_time_phase3
                    time_per_master_tile_creation = elapsed_phase3 / max(1, tiles_processed_count_ph3)
                    eta_phase3_sec = (num_seestar_stacks_to_process - tiles_processed_count_ph3) * time_per_master_tile_creation
                    current_progress_in_run_percent_ph3 = base_progress_phase3 + (tiles_processed_count_ph3 / max(1, num_seestar_stacks_to_process)) * PROGRESS_WEIGHT_PHASE3_MASTER_TILES
                    time_per_percent_point_global_ph3 = (time.monotonic() - start_time_total_run) / max(1, current_progress_in_run_percent_ph3) if current_progress_in_run_percent_ph3 > 0 else (time.monotonic() - start_time_total_run)
                    total_eta_sec_ph3 = eta_phase3_sec + (100 - current_progress_in_run_percent_ph3) * time_per_percent_point_global_ph3
                    update_gui_eta(total_eta_sec_ph3)

            # Toutes les futures sont terminées → fermeture propre
            # Stop the runtime adaptation monitor for Phase 3
            try:
                monitor_stop_evt.set()
                if monitor_thread and monitor_thread.is_alive():
                    monitor_thread.join(timeout=2.0)
            except Exception:
                pass
            executor_ph3.shutdown(wait=True)

            if enable_poststack_anchor_review_config and master_tiles_results_list_temp:
                post_review_cfg = {
                    "probe_limit": poststack_anchor_probe_limit_config,
                    "span_range": poststack_anchor_span_range_config,
                    "median_clip_sigma": poststack_anchor_median_clip_sigma_config,
                    "min_improvement": poststack_anchor_min_improvement_config,
                    "use_overlap_affine": poststack_anchor_use_overlap_affine_config,
                    "sky_percentile": center_out_settings.get("sky_percentile"),
                }
                try:
                    master_tiles_results_list_temp, anchor_shift_candidate = run_poststack_anchor_review(
                        master_tiles_results_list_temp,
                        prestack_anchor_tile_id,
                        post_review_cfg,
                        progress_callback,
                        tile_distances=getattr(center_out_context, "tile_distances", None) if center_out_context else None,
                    )
                    if isinstance(anchor_shift_candidate, tuple):
                        global_anchor_shift = anchor_shift_candidate
                except Exception:
                    logger.warning("Post-stack anchor review failed", exc_info=True)

            master_tiles_results_list = [master_tiles_results_list_temp[i] for i in sorted(master_tiles_results_list_temp.keys())]
            del master_tiles_results_list_temp; gc.collect()
            if not master_tiles_results_list:
                pcb("run_error_phase3_no_master_tiles_created", prog=(base_progress_phase3 + PROGRESS_WEIGHT_PHASE3_MASTER_TILES), lvl="ERROR"); return

            current_global_progress = base_progress_phase3 + PROGRESS_WEIGHT_PHASE3_MASTER_TILES
            _log_memory_usage(progress_callback, "Fin Phase 3");
            if step_times_ph3:
                avg_step = sum(step_times_ph3) / len(step_times_ph3)
                total_elapsed = time.time() - start_time_loop_ph3
                pcb(
                    "phase3_debug_timing",
                    prog=None,
                    lvl="DEBUG_DETAIL",
                    avg=f"{avg_step:.2f}",
                    total=f"{total_elapsed:.2f}",
                )
            pcb("run_info_phase3_finished_from_cache", prog=current_global_progress, lvl="INFO", num_master_tiles=len(master_tiles_results_list))
            
            # Assurer que le compteur final est bien affiché (au cas où la dernière itération n'aurait pas été exactement le total)
            # Bien que la logique dans la boucle devrait déjà le faire. Peut être redondant mais ne fait pas de mal.
            pcb(f"MASTER_TILE_COUNT_UPDATE:{tiles_processed_count_ph3}/{num_seestar_stacks_to_process}", prog=None, lvl="ETA_LEVEL")

            logger.info("All master tiles complete, entering Phase 5 (reproject & coadd)")
            if progress_callback:
                try:
                    progress_callback("run_info_phase3_finished", None, "INFO", num_master_tiles=len(master_tiles_results_list))
                except Exception:
                    logger.warning("progress_callback failed for phase3 finished", exc_info=True)




            
            
            # --- Phase 4 (Calcul Grille Finale) ---
            base_progress_phase4 = current_global_progress
            _log_memory_usage(progress_callback, "Début Phase 4 (Calcul Grille)")
            pcb("run_info_phase4_started", prog=base_progress_phase4, lvl="INFO")
            pcb("PHASE_UPDATE:4", prog=None, lvl="ETA_LEVEL")
            telemetry.maybe_emit_stats(
                _telemetry_context(
                    {
                        "phase_name": "Phase 4: Final Grid",
                        "phase_index": 4,
                        "tiles_total": len(master_tiles_results_list),
                    }
                )
            )
            wcs_list_for_final_grid = []; shapes_list_for_final_grid_hw = []
            start_time_loop_ph4 = time.time(); last_time_loop_ph4 = start_time_loop_ph4; step_times_ph4 = []
            total_steps_ph4 = len(master_tiles_results_list)
            for idx_loop, (mt_path_iter,mt_wcs_iter) in enumerate(master_tiles_results_list, 1):
                # ... (logique de récupération shape, inchangée) ...
                if not (mt_path_iter and _path_exists(mt_path_iter) and mt_wcs_iter and mt_wcs_iter.is_celestial): pcb("run_warn_phase4_invalid_master_tile_for_grid", prog=None, lvl="WARN", path=_safe_basename(mt_path_iter)); continue
                try:
                    h_mt_loc,w_mt_loc=0,0
                    if mt_wcs_iter.pixel_shape and mt_wcs_iter.pixel_shape[0] > 0 and mt_wcs_iter.pixel_shape[1] > 0 : h_mt_loc,w_mt_loc=mt_wcs_iter.pixel_shape[1],mt_wcs_iter.pixel_shape[0] 
                    else: 
                        with fits.open(mt_path_iter,memmap=True, do_not_scale_image_data=True) as hdul_mt_s:
                            if hdul_mt_s[0].data is None: pcb("run_warn_phase4_no_data_in_tile_fits", prog=None, lvl="WARN", path=_safe_basename(mt_path_iter)); continue
                            data_shape = hdul_mt_s[0].shape
                            if len(data_shape) == 3:
                                # data_shape == (height, width, channels)
                                h_mt_loc,w_mt_loc = data_shape[0],data_shape[1]
                            elif len(data_shape) == 2: h_mt_loc,w_mt_loc = data_shape[0],data_shape[1]
                            else: pcb("run_warn_phase4_unhandled_tile_shape", prog=None, lvl="WARN", path=_safe_basename(mt_path_iter), shape=data_shape); continue 
                            if mt_wcs_iter and mt_wcs_iter.is_celestial and mt_wcs_iter.pixel_shape is None:
                                try: mt_wcs_iter.pixel_shape=(w_mt_loc,h_mt_loc)
                                except Exception as e_set_ps: pcb("run_warn_phase4_failed_set_pixel_shape", prog=None, lvl="WARN", path=_safe_basename(mt_path_iter), error=str(e_set_ps))
                    if h_mt_loc > 0 and w_mt_loc > 0: shapes_list_for_final_grid_hw.append((int(h_mt_loc),int(w_mt_loc))); wcs_list_for_final_grid.append(mt_wcs_iter)
                    else: pcb("run_warn_phase4_zero_dimensions_tile", prog=None, lvl="WARN", path=_safe_basename(mt_path_iter))
                    now = time.time(); step_times_ph4.append(now - last_time_loop_ph4); last_time_loop_ph4 = now
                    if progress_callback:
                        try:
                            progress_callback("phase4_grid", idx_loop, total_steps_ph4)
                        except Exception:
                            pass
                    telemetry.maybe_emit_stats(
                        _telemetry_context(
                            {
                                "phase_name": "Phase 4: Final Grid",
                                "phase_index": 4,
                                "tiles_done": idx_loop,
                                "tiles_total": total_steps_ph4,
                            }
                        )
                    )
                except Exception as e_read_tile_shape: pcb("run_error_phase4_reading_tile_shape", prog=None, lvl="ERROR", path=_safe_basename(mt_path_iter), error=str(e_read_tile_shape)); logger.error(f"Erreur lecture shape tuile {_safe_basename(mt_path_iter)}:", exc_info=True); continue
            if not wcs_list_for_final_grid or not shapes_list_for_final_grid_hw or len(wcs_list_for_final_grid) != len(shapes_list_for_final_grid_hw): pcb("run_error_phase4_insufficient_tile_info", prog=(base_progress_phase4 + PROGRESS_WEIGHT_PHASE4_GRID_CALC), lvl="ERROR"); return
            final_mosaic_drizzle_scale = 1.0 
            final_output_wcs, final_output_shape_hw = _calculate_final_mosaic_grid(wcs_list_for_final_grid, shapes_list_for_final_grid_hw, final_mosaic_drizzle_scale, progress_callback)
            if not final_output_wcs or not final_output_shape_hw: pcb("run_error_phase4_grid_calc_failed", prog=(base_progress_phase4 + PROGRESS_WEIGHT_PHASE4_GRID_CALC), lvl="ERROR"); return
            current_global_progress = base_progress_phase4 + PROGRESS_WEIGHT_PHASE4_GRID_CALC
            _log_memory_usage(progress_callback, "Fin Phase 4");
            if step_times_ph4:
                avg_step = sum(step_times_ph4) / len(step_times_ph4)
                total_elapsed = time.time() - start_time_loop_ph4
                pcb(
                    "phase4_debug_timing",
                    prog=None,
                    lvl="DEBUG_DETAIL",
                    avg=f"{avg_step:.2f}",
                    total=f"{total_elapsed:.2f}",
                )
            pcb("run_info_phase4_finished", prog=current_global_progress, lvl="INFO", shape=final_output_shape_hw, crval=final_output_wcs.wcs.crval if final_output_wcs.wcs else 'N/A')

            base_progress_phase4_5 = current_global_progress
            phase45_options = _build_phase45_options_dict(base_progress_phase4_5)
            base_progress_phase5 = base_progress_phase4_5 + PROGRESS_WEIGHT_PHASE4_5_INTER_MASTER
            phase5_options = _build_phase5_options_dict(base_progress_phase5)

            telemetry.maybe_emit_stats(
                _telemetry_context(
                    {
                        "phase_name": "Phase 5: Assembly",
                        "phase_index": 5,
                        "tiles_total": len(master_tiles_results_list),
                    }
                )
            )
            (
                master_tiles_results_list,
                final_mosaic_data_HWC,
                final_mosaic_coverage_HW,
                final_alpha_map,
                alpha_final,
                current_global_progress,
            ) = _run_shared_phase45_phase5_pipeline(
                master_tiles_results_list,
                final_output_wcs=final_output_wcs,
                final_output_shape_hw=final_output_shape_hw,
                temp_master_tile_storage_dir=temp_master_tile_storage_dir,
                output_folder=output_folder,
                cache_retention_mode=cache_retention_mode,
                phase45_options=phase45_options,
                phase5_options=phase5_options,
                final_quality_pipeline_cfg=final_quality_pipeline_cfg,
                start_time_total_run=start_time_total_run,
                progress_callback=progress_callback,
                pcb=pcb,
                logger=logger,
            )
            if final_mosaic_data_HWC is None:
                return

    # --- Phase 6 (Sauvegarde) ---
    base_progress_phase6 = current_global_progress
    pcb("PHASE_UPDATE:6", prog=None, lvl="ETA_LEVEL")
    _log_memory_usage(progress_callback, "Début Phase 6 (Sauvegarde)")
    pcb("run_info_phase6_started", prog=base_progress_phase6, lvl="INFO")
    telemetry.maybe_emit_stats(
        _telemetry_context(
            {
                "phase_name": "Phase 6: Save",
                "phase_index": 6,
                "tiles_total": len(master_tiles_results_list),
            }
        )
    )
    output_base_name = f"zemosaic_MT{len(master_tiles_results_list)}_R{len(all_raw_files_processed_info)}"
    output_folder_path = Path(output_folder).expanduser()
    final_fits_path = output_folder_path / f"{output_base_name}.fits"
    
    final_header = fits.Header() 
    if final_output_wcs:
        try: final_header.update(final_output_wcs.to_header(relax=True))
        except Exception as e_hdr_wcs: pcb("run_warn_phase6_wcs_to_header_failed", error=str(e_hdr_wcs), lvl="WARN")
    
    final_header['SOFTWARE']=('ZeMosaic v4.1.0','Mosaic Software') # Incrémente la version 
    final_header['NMASTILE']=(len(master_tiles_results_list),"Master Tiles combined")
    final_header['NRAWINIT']=(num_total_raw_files,"Initial raw images found")
    final_header['NRAWPROC']=(len(all_raw_files_processed_info),"Raw images with WCS processed")
    # ... (autres clés de config comme ASTAP, Stacking, etc.) ...
    final_header['STK_NORM'] = (str(stack_norm_method), 'Stacking: Normalization Method')
    final_header['STK_WGHT'] = (str(stack_weight_method), 'Stacking: Weighting Method')
    if apply_radial_weight_config:
        final_header['STK_RADW'] = (True, 'Stacking: Radial Weighting Applied')
        final_header['STK_RADFF'] = (radial_feather_fraction_config, 'Stacking: Radial Feather Fraction')
        final_header['STK_RADPW'] = (radial_shape_power_config, 'Stacking: Radial Weight Shape Power')
        final_header['STK_RADFLR'] = (min_radial_weight_floor_config, 'Stacking: Min Radial Weight Floor')
    else:
        final_header['STK_RADW'] = (False, 'Stacking: Radial Weighting Applied')
    final_header['STK_REJ'] = (str(stack_reject_algo), 'Stacking: Rejection Algorithm')
    # ... (kappa, winsor si pertinent pour l'algo de rejet) ...
    final_header['STK_COMB'] = (str(stack_final_combine), 'Stacking: Final Combine Method')
    final_header['ALPHAEXT'] = (1 if alpha_final is not None else 0, 'Alpha mask ext present')
    final_header['ZMASMBMTH'] = (final_assembly_method_config, 'Final Assembly Method')
    final_header['ZM_WORKERS'] = (num_base_workers_config, 'GUI: Base workers config (0=auto)')

    if logger.isEnabledFor(logging.DEBUG):
        _dbg_rgb_stats(
            "P6_PRE_EXPORT",
            final_mosaic_data_HWC,
            coverage=final_mosaic_coverage_HW,
            alpha=alpha_final,
            logger=logger,
        )

    rgb_black_level_info: dict[str, Any] | None = None
    if (
        final_mosaic_black_point_equalize_enabled
        and final_mosaic_data_HWC is not None
        and not sds_mode_phase5
    ):
        coverage_for_bl = None if alpha_final is not None else final_mosaic_coverage_HW
        final_mosaic_data_HWC, rgb_black_level_info = _equalize_rgb_black_level_hwc(
            final_mosaic_data_HWC,
            alpha_mask=alpha_final,
            coverage_mask=coverage_for_bl,
            p_low=final_mosaic_black_point_percentile,
            logger=logger,
        )

    try:
        if not (ZEMOSAIC_UTILS_AVAILABLE and zemosaic_utils):
            raise RuntimeError("zemosaic_utils non disponible pour sauvegarde FITS.")
        legacy_rgb_flag = bool(legacy_rgb_cube_config)
        # Ensure the final mosaic buffer is a contiguous, writeable ndarray for I/O
        save_array = None
        try:
            if isinstance(final_mosaic_data_HWC, np.ndarray):
                save_array = np.ascontiguousarray(final_mosaic_data_HWC)
                if not save_array.flags.writeable:
                    save_array = save_array.copy()
        except Exception:
            save_array = final_mosaic_data_HWC
        def _attach_alpha_extension(target_path: Path, *, log_success: bool = False) -> None:
            path_obj = Path(target_path)
            if (
                alpha_final is None
                or not path_obj
                or not path_obj.exists()
                or not (ASTROPY_AVAILABLE and fits)
                or not (ZEMOSAIC_UTILS_AVAILABLE and zemosaic_utils)
                or not hasattr(zemosaic_utils, "append_alpha_hdu")
            ):
                return
            try:
                with fits.open(str(path_obj), mode="update") as hdul_final:
                    zemosaic_utils.append_alpha_hdu(hdul_final, alpha_final)
                    hdul_final.flush()
                if log_success:
                    logger.info(
                        "phase6: wrote ALPHA extension (uint8, 0–255), shape=%s",
                        getattr(alpha_final, "shape", None),
                    )
            except Exception as exc_alpha:
                logger.warning("phase6: could not write ALPHA extension: %s", exc_alpha)

        is_rgb = (
            isinstance(save_array, np.ndarray)
            and save_array.ndim == 3
            and save_array.shape[-1] == 3
        )
        zemosaic_utils.save_fits_image(
            image_data=save_array,
            output_path=str(final_fits_path),
            header=final_header,
            overwrite=True,
            save_as_float=True,
            legacy_rgb_cube=legacy_rgb_flag,
            progress_callback=progress_callback,
            axis_order="HWC",
            alpha_mask=alpha_final,
        )
        _attach_alpha_extension(final_fits_path, log_success=True)

        if (
            bool(save_final_as_uint16_config)
            and not legacy_rgb_flag
            and ZEMOSAIC_UTILS_AVAILABLE
            and hasattr(zemosaic_utils, "write_final_fits_uint16_color_aware")
        ):
            viewer_fits_path = output_folder_path / f"{output_base_name}_viewer.fits"
            try:
                zemosaic_utils.write_final_fits_uint16_color_aware(
                    str(viewer_fits_path),
                    save_array,
                    header=final_header,
                    force_rgb_planes=is_rgb,
                    legacy_rgb_cube=legacy_rgb_flag,
                    overwrite=True,
                )
                _attach_alpha_extension(viewer_fits_path, log_success=False)
                pcb(
                    "run_info_phase6_viewer_fits_saved",
                    prog=None,
                    lvl="INFO_DETAIL",
                    filename=_safe_basename(viewer_fits_path),
                )
            except Exception as e_viewer:
                pcb(
                    "run_warn_phase6_viewer_fits_failed",
                    prog=None,
                    lvl="WARN",
                    error=str(e_viewer),
                )
        
        if final_mosaic_coverage_HW is not None and np.any(final_mosaic_coverage_HW):
            coverage_path = output_folder_path / f"{output_base_name}_coverage.fits"
            cov_hdr = fits.Header() 
            if ASTROPY_AVAILABLE and final_output_wcs: 
                try: cov_hdr.update(final_output_wcs.to_header(relax=True))
                except: pass 
            cov_hdr['EXTNAME']=('COVERAGE','Coverage Map') 
            cov_hdr['BUNIT']=('count','Pixel contributions or sum of weights')
            zemosaic_utils.save_fits_image(
                final_mosaic_coverage_HW,
                str(coverage_path),
                header=cov_hdr,
                overwrite=True,
                save_as_float=True,
                progress_callback=progress_callback,
                axis_order="HWC",
            )
            pcb("run_info_coverage_map_saved", prog=None, lvl="INFO_DETAIL", filename=_safe_basename(coverage_path))

        logger.info("[Alpha] Final mosaic saved with ALPHA=%s", bool(alpha_final is not None))

        current_global_progress = base_progress_phase6 + PROGRESS_WEIGHT_PHASE6_SAVE
        pcb("run_success_mosaic_saved", prog=current_global_progress, lvl="SUCCESS", filename=_safe_basename(final_fits_path))

        if logger.isEnabledFor(logging.DEBUG):
            _dbg_rgb_stats(
                "P7_POST_EXPORT",
                final_mosaic_data_HWC,
                coverage=final_mosaic_coverage_HW,
                alpha=alpha_final,
                logger=logger,
            )
    except Exception as e_save_m:
        pcb("run_error_phase6_save_failed", prog=(base_progress_phase6 + PROGRESS_WEIGHT_PHASE6_SAVE), lvl="ERROR", error=str(e_save_m))
        logger.error("Erreur sauvegarde FITS final:", exc_info=True)
        # En cas d'échec de sauvegarde, on ne peut pas générer de preview car final_mosaic_data_HWC pourrait être le problème.
        # On essaie quand même de nettoyer avant de retourner.
        if 'final_mosaic_data_HWC' in locals() and final_mosaic_data_HWC is not None: del final_mosaic_data_HWC
        if 'final_mosaic_coverage_HW' in locals() and final_mosaic_coverage_HW is not None: del final_mosaic_coverage_HW
        gc.collect()
        return

    _log_memory_usage(progress_callback, "Fin Sauvegarde FITS (avant preview)")

    # --- MODIFIÉ : Génération de la Preview PNG avec stretch_auto_asifits_like ---
    if final_mosaic_data_HWC is not None and ZEMOSAIC_UTILS_AVAILABLE and zemosaic_utils:
        pcb("run_info_preview_stretch_started_auto_asifits", prog=None, lvl="INFO_DETAIL") # Log mis à jour
        try:
            # Downscale extremely large mosaics for preview to avoid OOM
            step = 1
            alpha_preview: np.ndarray | None = None
            try:
                h_prev, w_prev = int(final_mosaic_data_HWC.shape[0]), int(final_mosaic_data_HWC.shape[1])
                max_preview_dim = 4000  # cap the longest side for preview
                step_h = max(1, h_prev // max_preview_dim)
                step_w = max(1, w_prev // max_preview_dim)
                step = max(step_h, step_w)
                if step > 1:
                    preview_view = final_mosaic_data_HWC[::step, ::step, :]
                    pcb("run_info_preview_downscale", prog=None, lvl="INFO_DETAIL", downscale_step=step, src_shape=str(final_mosaic_data_HWC.shape), preview_shape=str(preview_view.shape))
                else:
                    preview_view = final_mosaic_data_HWC
            except Exception:
                preview_view = final_mosaic_data_HWC
                step = 1

            if alpha_final is not None:
                try:
                    alpha_src = np.asarray(alpha_final, dtype=np.uint8, copy=False)
                    if alpha_src.ndim == 3 and alpha_src.shape[-1] == 1:
                        alpha_src = alpha_src[..., 0]
                    elif alpha_src.ndim > 2:
                        alpha_src = np.squeeze(alpha_src)
                    if alpha_src.shape[:2] != final_mosaic_data_HWC.shape[:2]:
                        logger.debug(
                            "[Alpha] Preview source mask shape mismatch: alpha=%s vs mosaic=%s",
                            getattr(alpha_src, "shape", None),
                            getattr(final_mosaic_data_HWC, "shape", None),
                        )
                        try:
                            import cv2  # type: ignore

                            alpha_src = cv2.resize(
                                alpha_src,
                                (final_mosaic_data_HWC.shape[1], final_mosaic_data_HWC.shape[0]),
                                interpolation=cv2.INTER_NEAREST,
                            )
                        except Exception:
                            alpha_src = None
                    if alpha_src is None:
                        alpha_preview = None
                        raise ValueError("alpha source unavailable for preview masking")
                    alpha_preview = alpha_src[::step, ::step] if step > 1 else alpha_src
                    if alpha_preview.shape[:2] != preview_view.shape[:2]:
                        try:
                            import cv2  # type: ignore

                            alpha_preview = cv2.resize(
                                alpha_preview,
                                (preview_view.shape[1], preview_view.shape[0]),
                                interpolation=cv2.INTER_NEAREST,
                            )
                        except Exception:
                            if (
                                alpha_preview.shape[0] >= preview_view.shape[0]
                                and alpha_preview.shape[1] >= preview_view.shape[1]
                            ):
                                alpha_preview = alpha_preview[
                                    : preview_view.shape[0], : preview_view.shape[1]
                                ]
                            else:
                                alpha_preview = None
                    if alpha_preview is not None and alpha_preview.shape[:2] == preview_view.shape[:2]:
                        mask_zero = alpha_preview == 0
                        if np.any(mask_zero):
                            preview_view = np.array(preview_view, copy=True)
                            try:
                                preview_view[mask_zero[..., None]] = np.nan
                            except Exception as e_nan:
                                logger.warning(
                                    "phase6: preview NaN masking failed: %s (shape preview=%s, alpha=%s)",
                                    e_nan,
                                    getattr(preview_view, "shape", None),
                                    getattr(alpha_preview, "shape", None),
                                )
                    else:
                        alpha_preview = None
                except Exception as exc_alpha_prev:
                    logger.warning(
                        "phase6: preview NaN masking failed: %s (shape preview=%s, alpha=%s)",
                        exc_alpha_prev,
                        getattr(preview_view, "shape", None),
                        getattr(alpha_final, "shape", None),
                    )
                    alpha_preview = None

            # Vérifier si la fonction stretch_auto_asifits_like existe dans zemosaic_utils
            if hasattr(zemosaic_utils, 'stretch_auto_asifits_like') and callable(zemosaic_utils.stretch_auto_asifits_like):
                
                # Paramètres pour stretch_auto_asifits_like (à ajuster si besoin)
                # Ces valeurs sont des exemples, tu devras peut-être les affiner
                # ou les rendre configurables plus tard.
                preview_p_low = 2.5  # Percentile pour le point noir (plus élevé que pour asinh seul)
                preview_p_high = 99.8 # Percentile pour le point blanc initial
                                      # Facteur 'a' pour le stretch asinh après la normalisation initiale
                                      # Pour un stretch plus "doux" similaire à ASIFitsView, 'a' peut être plus grand.
                                      # ASIFitsView utilise souvent un 'midtones balance' (gamma-like) aussi.
                                      # Un 'a' de 10 comme dans ton code de test est très doux. Essayons 0.5 ou 1.0.
                preview_asinh_a = 20.0 # Test avec une valeur plus douce pour le 'a' de asinh

                # Prefer GPU stretch when GPU is enabled/available
                if use_gpu_phase5_flag and hasattr(zemosaic_utils, 'stretch_auto_asifits_like_gpu'):
                    m_stretched = zemosaic_utils.stretch_auto_asifits_like_gpu(
                        preview_view,
                        p_low=preview_p_low,
                        p_high=preview_p_high,
                        asinh_a=preview_asinh_a,
                        apply_wb=True,
                    )
                else:
                    m_stretched = zemosaic_utils.stretch_auto_asifits_like(
                        preview_view,
                        p_low=preview_p_low,
                        p_high=preview_p_high,
                        asinh_a=preview_asinh_a,
                        apply_wb=True  # Applique une balance des blancs automatique
                    )

                if m_stretched is not None:
                    img_u8 = (
                        np.nan_to_num(
                            np.clip(m_stretched.astype(np.float32), 0, 1)
                        )
                        * 255
                    ).astype(np.uint8)
                    png_path = output_folder_path / f"{output_base_name}_preview.png"
                    try: 
                        import cv2 # Importer cv2 seulement si nécessaire
                        alpha_png = None
                        if alpha_preview is not None:
                            alpha_png = np.clip(alpha_preview, 0, 255).astype(np.uint8, copy=False)
                            if alpha_png.shape[:2] != img_u8.shape[:2]:
                                alpha_png = cv2.resize(
                                    alpha_png,
                                    (img_u8.shape[1], img_u8.shape[0]),
                                    interpolation=cv2.INTER_NEAREST,
                                )
                        if alpha_png is not None:
                            img_bgra = cv2.cvtColor(img_u8, cv2.COLOR_RGB2BGRA)
                            img_bgra[..., 3] = alpha_png
                            write_success = cv2.imwrite(str(png_path), img_bgra)
                        else:
                            img_bgr = cv2.cvtColor(img_u8, cv2.COLOR_RGB2BGR)
                            write_success = cv2.imwrite(str(png_path), img_bgr)
                        if write_success: 
                            pcb("run_success_preview_saved_auto_asifits", prog=None, lvl="SUCCESS", filename=_safe_basename(png_path))
                            if alpha_png is not None:
                                logger.info("phase6: preview masked (NaN pre-stretch) and saved as RGBA PNG")
                        else: 
                            pcb("run_warn_preview_imwrite_failed_auto_asifits", prog=None, lvl="WARN", filename=_safe_basename(png_path))
                    except ImportError: 
                        pcb("run_warn_preview_opencv_missing_for_auto_asifits", prog=None, lvl="WARN")
                    except Exception as e_cv2_prev: 
                        pcb("run_error_preview_opencv_failed_auto_asifits", prog=None, lvl="ERROR", error=str(e_cv2_prev))
                else:
                    pcb("run_error_preview_stretch_auto_asifits_returned_none", prog=None, lvl="ERROR")
            else:
                pcb("run_warn_preview_stretch_auto_asifits_func_missing", prog=None, lvl="WARN")
                # Fallback sur l'ancienne méthode si stretch_auto_asifits_like n'est pas trouvée
                # (Tu peux supprimer ce fallback si tu es sûr que la fonction existe)
                pcb("run_info_preview_fallback_to_simple_asinh", prog=None, lvl="DEBUG_DETAIL")
                if hasattr(zemosaic_utils, 'stretch_percentile_rgb') and zemosaic_utils.ASTROPY_VISUALIZATION_AVAILABLE:
                     m_stretched_fallback = zemosaic_utils.stretch_percentile_rgb(final_mosaic_data_HWC, p_low=0.5, p_high=99.9, independent_channels=False, asinh_a=0.01 )
                     if m_stretched_fallback is not None:
                        img_u8_fb = (np.clip(m_stretched_fallback.astype(np.float32), 0, 1) * 255).astype(np.uint8)
                        png_path_fb = output_folder_path / f"{output_base_name}_preview_fallback.png"
                        try:
                            import cv2
                            alpha_png_fb = None
                            alpha_source_fb = alpha_final if alpha_final is not None else alpha_preview
                            if alpha_source_fb is not None:
                                alpha_png_fb = np.clip(alpha_source_fb, 0, 255).astype(np.uint8, copy=False)
                                if alpha_png_fb.shape[:2] != img_u8_fb.shape[:2]:
                                    alpha_png_fb = cv2.resize(
                                        alpha_png_fb,
                                        (img_u8_fb.shape[1], img_u8_fb.shape[0]),
                                        interpolation=cv2.INTER_NEAREST,
                                    )
                            if alpha_png_fb is not None:
                                img_bgra_fb = cv2.cvtColor(img_u8_fb, cv2.COLOR_RGB2BGRA)
                                img_bgra_fb[..., 3] = alpha_png_fb
                                cv2.imwrite(str(png_path_fb), img_bgra_fb)
                                logger.info("phase6: preview masked (NaN pre-stretch) and saved as RGBA PNG")
                            else:
                                img_bgr_fb = cv2.cvtColor(img_u8_fb, cv2.COLOR_RGB2BGR)
                                cv2.imwrite(str(png_path_fb), img_bgr_fb)
                            pcb("run_success_preview_saved_fallback", prog=None, lvl="INFO_DETAIL", filename=_safe_basename(png_path_fb))
                        except: pass # Ignorer erreur fallback

        except Exception as e_stretch_main: 
            pcb("run_error_preview_stretch_unexpected_main", prog=None, lvl="ERROR", error=str(e_stretch_main))
            logger.error("Erreur imprévue lors de la génération de la preview:", exc_info=True)
            
    if 'final_mosaic_data_HWC' in locals() and final_mosaic_data_HWC is not None: del final_mosaic_data_HWC
    if 'final_mosaic_coverage_HW' in locals() and final_mosaic_coverage_HW is not None: del final_mosaic_coverage_HW
    gc.collect()

    # Cleanup memmap .dat files now that arrays are released (Windows requires handles closed)
    def _cleanup_memmap_artifacts():
        if not cleanup_temp_artifacts_config:
            return

        if (
            bool(coadd_use_memmap_config)
            and bool(coadd_cleanup_memmap_config)
            and coadd_memmap_dir_config
            and _path_isdir(coadd_memmap_dir_config)
        ):
            try:
                memmap_cleanup_dir = Path(coadd_memmap_dir_config).expanduser()
                for entry in memmap_cleanup_dir.iterdir():
                    name_l = entry.name.lower()
                    if entry.is_file() and name_l.endswith(".dat") and (
                        name_l.startswith("mosaic_")
                        or name_l.startswith("coverage_")
                        or name_l.startswith("zemosaic_")
                    ):
                        try:
                            entry.unlink()
                        except OSError:
                            pass
                        continue
                    if entry.is_dir() and name_l.startswith("mosaic_first_"):
                        try:
                            shutil.rmtree(str(entry), ignore_errors=False)
                        except Exception:
                            pass
            except Exception:
                pass

        try:
            runtime_temp_root = get_runtime_temp_dir()
        except Exception:
            runtime_temp_root = None
        if runtime_temp_root:
            try:
                runtime_path = Path(runtime_temp_root).expanduser()
                if runtime_path.is_dir():
                    for entry in runtime_path.iterdir():
                        if entry.is_dir() and entry.name.lower().startswith("mosaic_first_"):
                            try:
                                shutil.rmtree(str(entry), ignore_errors=False)
                            except Exception:
                                pass
            except Exception:
                pass

        wcs_candidates: list[Path] = []
        try:
            if isinstance(global_wcs_plan, dict):
                if global_wcs_plan.get("fits_path"):
                    wcs_candidates.append(Path(global_wcs_plan.get("fits_path")))
                if global_wcs_plan.get("json_path"):
                    wcs_candidates.append(Path(global_wcs_plan.get("json_path")))
        except Exception:
            pass
        default_wcs_name = str(
            (worker_config_cache or {}).get("global_wcs_output_path", "global_mosaic_wcs.fits")
        ).strip() or "global_mosaic_wcs.fits"
        if output_folder:
            output_folder_path = Path(output_folder).expanduser()
            wcs_candidates.append(output_folder_path / default_wcs_name)
            wcs_candidates.append(output_folder_path / f"{default_wcs_name}.json")
        for candidate in wcs_candidates:
            if not candidate:
                continue
            try:
                candidate_path = Path(candidate)
                if candidate_path.is_dir():
                    continue
                if candidate_path.exists() and "global_mosaic_wcs" in candidate_path.name.lower():
                    candidate_path.unlink()
            except OSError:
                pass

    _cleanup_memmap_artifacts()

    if sds_runtime_tile_dir and cleanup_temp_artifacts_config:
        try:
            shutil.rmtree(sds_runtime_tile_dir, ignore_errors=True)
        except Exception:
            pass


    # --- Phase 7 (Nettoyage) ---
    # ... (contenu Phase 7 inchangé) ...
    base_progress_phase7 = current_global_progress
    _log_memory_usage(progress_callback, "Début Phase 7 (Nettoyage)")
    pcb("run_info_phase7_cleanup_starting", prog=base_progress_phase7, lvl="INFO")
    pcb("PHASE_UPDATE:7", prog=None, lvl="ETA_LEVEL")
    telemetry.maybe_emit_stats(
        _telemetry_context({"phase_name": "Phase 7: Cleanup", "phase_index": 7})
    )
    def _log_cleanup_warning(msg_key: str, directory: str | None, exc: Exception) -> None:
        pcb(
            msg_key,
            prog=None,
            lvl="WARN",
            directory=directory or "<unknown>",
            error=str(exc),
        )

    if cache_retention_mode == "keep":
        if _path_exists(temp_image_cache_dir):
            pcb(
                "run_info_temp_preprocessed_cache_kept",
                prog=None,
                lvl="INFO_DETAIL",
                directory=temp_image_cache_dir,
            )
    else:
        if _path_exists(temp_image_cache_dir):
            try:
                shutil.rmtree(temp_image_cache_dir)
                pcb(
                    "run_info_temp_preprocessed_cache_cleaned",
                    prog=None,
                    lvl="INFO_DETAIL",
                    directory=temp_image_cache_dir,
                )
            except Exception as cache_exc:
                _log_cleanup_warning(
                    "run_warn_phase7_cache_cleanup_failed",
                    temp_image_cache_dir,
                    cache_exc,
                )

    master_tiles_dir = temp_master_tile_storage_dir
    if master_tiles_dir and _path_exists(master_tiles_dir):
        if cleanup_temp_artifacts_config:
            try:
                shutil.rmtree(master_tiles_dir)
                pcb(
                    "run_info_temp_master_tiles_fits_cleaned",
                    prog=None,
                    lvl="INFO_DETAIL",
                    directory=master_tiles_dir,
                )
            except Exception as mt_exc:
                _log_cleanup_warning(
                    "run_warn_phase7_master_tiles_fits_cleanup_failed",
                    master_tiles_dir,
                    mt_exc,
                )
        else:
            pcb(
                "run_info_temp_master_tiles_retained_cleanup_disabled",
                prog=None,
                lvl="INFO_DETAIL",
                directory=master_tiles_dir,
            )
    current_global_progress = base_progress_phase7 + PROGRESS_WEIGHT_PHASE7_CLEANUP; current_global_progress = min(100, current_global_progress)
    _log_memory_usage(progress_callback, "Fin Phase 7"); pcb("CHRONO_STOP_REQUEST", prog=None, lvl="CHRONO_LEVEL"); update_gui_eta(0)
    total_duration_sec = time.monotonic() - start_time_total_run
    pcb("run_success_processing_completed", prog=current_global_progress, lvl="SUCCESS", duration=f"{total_duration_sec:.2f}")
    gc.collect(); _log_memory_usage(progress_callback, "Fin Run Hierarchical Mosaic (après GC final)")
    _log_alignment_warning_summary()
    telemetry.close()
    logger.info(f"===== Run Hierarchical Mosaic COMPLETED in {total_duration_sec:.2f}s =====")
################################################################################
################################################################################
####


def run_hierarchical_mosaic(
    input_folder: str,
    output_folder: str,
    astap_exe_path: str,
    astap_data_dir_param: str,
    astap_search_radius_config: float,
    astap_downsample_config: int,
    astap_sensitivity_config: int,
    cluster_threshold_config: float,
    cluster_target_groups_config: int,
    cluster_orientation_split_deg_config: float,
    progress_callback: callable,
    stack_ram_budget_gb_config: float,
    stack_norm_method: str,
    stack_weight_method: str,
    stack_reject_algo: str,
    stack_kappa_low: float,
    stack_kappa_high: float,
    parsed_winsor_limits: tuple[float, float],
    stack_final_combine: str,
    poststack_equalize_rgb_config: bool,
    apply_radial_weight_config: bool,
    radial_feather_fraction_config: float,
    radial_shape_power_config: float,
    min_radial_weight_floor_config: float,
    final_assembly_method_config: str,
    inter_master_merge_enable_config: bool,
    inter_master_overlap_threshold_config: float,
    inter_master_min_group_size_config: int,
    inter_master_stack_method_config: str,
    inter_master_memmap_policy_config: str,
    inter_master_local_scale_config: str,
    inter_master_max_group_config: int,
    num_base_workers_config: int,
    # --- ARGUMENTS POUR LE ROGNAGE ---
    apply_master_tile_crop_config: bool,
    master_tile_crop_percent_config: float,
    quality_crop_enabled_config: bool,
    quality_crop_band_px_config: int,
    quality_crop_k_sigma_config: float,
    quality_crop_margin_px_config: int,
    quality_crop_min_run_config: int,
    altaz_cleanup_enabled_config: bool,
    altaz_margin_percent_config: float,
    altaz_decay_config: float,
    altaz_nanize_config: bool,
    quality_gate_enabled_config: bool,
    quality_gate_threshold_config: float,
    quality_gate_edge_band_px_config: int,
    quality_gate_k_sigma_config: float,
    quality_gate_erode_px_config: int,
    quality_gate_move_rejects_config: bool,
    save_final_as_uint16_config: bool,
    legacy_rgb_cube_config: bool,

    coadd_use_memmap_config: bool,
    coadd_memmap_dir_config: str,
    coadd_cleanup_memmap_config: bool,
    assembly_process_workers_config: int,
    auto_limit_frames_per_master_tile_config: bool,
    winsor_max_frames_per_pass_config: int,
    winsor_worker_limit_config: int,
    max_raw_per_master_tile_config: int,
    intertile_photometric_match_config: bool = True,
    intertile_preview_size_config: int = 512,
    intertile_overlap_min_config: float = 0.05,
    intertile_sky_percentile_config: tuple[float, float] | list[float] = (30.0, 70.0),
    intertile_robust_clip_sigma_config: float = 2.5,
    intertile_global_recenter_config: bool = True,
    intertile_recenter_clip_config: tuple[float, float] | list[float] = (0.85, 1.18),
    use_auto_intertile_config: bool = False,
    match_background_for_final_config: bool = True,
    incremental_feather_parity_config: bool = False,
    two_pass_coverage_renorm_config: bool = False,
    two_pass_cov_sigma_px_config: int = 50,
    two_pass_cov_gain_clip_config: tuple[float, float] | list[float] = (0.85, 1.18),
    center_out_normalization_p3_config: bool = True,
    p3_center_sky_percentile_config: tuple[float, float] | list[float] = (25.0, 60.0),
    p3_center_robust_clip_sigma_config: float = 2.5,
    p3_center_preview_size_config: int = 256,
    p3_center_min_overlap_fraction_config: float = 0.03,
    center_out_anchor_mode_config: str = "auto_central_quality",
    anchor_quality_probe_limit_config: int = 12,
    anchor_quality_span_range_config: tuple[float, float] | list[float] = (0.02, 6.0),
    anchor_quality_median_clip_sigma_config: float = 2.5,
    enable_poststack_anchor_review_config: bool = True,
    poststack_anchor_probe_limit_config: int = 8,
    poststack_anchor_span_range_config: tuple[float, float] | list[float] = (0.004, 10.0),
    poststack_anchor_median_clip_sigma_config: float = 3.5,
    poststack_anchor_min_improvement_config: float = 0.12,
    poststack_anchor_use_overlap_affine_config: bool = True,
    use_gpu_phase5: bool = False,
    gpu_id_phase5: int | None = None,
    enable_tile_weighting_config: bool | None = None,
    tile_weight_mode_config: str = "n_frames",
    logging_level_config: str = "INFO",
    solver_settings: dict | None = None,
    skip_filter_ui: bool = False,
    # New optional integration points when filter ran in GUI
    filter_invoked: bool = False,
    filter_overrides: dict | None = None,
    filtered_header_items: list[dict] | None = None,
    early_filter_enabled: bool | None = None,
    final_mosaic_rgb_equalize_enabled_config: bool | None = None,
    final_mosaic_black_point_equalize_enabled_config: bool | None = None,
    final_mosaic_black_point_percentile_config: float | None = None,
):
    """
    Orchestre le traitement de la mosaïque hiérarchique.

    Parameters
    ----------
    winsor_max_frames_per_pass_config : int
        Limite du nombre d'images traitées simultanément par le rejet Winsorized (0 = illimité).
    winsor_worker_limit_config : int
        Nombre maximal de workers pour la phase de rejet Winsorized.
    stack_ram_budget_gb_config : float
        Budget RAM (en Gio) autorisé pour le chargement d'un groupe de stacking (0 = illimité).
    """
    worker_config_cache: dict[str, Any] = {}
    if ZEMOSAIC_CONFIG_AVAILABLE and zemosaic_config:
        try:
            worker_config_cache = zemosaic_config.load_config() or {}
        except Exception:
            worker_config_cache = {}

    try:
        zconfig = SimpleNamespace(**worker_config_cache)
    except Exception:
        zconfig = SimpleNamespace()

    def _coerce_bool_flag(value) -> bool | None:
        """Interpret various truthy/falsy representations coming from configs/UI."""

        if value is None:
            return None
        if isinstance(value, bool):
            return value
        if isinstance(value, (int, float)) and not isinstance(value, bool):
            return value != 0
        if isinstance(value, str):
            normalized = value.strip().lower()
            if not normalized:
                return None
            if normalized in {"1", "true", "yes", "on", "enable", "enabled"}:
                return True
            if normalized in {"0", "false", "no", "off", "disable", "disabled"}:
                return False
        try:
            return bool(value)
        except Exception:
            return None

    grid_rgb_equalize_source = "argument"
    grid_rgb_equalize_flag = _coerce_bool_flag(poststack_equalize_rgb_config)
    cfg_rgb = _coerce_bool_flag(worker_config_cache.get("grid_rgb_equalize"))
    if cfg_rgb is None:
        cfg_rgb = _coerce_bool_flag(worker_config_cache.get("poststack_equalize_rgb"))
    if cfg_rgb is not None:
        grid_rgb_equalize_source = "config"
        grid_rgb_equalize_flag = cfg_rgb
    if grid_rgb_equalize_flag is None:
        grid_rgb_equalize_flag = True
        grid_rgb_equalize_source = "default"
    poststack_equalize_rgb_config = bool(grid_rgb_equalize_flag)
    setattr(zconfig, "poststack_equalize_rgb", bool(grid_rgb_equalize_flag))

    final_mosaic_rgb_equalize_enabled = _coerce_bool_flag(
        final_mosaic_rgb_equalize_enabled_config
    )
    if final_mosaic_rgb_equalize_enabled is None:
        final_mosaic_rgb_equalize_enabled = _coerce_bool_flag(
            worker_config_cache.get("final_mosaic_rgb_equalize_enabled")
        )
    if final_mosaic_rgb_equalize_enabled is None:
        final_mosaic_rgb_equalize_enabled = False
    final_mosaic_rgb_equalize_enabled = bool(final_mosaic_rgb_equalize_enabled)
    setattr(zconfig, "final_mosaic_rgb_equalize_enabled", final_mosaic_rgb_equalize_enabled)

    final_mosaic_black_point_equalize_enabled = _coerce_bool_flag(
        final_mosaic_black_point_equalize_enabled_config
    )
    if final_mosaic_black_point_equalize_enabled is None:
        final_mosaic_black_point_equalize_enabled = _coerce_bool_flag(
            worker_config_cache.get("final_mosaic_black_point_equalize_enabled")
        )
    if final_mosaic_black_point_equalize_enabled is None:
        final_mosaic_black_point_equalize_enabled = False
    final_mosaic_black_point_equalize_enabled = bool(final_mosaic_black_point_equalize_enabled)
    setattr(zconfig, "final_mosaic_black_point_equalize_enabled", final_mosaic_black_point_equalize_enabled)

    try:
        final_mosaic_black_point_percentile = float(
            final_mosaic_black_point_percentile_config
        )
    except (ValueError, TypeError):
        final_mosaic_black_point_percentile = None

    if final_mosaic_black_point_percentile is None:
        try:
            final_mosaic_black_point_percentile = float(
                worker_config_cache.get("final_mosaic_black_point_percentile")
            )
        except (ValueError, TypeError):
            final_mosaic_black_point_percentile = 0.1

    if not (
        isinstance(final_mosaic_black_point_percentile, (int, float))
        and math.isfinite(final_mosaic_black_point_percentile)
    ):
        final_mosaic_black_point_percentile = 0.1
    final_mosaic_black_point_percentile = float(
        max(0.0, min(100.0, final_mosaic_black_point_percentile))
    )
    setattr(zconfig, "final_mosaic_black_point_percentile", final_mosaic_black_point_percentile)

    grid_mode_detected = detect_grid_mode(input_folder)
    if grid_mode_detected:
        if grid_mode and hasattr(grid_mode, "run_grid_mode"):
            try:
                logger.info(
                    "[GRID] Invoking grid_mode.run_grid_mode(...) with "
                    "grid_rgb_equalize=%s (source=%s), stack_norm=%s, stack_weight=%s, reject_algo=%s, combine=%s",
                    grid_rgb_equalize_flag,
                    grid_rgb_equalize_source,
                    stack_norm_method,
                    stack_weight_method,
                    stack_reject_algo,
                    stack_final_combine,
                )
                grid_mode.run_grid_mode(  # type: ignore[attr-defined]
                    input_folder=input_folder,
                    output_folder=output_folder,
                    progress_callback=progress_callback,
                    stack_norm_method=stack_norm_method,
                    stack_weight_method=stack_weight_method,
                    stack_reject_algo=stack_reject_algo,
                    stack_kappa_low=stack_kappa_low,
                    stack_kappa_high=stack_kappa_high,
                    winsor_limits=parsed_winsor_limits,
                    stack_final_combine=stack_final_combine,
                    apply_radial_weight=apply_radial_weight_config,
                    radial_feather_fraction=radial_feather_fraction_config,
                    radial_shape_power=radial_shape_power_config,
                    save_final_as_uint16=save_final_as_uint16_config,
                    legacy_rgb_cube=legacy_rgb_cube_config,
                    grid_rgb_equalize=grid_rgb_equalize_flag,
                    zconfig=zconfig,
                )
                return
            except Exception:
                logger.error("[GRID] Grid/Survey mode failed; aborting without classic fallback", exc_info=True)
                raise
        error_msg = "[GRID] grid_mode module unavailable; aborting (no classic fallback)."
        logger.error(error_msg)
        raise RuntimeError(error_msg)

    def pcb(msg_key, prog=None, lvl="INFO", **kwargs):
        """Shortcut to emit log+callback events with the current progress callback."""
        _log_and_callback(msg_key, prog, lvl, callback=progress_callback, **kwargs)

    global_wcs_plan = _prepare_global_wcs_plan(
        output_folder,
        worker_config_cache,
        filter_overrides,
    )

    config_sds_default = _coerce_bool_flag(worker_config_cache.get("sds_mode_default"))
    override_flag = None
    override_defined = False
    plan_override_flag = None
    plan_override_defined = False
    plan_override = None
    if isinstance(filter_overrides, dict):
        if "sds_mode" in filter_overrides:
            override_flag = _coerce_bool_flag(filter_overrides.get("sds_mode"))
            override_defined = override_flag is not None
        plan_override = filter_overrides.get("global_wcs_plan_override")
    if isinstance(plan_override, dict) and "sds_mode" in plan_override:
        plan_override_flag = _coerce_bool_flag(plan_override.get("sds_mode"))
        plan_override_defined = plan_override_flag is not None
    if override_defined:
        sds_mode_flag = override_flag
    elif plan_override_defined:
        sds_mode_flag = plan_override_flag
    elif config_sds_default is not None:
        sds_mode_flag = config_sds_default
    else:
        sds_mode_flag = False
    global_wcs_plan["sds_mode"] = bool(sds_mode_flag)

    if (not grid_mode_detected) and (not sds_mode_flag):
        logger.info("[Classic] Using legacy classic pipeline from non grid worker")
        return run_hierarchical_mosaic_classic_legacy(
            input_folder=input_folder,
            output_folder=output_folder,
            astap_exe_path=astap_exe_path,
            astap_data_dir_param=astap_data_dir_param,
            astap_search_radius_config=astap_search_radius_config,
            astap_downsample_config=astap_downsample_config,
            astap_sensitivity_config=astap_sensitivity_config,
            cluster_threshold_config=cluster_threshold_config,
            cluster_target_groups_config=cluster_target_groups_config,
            cluster_orientation_split_deg_config=cluster_orientation_split_deg_config,
            progress_callback=progress_callback,
            stack_ram_budget_gb_config=stack_ram_budget_gb_config,
            stack_norm_method=stack_norm_method,
            stack_weight_method=stack_weight_method,
            stack_reject_algo=stack_reject_algo,
            stack_kappa_low=stack_kappa_low,
            stack_kappa_high=stack_kappa_high,
            parsed_winsor_limits=parsed_winsor_limits,
            stack_final_combine=stack_final_combine,
            poststack_equalize_rgb_config=poststack_equalize_rgb_config,
            apply_radial_weight_config=apply_radial_weight_config,
            radial_feather_fraction_config=radial_feather_fraction_config,
            radial_shape_power_config=radial_shape_power_config,
            min_radial_weight_floor_config=min_radial_weight_floor_config,
            final_assembly_method_config=final_assembly_method_config,
            inter_master_merge_enable_config=inter_master_merge_enable_config,
            inter_master_overlap_threshold_config=inter_master_overlap_threshold_config,
            inter_master_min_group_size_config=inter_master_min_group_size_config,
            inter_master_stack_method_config=inter_master_stack_method_config,
            inter_master_memmap_policy_config=inter_master_memmap_policy_config,
            inter_master_local_scale_config=inter_master_local_scale_config,
            inter_master_max_group_config=inter_master_max_group_config,
            num_base_workers_config=num_base_workers_config,
            apply_master_tile_crop_config=apply_master_tile_crop_config,
            master_tile_crop_percent_config=master_tile_crop_percent_config,
            quality_crop_enabled_config=quality_crop_enabled_config,
            quality_crop_band_px_config=quality_crop_band_px_config,
            quality_crop_k_sigma_config=quality_crop_k_sigma_config,
            quality_crop_margin_px_config=quality_crop_margin_px_config,
            quality_crop_min_run_config=quality_crop_min_run_config,
            altaz_cleanup_enabled_config=altaz_cleanup_enabled_config,
            altaz_margin_percent_config=altaz_margin_percent_config,
            altaz_decay_config=altaz_decay_config,
            altaz_nanize_config=altaz_nanize_config,
            quality_gate_enabled_config=quality_gate_enabled_config,
            quality_gate_threshold_config=quality_gate_threshold_config,
            quality_gate_edge_band_px_config=quality_gate_edge_band_px_config,
            quality_gate_k_sigma_config=quality_gate_k_sigma_config,
            quality_gate_erode_px_config=quality_gate_erode_px_config,
            quality_gate_move_rejects_config=quality_gate_move_rejects_config,
            save_final_as_uint16_config=save_final_as_uint16_config,
            legacy_rgb_cube_config=legacy_rgb_cube_config,
            coadd_use_memmap_config=coadd_use_memmap_config,
            coadd_memmap_dir_config=coadd_memmap_dir_config,
            coadd_cleanup_memmap_config=coadd_cleanup_memmap_config,
            assembly_process_workers_config=assembly_process_workers_config,
            auto_limit_frames_per_master_tile_config=auto_limit_frames_per_master_tile_config,
            winsor_max_frames_per_pass_config=winsor_max_frames_per_pass_config,
            winsor_worker_limit_config=winsor_worker_limit_config,
            max_raw_per_master_tile_config=max_raw_per_master_tile_config,
            intertile_photometric_match_config=intertile_photometric_match_config,
            intertile_preview_size_config=intertile_preview_size_config,
            intertile_overlap_min_config=intertile_overlap_min_config,
            intertile_sky_percentile_config=intertile_sky_percentile_config,
            intertile_robust_clip_sigma_config=intertile_robust_clip_sigma_config,
            intertile_global_recenter_config=intertile_global_recenter_config,
            intertile_recenter_clip_config=intertile_recenter_clip_config,
            use_auto_intertile_config=use_auto_intertile_config,
            match_background_for_final_config=match_background_for_final_config,
            incremental_feather_parity_config=incremental_feather_parity_config,
            two_pass_coverage_renorm_config=two_pass_coverage_renorm_config,
            two_pass_cov_sigma_px_config=two_pass_cov_sigma_px_config,
            two_pass_cov_gain_clip_config=two_pass_cov_gain_clip_config,
            center_out_normalization_p3_config=center_out_normalization_p3_config,
            p3_center_sky_percentile_config=p3_center_sky_percentile_config,
            p3_center_robust_clip_sigma_config=p3_center_robust_clip_sigma_config,
            p3_center_preview_size_config=p3_center_preview_size_config,
            p3_center_min_overlap_fraction_config=p3_center_min_overlap_fraction_config,
            center_out_anchor_mode_config=center_out_anchor_mode_config,
            anchor_quality_probe_limit_config=anchor_quality_probe_limit_config,
            anchor_quality_span_range_config=anchor_quality_span_range_config,
            anchor_quality_median_clip_sigma_config=anchor_quality_median_clip_sigma_config,
            enable_poststack_anchor_review_config=enable_poststack_anchor_review_config,
            poststack_anchor_probe_limit_config=poststack_anchor_probe_limit_config,
            poststack_anchor_span_range_config=poststack_anchor_span_range_config,
            poststack_anchor_median_clip_sigma_config=poststack_anchor_median_clip_sigma_config,
            poststack_anchor_min_improvement_config=poststack_anchor_min_improvement_config,
            poststack_anchor_use_overlap_affine_config=poststack_anchor_use_overlap_affine_config,
            use_gpu_phase5=use_gpu_phase5,
            gpu_id_phase5=gpu_id_phase5,
            enable_tile_weighting_config=enable_tile_weighting_config,
            tile_weight_mode_config=tile_weight_mode_config,
            logging_level_config=logging_level_config,
            solver_settings=solver_settings,
            skip_filter_ui=skip_filter_ui,
            filter_invoked=filter_invoked,
            filter_overrides=filter_overrides,
            filtered_header_items=filtered_header_items,
            early_filter_enabled=early_filter_enabled,
            final_mosaic_rgb_equalize_enabled_config=final_mosaic_rgb_equalize_enabled_config,
            final_mosaic_black_point_equalize_enabled=final_mosaic_black_point_equalize_enabled,
            final_mosaic_black_point_percentile=final_mosaic_black_point_percentile,
        )

    try:
        sds_coverage_threshold_config = float(worker_config_cache.get("sds_coverage_threshold", 0.92))
    except Exception:
        sds_coverage_threshold_config = 0.92
    if not math.isfinite(sds_coverage_threshold_config):
        sds_coverage_threshold_config = 0.92
    sds_coverage_threshold_config = max(0.10, min(0.99, sds_coverage_threshold_config))
    try:
        sds_min_batch_size_config = int(worker_config_cache.get("sds_min_batch_size", 5) or 5)
    except Exception:
        sds_min_batch_size_config = 5
    if sds_min_batch_size_config <= 0:
        sds_min_batch_size_config = 1
    try:
        sds_target_batch_size_config = int(worker_config_cache.get("sds_target_batch_size", 10) or 10)
    except Exception:
        sds_target_batch_size_config = 10
    if sds_target_batch_size_config < sds_min_batch_size_config:
        sds_target_batch_size_config = sds_min_batch_size_config
    try:
        sds_min_coverage_keep_config = float(worker_config_cache.get("sds_min_coverage_keep", 0.4))
    except Exception:
        sds_min_coverage_keep_config = 0.4
    if not math.isfinite(sds_min_coverage_keep_config):
        sds_min_coverage_keep_config = 0.4
    sds_min_coverage_keep_config = max(0.0, min(1.0, sds_min_coverage_keep_config))

    global_wcs_autocrop_enabled_config = bool(worker_config_cache.get("global_wcs_autocrop_enabled"))
    try:
        global_wcs_autocrop_margin_px_config = int(worker_config_cache.get("global_wcs_autocrop_margin_px", 64) or 0)
    except Exception:
        global_wcs_autocrop_margin_px_config = 0
    if global_wcs_autocrop_margin_px_config < 0:
        global_wcs_autocrop_margin_px_config = 0

    if global_wcs_plan.get("enabled"):
        logger.info(
            "[Worker] Global WCS descriptor ready (%s)",
            global_wcs_plan.get("fits_path"),
        )
    else:
        requested_mode = ""
        if isinstance(filter_overrides, dict):
            requested_mode = str(filter_overrides.get("mode") or "").strip().lower()
        if requested_mode == "seestar":
            pcb(
                "global_coadd_error_descriptor_missing",
                prog=None,
                lvl="WARN",
                path=str(filter_overrides.get("global_wcs_path") or ""),
            )

    # Cache retention policy (Phase 1 preprocessed cache cleanup)
    cache_retention_mode = "run_end"
    allowed_cache_modes = {"run_end", "per_tile", "keep"}
    if worker_config_cache:
        try:
            cache_retention_mode = str(worker_config_cache.get("cache_retention", "run_end")).strip().lower()
        except Exception:
            cache_retention_mode = "run_end"
    if cache_retention_mode not in allowed_cache_modes:
        cache_retention_mode = "run_end"
    logger.info("Cache retention mode: %s", cache_retention_mode)
    try:
        pcb("run_info_cache_retention_mode", prog=None, lvl="INFO_DETAIL", mode=cache_retention_mode)
    except Exception:
        pass

    cleanup_temp_artifacts_value = (worker_config_cache or {}).get("cleanup_temp_artifacts")
    cleanup_temp_artifacts_config = _coerce_bool_flag(cleanup_temp_artifacts_value)
    if cleanup_temp_artifacts_config is None:
        cleanup_temp_artifacts_config = True

    _configure_worker_logging(logging_level_config, source_hint="qt_gui_config")

    try:
        batch_overlap_pct_config = float(worker_config_cache.get("batch_overlap_pct", 0.0))
    except Exception:
        batch_overlap_pct_config = 0.0
    batch_overlap_pct_override = None
    try:
        if isinstance(filter_overrides, dict) and "batch_overlap_pct" in filter_overrides:
            batch_overlap_pct_override = float(filter_overrides.get("batch_overlap_pct", batch_overlap_pct_config))
    except Exception:
        batch_overlap_pct_override = None
    if batch_overlap_pct_override is not None:
        batch_overlap_pct_config = batch_overlap_pct_override
    overlap_fraction_config = max(0.0, min(0.7, float(batch_overlap_pct_config) / 100.0))

    try:
        min_safe_stack_config = int(worker_config_cache.get("min_safe_stack", 3) or 3)
    except Exception:
        min_safe_stack_config = 3
    try:
        target_stack_config = int(worker_config_cache.get("target_stack", 5) or 5)
    except Exception:
        target_stack_config = 5
    allow_duplication_config = _coerce_bool_flag(worker_config_cache.get("allow_batch_duplication", True))
    if allow_duplication_config is None:
        allow_duplication_config = True
    if isinstance(filter_overrides, dict):
        if "min_safe_stack" in filter_overrides:
            try:
                min_safe_stack_config = int(filter_overrides.get("min_safe_stack", min_safe_stack_config))
            except Exception:
                pass
        if "target_stack" in filter_overrides:
            try:
                target_stack_config = int(filter_overrides.get("target_stack", target_stack_config))
            except Exception:
                pass
        if "allow_batch_duplication" in filter_overrides:
            allow_override = _coerce_bool_flag(filter_overrides.get("allow_batch_duplication"))
            if allow_override is not None:
                allow_duplication_config = allow_override
    min_safe_stack_config = max(1, min_safe_stack_config)
    target_stack_config = max(min_safe_stack_config, target_stack_config)

    # --- Harmoniser les méthodes de pondération issues du GUI / CLI / fallback config ---
    requested_stack_weight_method = stack_weight_method
    stack_weight_method_normalized = str(stack_weight_method or "").lower().strip()
    if not stack_weight_method_normalized:
        stack_weight_method_normalized = ""
        if ZEMOSAIC_CONFIG_AVAILABLE and zemosaic_config:
            try:
                cfg_weight = zemosaic_config.load_config() or {}
                stack_weight_method_normalized = str(
                    cfg_weight.get("stacking_weighting_method", "")
                ).lower().strip()
            except Exception:
                stack_weight_method_normalized = ""
    if not stack_weight_method_normalized:
        stack_weight_method_normalized = "none"
    if stack_weight_method_normalized not in {"none", "noise_variance", "noise_fwhm"}:
        stack_weight_method_normalized = "none"
    if str(requested_stack_weight_method or "").lower().strip() != stack_weight_method_normalized:
        _log_and_callback(
            f"[Worker] stack_weight_method fallback -> '{stack_weight_method_normalized}'",
            lvl="INFO",
            callback=progress_callback,
        )
    stack_weight_method = stack_weight_method_normalized

    # Reset alignment warning counters at start of run
    for k in ALIGN_WARNING_COUNTS:
        ALIGN_WARNING_COUNTS[k] = 0
    
    def update_gui_eta(eta_seconds_total):
        if progress_callback and callable(progress_callback):
            eta_str = "--:--:--"
            if eta_seconds_total is not None and eta_seconds_total >= 0:
                h, rem = divmod(int(eta_seconds_total), 3600); m, s = divmod(rem, 60)
                eta_str = f"{h:02d}:{m:02d}:{s:02d}"
            pcb(f"ETA_UPDATE:{eta_str}", prog=None, lvl="ETA_LEVEL")


    try:
        telemetry_enabled = bool(getattr(zconfig, "enable_resource_telemetry", False))
    except Exception:
        telemetry_enabled = False
    try:
        telemetry_interval = float(getattr(zconfig, "resource_telemetry_interval_sec", 1.5) or 1.5)
    except Exception:
        telemetry_interval = 1.5
    try:
        telemetry_log_to_csv = bool(getattr(zconfig, "resource_telemetry_log_to_csv", True))
    except Exception:
        telemetry_log_to_csv = True
    telemetry_csv_path = None
    if telemetry_log_to_csv and output_folder:
        try:
            telemetry_csv_path = os.path.join(output_folder, "resource_telemetry.csv")
        except Exception:
            telemetry_csv_path = None
    telemetry = ResourceTelemetryController(
        enabled=telemetry_enabled,
        interval_sec=telemetry_interval,
        callback=progress_callback,
        csv_path=telemetry_csv_path,
        log_and_callback=_log_and_callback,
    )

    def _telemetry_context(extra: dict | None = None) -> dict:
        ctx: dict[str, Any] = {}
        plan_candidate = None
        try:
            plan_candidate = getattr(zconfig, "parallel_plan", None)
        except Exception:
            plan_candidate = None
        if plan_candidate is None:
            try:
                plan_candidate = worker_config_cache.get("parallel_plan")
            except Exception:
                plan_candidate = None
        if plan_candidate is not None:
            for attr in (
                "cpu_workers",
                "rows_per_chunk",
                "gpu_rows_per_chunk",
                "max_chunk_bytes",
                "gpu_max_chunk_bytes",
                "use_gpu",
                "use_gpu_phase5",
            ):
                value = None
                if hasattr(plan_candidate, attr):
                    try:
                        value = getattr(plan_candidate, attr)
                    except Exception:
                        value = None
                elif isinstance(plan_candidate, dict):
                    value = plan_candidate.get(attr)
                if value is not None:
                    ctx[attr] = value
        try:
            ctx.setdefault("use_gpu_phase5", bool(use_gpu_phase5))
        except Exception:
            pass
        if extra:
            ctx.update(extra)
        return ctx

    telemetry.maybe_emit_stats(_telemetry_context({"phase_name": "Init", "phase_index": 0}))


    resource_probe_info = _probe_system_resources(
        output_folder,
        two_pass_enabled=two_pass_coverage_renorm_config,
        two_pass_sigma_px=two_pass_cov_sigma_px_config,
        two_pass_gain_clip=two_pass_cov_gain_clip_config,
    )
    _emit_gpu_info_summary(progress_callback, resource_probe_info)
    two_pass_enabled = bool(resource_probe_info.get("two_pass_enabled", False))
    try:
        two_pass_sigma_px = int(resource_probe_info.get("two_pass_sigma_px", 50) or 50)
    except (TypeError, ValueError):
        two_pass_sigma_px = 50
    gain_clip_raw = resource_probe_info.get("two_pass_gain_clip")
    gain_clip_tuple: tuple[float, float]
    if isinstance(gain_clip_raw, (list, tuple)) and len(gain_clip_raw) >= 2:
        try:
            low = float(gain_clip_raw[0])
            high = float(gain_clip_raw[1])
            if low > high:
                low, high = high, low
            gain_clip_tuple = (low, high)
        except (TypeError, ValueError):
            gain_clip_tuple = (0.85, 1.18)
    else:
        gain_clip_tuple = (0.85, 1.18)
    try:
        if (
            isinstance(intertile_recenter_clip_config, (list, tuple))
            and len(intertile_recenter_clip_config) >= 2
        ):
            clip_low = float(intertile_recenter_clip_config[0])
            clip_high = float(intertile_recenter_clip_config[1])
            if clip_low > clip_high:
                clip_low, clip_high = clip_high, clip_low
            intertile_recenter_clip_tuple = (clip_low, clip_high)
        else:
            intertile_recenter_clip_tuple = (0.85, 1.18)
    except Exception:
        intertile_recenter_clip_tuple = (0.85, 1.18)
    auto_caps_info: dict | None = None
    auto_resource_strategy: dict = {}
    phase0_header_items: list[dict] = []
    phase0_lookup: dict[str, dict] = {}
    preplan_groups_override_paths: list[list[str]] | None = None
    intertile_match_flag = bool(intertile_photometric_match_config)
    match_background_flag = (
        True
        if match_background_for_final_config is None
        else bool(match_background_for_final_config)
    )
    feather_parity_flag = bool(incremental_feather_parity_config)
    tile_weighting_enabled_config = _coerce_bool_flag(enable_tile_weighting_config)
    if tile_weighting_enabled_config is None:
        tile_weighting_enabled_config = _coerce_bool_flag(worker_config_cache.get("enable_tile_weighting"))
    if tile_weighting_enabled_config is None:
        tile_weighting_enabled_config = True
    weight_mode_value = tile_weight_mode_config or worker_config_cache.get("tile_weight_mode") or "n_frames"
    tile_weight_mode_config = str(weight_mode_value).strip().lower() or "n_frames"
    if sds_mode_flag:
        tile_weighting_enabled_config = False

    try:
        if isinstance(intertile_sky_percentile_config, (list, tuple)) and len(intertile_sky_percentile_config) >= 2:
            intertile_sky_percentile_tuple = (
                float(intertile_sky_percentile_config[0]),
                float(intertile_sky_percentile_config[1]),
            )
        else:
            intertile_sky_percentile_tuple = (30.0, 70.0)
    except Exception:
        intertile_sky_percentile_tuple = (30.0, 70.0)

    def _normalize_path_for_matching(path_value: str | None) -> str | None:
        if not path_value:
            return None
        normalized = _normcase_path(Path(path_value).expanduser())
        return normalized or None


    # Seuil de clustering : valeur de repli à 0.05° si l'option est absente ou non positive
    try:
        cluster_threshold = float(cluster_threshold_config or 0)
    except (TypeError, ValueError):
        cluster_threshold = 0
    SEESTAR_STACK_CLUSTERING_THRESHOLD_DEG = (
        cluster_threshold if cluster_threshold > 0 else 0.05

    )
    # Orientation split threshold (degrees). 0 disables orientation filtering
    try:
        orientation_split_thr = float(cluster_orientation_split_deg_config or 0)
    except (TypeError, ValueError):
        orientation_split_thr = 0.0
    ORIENTATION_SPLIT_THRESHOLD_DEG = orientation_split_thr if orientation_split_thr > 0 else 0.0
    try:
        stack_ram_budget_gb = float(stack_ram_budget_gb_config or 0.0)
    except (TypeError, ValueError):
        stack_ram_budget_gb = 0.0
    STACK_RAM_BUDGET_BYTES = int(stack_ram_budget_gb * (1024 ** 3)) if stack_ram_budget_gb > 0 else 0
    PROGRESS_WEIGHT_PHASE1_RAW_SCAN = 30; PROGRESS_WEIGHT_PHASE2_CLUSTERING = 5
    PROGRESS_WEIGHT_PHASE3_MASTER_TILES = 35; PROGRESS_WEIGHT_PHASE4_GRID_CALC = 5
    PROGRESS_WEIGHT_PHASE4_5_INTER_MASTER = 6
    PROGRESS_WEIGHT_PHASE5_ASSEMBLY = 9; PROGRESS_WEIGHT_PHASE6_SAVE = 8
    PROGRESS_WEIGHT_PHASE7_CLEANUP = 2

    DEFAULT_PHASE_WORKER_RATIO = 1.0
    ALIGNMENT_PHASE_WORKER_RATIO = 1.0  # Phase 3 targets ~90% of logical cores while keeping one free

    if use_gpu_phase5 and gpu_id_phase5 is not None and CUPY_AVAILABLE:
        os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
        os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id_phase5)
        try:
            import cupy
            cupy.cuda.Device(0).use()
        except Exception as e:
            pcb(
                "run_error_gpu_init_failed",
                prog=None,
                lvl="ERROR",
                error=str(e),
            )
            use_gpu_phase5 = False
    else:
        for v in ("CUDA_VISIBLE_DEVICES", "CUDA_DEVICE_ORDER"):
            os.environ.pop(v, None)

    # Determine final GPU usage flag only if a valid NVIDIA GPU is selected
    use_gpu_phase5_flag = (
        use_gpu_phase5
        and gpu_id_phase5 is not None
        and CUPY_AVAILABLE
        and gpu_is_available()
    )
    if use_gpu_phase5_flag and ZEMOSAIC_UTILS_AVAILABLE and zemosaic_utils:
        try:
            # Initialize CuPy memory pools on the selected device (index 0 under the mask)
            if hasattr(zemosaic_utils, 'ensure_cupy_pool_initialized'):
                zemosaic_utils.ensure_cupy_pool_initialized(0)
        except Exception:
            pass
    if use_gpu_phase5_flag:
        _log_and_callback("phase5_using_gpu", callback=progress_callback, lvl="INFO")
    else:
        _log_and_callback("phase5_using_cpu", callback=progress_callback, lvl="INFO")
    def _cleanup_per_tile_cache(cache_paths: Iterable[str]) -> tuple[int, int]:
        """Remove preprocessed cache files for a completed master tile."""

        removed_count = 0
        removed_bytes = 0
        seen_paths: set[str] = set()

        for path in cache_paths or ():
            if path is None:
                continue
            try:
                path_obj = Path(os.fspath(path)).expanduser()
                try:
                    resolved = path_obj.resolve(strict=False)
                except Exception:
                    resolved = path_obj
                norm_path = str(resolved)
            except Exception:
                norm_path = None
            if not norm_path or norm_path in seen_paths:
                continue
            seen_paths.add(norm_path)
            if not norm_path.lower().endswith(".npy"):
                continue
            cache_path_obj = Path(norm_path)
            if not cache_path_obj.is_file():
                continue

            file_size = 0
            try:
                file_size = cache_path_obj.stat().st_size
            except OSError:
                file_size = 0

            try:
                cache_path_obj.unlink()
                removed_count += 1
                removed_bytes += file_size
                logger.debug("Removed per-tile cache file: %s", cache_path_obj)
            except FileNotFoundError:
                continue
            except OSError as exc_remove:
                logger.warning("Failed to remove per-tile cache file %s: %s", cache_path_obj, exc_remove)

        return removed_count, removed_bytes

    def _compute_phase_workers(base_workers: int, num_tasks: int, ratio: float = DEFAULT_PHASE_WORKER_RATIO) -> int:
        workers = max(1, int(base_workers * ratio))
        if num_tasks > 0:
            workers = min(workers, num_tasks)
        return max(1, workers)
    current_global_progress = 0
    
    error_messages_deps = []
    if not (ASTROPY_AVAILABLE and WCS and SkyCoord and Angle and fits and u): error_messages_deps.append("Astropy")
    if not (REPROJECT_AVAILABLE and find_optimal_celestial_wcs and reproject_and_coadd and reproject_interp): error_messages_deps.append("Reproject")
    if not (ZEMOSAIC_UTILS_AVAILABLE and zemosaic_utils): error_messages_deps.append("zemosaic_utils")
    if not (ZEMOSAIC_ASTROMETRY_AVAILABLE and zemosaic_astrometry): error_messages_deps.append("zemosaic_astrometry")
    if not (ZEMOSAIC_ALIGN_STACK_AVAILABLE and zemosaic_align_stack): error_messages_deps.append("zemosaic_align_stack")
    try: import psutil
    except ImportError: error_messages_deps.append("psutil")
    if error_messages_deps:
        pcb("run_error_critical_deps_missing", prog=None, lvl="ERROR", modules=", ".join(error_messages_deps)); return

    start_time_total_run = time.monotonic()
    pcb("CHRONO_START_REQUEST", prog=None, lvl="CHRONO_LEVEL")
    _log_memory_usage(progress_callback, "Début Run Hierarchical Mosaic")
    pcb("run_info_processing_started", prog=current_global_progress, lvl="INFO")
    _log_and_callback(
        (
            f"Options Stacking (Master Tiles): Norm='{stack_norm_method}', "
            f"Weight='{stack_weight_method}', Reject='{stack_reject_algo}', "
            f"Combine='{stack_final_combine}'"
        ),
        lvl="INFO",
        callback=progress_callback,
    )
    pcb(
        f"  Config ASTAP: Exe='{_safe_basename(astap_exe_path) if astap_exe_path else 'N/A'}', "
        f"Data='{_safe_basename(astap_data_dir_param) if astap_data_dir_param else 'N/A'}', "
        f"Radius={astap_search_radius_config}deg, Downsample={astap_downsample_config}, Sens={astap_sensitivity_config}",
        prog=None,
        lvl="DEBUG_DETAIL",
    )
    pcb(f"  Config Workers (GUI): Base demandé='{num_base_workers_config}' (0=auto)", prog=None, lvl="DEBUG_DETAIL")
    pcb(
        f"  Options Stacking (Master Tuiles): Norm='{stack_norm_method}', Weight='{stack_weight_method}', Reject='{stack_reject_algo}', "
        f"Combine='{stack_final_combine}', RGBEqualize={poststack_equalize_rgb_config}, RadialWeight={apply_radial_weight_config} "
        f"(Feather={radial_feather_fraction_config if apply_radial_weight_config else 'N/A'}, "
        f"Power={radial_shape_power_config if apply_radial_weight_config else 'N/A'}, "
        f"Floor={min_radial_weight_floor_config if apply_radial_weight_config else 'N/A'})",
        prog=None,
        lvl="DEBUG_DETAIL",
    )
    pcb(f"  Options Assemblage Final: Méthode='{final_assembly_method_config}'", prog=None, lvl="DEBUG_DETAIL")

    time_per_raw_file_wcs = None; time_per_master_tile_creation = None
    cache_dir_name = ".zemosaic_img_cache"
    temp_image_cache_dir = str(Path(output_folder).expanduser() / cache_dir_name)
    temp_master_tile_storage_dir: str | None = None
    try:
        if _path_exists(temp_image_cache_dir): shutil.rmtree(temp_image_cache_dir)
        os.makedirs(temp_image_cache_dir, exist_ok=True)
    except OSError as e_mkdir_cache:
        pcb("run_error_cache_dir_creation_failed", prog=None, lvl="ERROR", directory=temp_image_cache_dir, error=str(e_mkdir_cache)); return
    try:
        cache_probe = _probe_system_resources(
            temp_image_cache_dir,
            two_pass_enabled=two_pass_coverage_renorm_config,
            two_pass_sigma_px=two_pass_cov_sigma_px_config,
            two_pass_gain_clip=two_pass_cov_gain_clip_config,
        )
        for key, value in cache_probe.items():
            if value is not None:
                resource_probe_info[key] = value
    except Exception:
        pass

# --- Phase 1 (Prétraitement et WCS) ---
    base_progress_phase1 = current_global_progress
    _log_memory_usage(progress_callback, "Début Phase 1 (Prétraitement)")
    pcb("run_info_phase1_started_cache", prog=base_progress_phase1, lvl="INFO")
    pcb("PHASE_UPDATE:1", prog=None, lvl="ETA_LEVEL")
    
    fits_file_paths = []
    # Prépare des exclusions supplémentaires: dossier de sortie et WCS global
    try:
        _output_abs_path = Path(output_folder).expanduser().resolve() if output_folder else None
        _output_abs_norm = _normcase_path(_output_abs_path) if _output_abs_path else None
    except Exception:
        _output_abs_path = None
        _output_abs_norm = None
    try:
        _wcs_out_base = _safe_basename((worker_config_cache or {}).get("global_wcs_output_path", "global_mosaic_wcs.fits")).strip().lower()
    except Exception:
        _wcs_out_base = "global_mosaic_wcs.fits"
    # Scan des fichiers FITS dans le dossier d'entrée et ses sous-dossiers
    for root_dir_iter, dirnames_iter, files_in_dir_iter in os.walk(input_folder):
        root_path = Path(root_dir_iter)
        # Exclure les dossiers interdits dès la descente
        try:
            filtered_dirs = []
            for d in dirnames_iter:
                child = root_path / d
                # Exclusion via règle standard
                if is_path_excluded(child, EXCLUDED_DIRS):
                    continue
                # Exclure aussi le sous-arbre du dossier de sortie
                try:
                    if _output_abs_norm:
                        try:
                            child_norm = _normcase_path(child.resolve())
                        except Exception:
                            child_norm = _normcase_path(child)
                        if child_norm.startswith(_output_abs_norm):
                            continue
                except Exception:
                    pass
                filtered_dirs.append(d)
            dirnames_iter[:] = filtered_dirs
        except Exception:
            dirnames_iter[:] = [
                d
                for d in dirnames_iter
                if UNALIGNED_DIRNAME not in (root_path / d).parts
            ]

        # Assurer un ordre déterministe quelle que soit la plateforme/FS
        try:
            files_in_dir_iter = sorted(files_in_dir_iter, key=lambda s: s.lower())
        except Exception:
            files_in_dir_iter = list(files_in_dir_iter)

        for file_name_iter in files_in_dir_iter:
            if file_name_iter.lower().endswith((".fit", ".fits")):
                full_path = root_path / file_name_iter
                full_path_str = str(full_path)
                try:
                    if is_path_excluded(full_path, EXCLUDED_DIRS):
                        continue
                except Exception:
                    if UNALIGNED_DIRNAME in full_path.parts:
                        continue
                # Ignorer l'artefact du WCS global si présent dans l'arbre d'entrée
                try:
                    if _wcs_out_base and file_name_iter.strip().lower() == _wcs_out_base:
                        continue
                except Exception:
                    pass
                fits_file_paths.append(full_path_str)
    # Tri global déterministe
    try:
        fits_file_paths.sort(key=lambda p: p.lower())
    except Exception:
        fits_file_paths.sort()
    
    if not fits_file_paths: 
        pcb("run_error_no_fits_found_input", prog=current_global_progress, lvl="ERROR")
        return # Sortie anticipée si aucun fichier FITS n'est trouvé
    
    num_total_raw_files = len(fits_file_paths)
    pcb("run_info_found_potential_fits", prog=base_progress_phase1, lvl="INFO_DETAIL", num_files=num_total_raw_files)

    telemetry.maybe_emit_stats(
        _telemetry_context(
            {
                "phase_name": "Phase 1: Preprocessing",
                "phase_index": 1,
                "files_total": num_total_raw_files,
            }
        )
    )

    # --- Parallel auto-tune plan (optional) ---
    parallel_caps: ParallelCapabilities | None = None  # type: ignore[assignment]
    global_parallel_plan: ParallelPlan | None = None  # type: ignore[assignment]
    if PARALLEL_HELPERS_AVAILABLE:
        try:
            parallel_caps = detect_parallel_capabilities()
        except Exception as exc_parallel_caps:
            parallel_caps = None
            logger.warning("Parallel capability detection failed: %s", exc_parallel_caps)
        frame_h = 0
        frame_w = 0
        if isinstance(global_wcs_plan, dict):
            try:
                frame_h = int(global_wcs_plan.get("height") or 0)
            except Exception:
                frame_h = 0
            try:
                frame_w = int(global_wcs_plan.get("width") or 0)
            except Exception:
                frame_w = 0
        frame_shape = (frame_h, frame_w) if (frame_h > 0 and frame_w > 0) else (frame_h, frame_w)
        try:
            global_parallel_plan = auto_tune_parallel_plan(
                kind="global",
                frame_shape=frame_shape,
                n_frames=max(1, num_total_raw_files),
                bytes_per_pixel=4,
                config=worker_config_cache,
                caps=parallel_caps,
            )
            worker_config_cache["parallel_plan"] = global_parallel_plan
            setattr(zconfig, "parallel_plan", global_parallel_plan)
            if parallel_caps is not None:
                worker_config_cache["parallel_capabilities"] = parallel_caps
                setattr(zconfig, "parallel_capabilities", parallel_caps)
            try:
                pcb(
                    "parallel_plan_summary",
                    prog=None,
                    lvl="INFO_DETAIL",
                    cpu_workers=int(getattr(global_parallel_plan, "cpu_workers", 0)),
                    use_gpu=bool(getattr(global_parallel_plan, "use_gpu", False)),
                    rows=int(getattr(global_parallel_plan, "rows_per_chunk", 0) or 0),
                    gpu_rows=int(getattr(global_parallel_plan, "gpu_rows_per_chunk", 0) or 0),
                    memmap=bool(getattr(global_parallel_plan, "use_memmap", False)),
                    chunk_mb=float(
                        getattr(global_parallel_plan, "max_chunk_bytes", 0) / (1024 ** 2)
                        if getattr(global_parallel_plan, "max_chunk_bytes", 0)
                        else 0.0
                    ),
                )
            except Exception:
                pass
        except Exception as exc_parallel_plan:
            logger.warning("Parallel auto-tune failed: %s", exc_parallel_plan)
            global_parallel_plan = None

    # Kick off a stage progress stream so the GUI progress bar animates
    try:
        if progress_callback and callable(progress_callback):
            progress_callback("phase1_scan", 0, int(num_total_raw_files))
        # Also update a dedicated raw files counter in the GUI
        pcb(f"RAW_FILE_COUNT_UPDATE:0/{num_total_raw_files}", prog=None, lvl="ETA_LEVEL")
    except Exception:
        pass

    # --- Phase 0 (Header-only scan + early filter) ---
    # Preserve GUI-provided filter context arguments
    filter_invoked_arg = filter_invoked
    filter_overrides_arg = filter_overrides
    filtered_header_items_arg = filtered_header_items

    skip_filter_ui = bool(skip_filter_ui)
    # Resolve early filter enable policy: explicit argument takes precedence,
    # otherwise load from config, then apply skip_filter_ui override.
    if early_filter_enabled is None:
        early_filter_enabled = True
        try:
            if ZEMOSAIC_CONFIG_AVAILABLE and zemosaic_config:
                cfg0 = zemosaic_config.load_config() or {}
                early_filter_enabled = bool(cfg0.get("enable_early_filter", True))
        except Exception:
            early_filter_enabled = True
    else:
        early_filter_enabled = bool(early_filter_enabled)

    if skip_filter_ui:
        early_filter_enabled = False
        pcb("log_filter_ui_skipped", prog=None, lvl="INFO_DETAIL")

    if ASTROPY_AVAILABLE and fits is not None:
        header_items_for_filter: list[dict] = []
        filtered_items: list[dict] | None = None
        # If caller provided overrides or prior filter state, adopt them
        filter_overrides = filter_overrides_arg if isinstance(filter_overrides_arg, dict) else None
        filter_accepted = False
        filter_invoked = bool(filter_invoked_arg)
        streaming_filter_success = False

        launch_filter_interface_fn = None
        if early_filter_enabled:
            try:
                from zemosaic_filter_gui import launch_filter_interface as launch_filter_interface_fn  # type: ignore
            except ImportError:
                launch_filter_interface_fn = None
                pcb("Phase 0: filter GUI not available", prog=None, lvl="DEBUG_DETAIL")

        def _parse_filter_result(ret_obj):
            filt_items = None
            accepted_flag = False
            overrides_obj = None
            if isinstance(ret_obj, tuple) and len(ret_obj) >= 1:
                filt_items = ret_obj[0]
                if len(ret_obj) >= 2:
                    try:
                        accepted_flag = bool(ret_obj[1])
                    except Exception:
                        accepted_flag = False
                if len(ret_obj) >= 3:
                    overrides_obj = ret_obj[2]
            elif isinstance(ret_obj, list):
                filt_items = ret_obj
                accepted_flag = True
            return filt_items, accepted_flag, overrides_obj

        initial_filter_overrides = None
        try:
            initial_filter_overrides = {
                "cluster_panel_threshold": float(cluster_threshold_config),
                "cluster_target_groups": int(cluster_target_groups_config),
                "cluster_orientation_split_deg": float(cluster_orientation_split_deg_config),
            }
        except Exception:
            initial_filter_overrides = None

        # If the GUI already provided a filtered list, adopt it directly and
        # mark the streaming path as successful to avoid relaunching the UI.
        if isinstance(filtered_header_items_arg, list) and filtered_header_items_arg:
            try:
                header_items_for_filter = filtered_header_items_arg
            except Exception:
                header_items_for_filter = list(filtered_header_items_arg)
            filter_invoked = True
            filter_accepted = True
            streaming_filter_success = True
            try:
                filtered_items = list(header_items_for_filter)
            except Exception:
                filtered_items = header_items_for_filter

        solver_payload_for_filter = solver_settings if isinstance(solver_settings, dict) else None
        config_payload_for_filter = {
            "astap_executable_path": astap_exe_path,
            "astap_data_directory_path": astap_data_dir_param,
            "astap_default_search_radius": astap_search_radius_config,
            "astap_default_downsample": astap_downsample_config,
            "astap_default_sensitivity": astap_sensitivity_config,
        }

        if launch_filter_interface_fn is not None:
            try:
                filter_invoked = True
                filter_ret = launch_filter_interface_fn(
                    input_folder,
                    initial_filter_overrides,
                    stream_scan=True,
                    scan_recursive=True,
                    batch_size=200,
                    preview_cap=1500,
                    solver_settings_dict=solver_payload_for_filter,
                    config_overrides=config_payload_for_filter,
                )
                filtered_items, filter_accepted, filter_overrides = _parse_filter_result(filter_ret)
                # If the user cancelled from the filter UI, abort the run cleanly
                if isinstance(filter_overrides, dict) and filter_overrides.get("filter_cancelled"):
                    pcb("run_warn_phase0_filter_cancelled", prog=None, lvl="WARN")
                    pcb("log_key_processing_cancelled", prog=None, lvl="WARN")
                    return
                # In streaming mode the UI returns the final filtered list, not
                # the header pre-scan items. Consider the streaming path a success
                # whenever the UI was invoked without raising.
                streaming_filter_success = True
                if isinstance(filtered_items, list):
                    header_items_for_filter = filtered_items
                pcb(
                    "Phase 0: streaming filter UI completed",
                    prog=None,
                    lvl="INFO_DETAIL",
                )
            except Exception as e_filter:
                # If we fail to invoke the streaming UI, fall back to header scan.
                filter_invoked = False
                header_items_for_filter = []
                filtered_items = None
                filter_overrides = None
                filter_accepted = False
                pcb(f"Phase 0 streaming filter failed: {e_filter}", prog=None, lvl="WARN")

        if not streaming_filter_success:
            pcb("Phase 0: header scan start", prog=None, lvl="INFO_DETAIL")
            t0_hscan = time.monotonic()
            header_items_for_filter = []
            num_scanned = 0
            for idx_file, fpath in enumerate(fits_file_paths):
                hdr = None
                wcs0 = None
                shp_hw = None
                center_sc = None
                try:
                    hdr = fits.getheader(fpath, 0)
                    try:
                        nax1 = int(hdr.get("NAXIS1", 0))
                        nax2 = int(hdr.get("NAXIS2", 0))
                        if nax1 > 0 and nax2 > 0:
                            shp_hw = (nax2, nax1)
                    except Exception:
                        shp_hw = None
                    try:
                        w = WCS(hdr, naxis=2, relax=True) if WCS is not None else None
                        if w and getattr(w, "is_celestial", False):
                            wcs0 = w
                    except Exception:
                        wcs0 = None
                    if wcs0 is None:
                        try:
                            if (
                                ZEMOSAIC_ASTROMETRY_AVAILABLE
                                and zemosaic_astrometry
                                and hasattr(zemosaic_astrometry, "extract_center_from_header")
                            ):
                                center_sc = zemosaic_astrometry.extract_center_from_header(hdr)
                        except Exception:
                            center_sc = None
                    item = {
                        "path": fpath,
                        "header": hdr,
                        "index": idx_file,
                    }
                    if shp_hw:
                        item["shape"] = shp_hw
                    if wcs0 is not None:
                        item["wcs"] = wcs0
                    if center_sc is not None:
                        item["center"] = center_sc
                    header_items_for_filter.append(item)
                    num_scanned += 1
                except Exception:
                    header_items_for_filter.append({"path": fpath, "index": idx_file})
                    num_scanned += 1
            t1_hscan = time.monotonic()
            avg_t = (t1_hscan - t0_hscan) / max(1, num_scanned)
            pcb(
                f"Phase 0: header scan finished — files={num_scanned}, avg={avg_t:.4f}s/header",
                prog=None,
                lvl="DEBUG",
            )

            if launch_filter_interface_fn is not None and not filter_invoked:
                try:
                    filter_invoked = True
                    filter_ret = launch_filter_interface_fn(header_items_for_filter, initial_filter_overrides)
                    filtered_items, filter_accepted, filter_overrides = _parse_filter_result(filter_ret)
                    if isinstance(filter_overrides, dict) and filter_overrides.get("filter_cancelled"):
                        pcb("run_warn_phase0_filter_cancelled", prog=None, lvl="WARN")
                        pcb("log_key_processing_cancelled", prog=None, lvl="WARN")
                        return
                except Exception as e_filter:
                    filter_invoked = False
                    filtered_items = None
                    filter_overrides = None
                    filter_accepted = False
                    pcb(f"Phase 0 filter UI failed: {e_filter}", prog=None, lvl="WARN")
            elif not early_filter_enabled:
                pcb("Phase 0: header scan completed (filter UI disabled)", prog=None, lvl="DEBUG_DETAIL")

        phase0_header_items = header_items_for_filter

        if filter_invoked:
            if filter_overrides:
                try:
                    if "cluster_panel_threshold" in filter_overrides:
                        cluster_threshold_config = filter_overrides["cluster_panel_threshold"]
                        pcb(
                            "clusterstacks_info_override_threshold",
                            prog=None,
                            lvl="INFO_DETAIL",
                            value=cluster_threshold_config,
                        )
                    if "cluster_target_groups" in filter_overrides:
                        cluster_target_groups_config = filter_overrides["cluster_target_groups"]
                        pcb(
                            "clusterstacks_info_override_target_groups",
                            prog=None,
                            lvl="INFO_DETAIL",
                            value=cluster_target_groups_config,
                        )
                    if "cluster_orientation_split_deg" in filter_overrides:
                        cluster_orientation_split_deg_config = filter_overrides["cluster_orientation_split_deg"]
                        pcb(
                            "clusterstacks_info_override_orientation_split",
                            prog=None,
                            lvl="INFO_DETAIL",
                            value=cluster_orientation_split_deg_config,
                        )
                except Exception:
                    pass
                try:
                    raw_groups_override = (
                        filter_overrides.get("preplan_master_groups")
                        if isinstance(filter_overrides, dict)
                        else None
                    )
                    if isinstance(raw_groups_override, list):
                        mapped_groups: list[list[str]] = []
                        for group in raw_groups_override:
                            if not isinstance(group, (list, tuple)):
                                continue
                            normalized_group: list[str] = []
                            for item in group:
                                path_val = None
                                if isinstance(item, dict):
                                    path_val = item.get("path") or item.get("path_raw")
                                elif isinstance(item, str):
                                    path_val = item
                                norm_path = _normalize_path_for_matching(path_val)
                                if norm_path:
                                    normalized_group.append(norm_path)
                            if normalized_group:
                                mapped_groups.append(normalized_group)
                        if mapped_groups:
                            preplan_groups_override_paths = mapped_groups
                            pcb(
                                f"Phase 0 filter provided {len(mapped_groups)} preplanned group(s).",
                                prog=None,
                                lvl="INFO_DETAIL",
                            )
                except Exception as e_preplan:
                    pcb(
                        f"Phase 0 filter preplan override failed: {e_preplan}",
                        prog=None,
                        lvl="DEBUG_DETAIL",
                    )

            if not filter_accepted:
                pcb("run_warn_phase0_filter_cancelled", prog=None, lvl="WARN")
                pcb("Phase 0: filter cancelled -> proceeding with all files", prog=None, lvl="INFO_DETAIL")
            if filter_accepted and isinstance(filtered_items, list):
                new_paths = [
                    item.get("path")
                    for item in filtered_items
                    if isinstance(item, dict) and item.get("path")
                ]
                filtered_paths: list[str] = []
                for candidate_path in new_paths:
                    try:
                        if is_path_excluded(candidate_path, EXCLUDED_DIRS):
                            continue
                    except Exception:
                        if UNALIGNED_DIRNAME in _normpath_parts(candidate_path):
                            continue
                    filtered_paths.append(candidate_path)

                fits_file_paths = filtered_paths
                pcb(
                    f"Phase 0: selection after filter = {len(fits_file_paths)} files",
                    prog=None,
                    lvl="INFO_DETAIL",
                )
                if fits_file_paths:
                    try:
                        fits_file_paths.sort(key=lambda p: p.lower())
                    except Exception:
                        fits_file_paths.sort()
            elif filter_accepted and not filtered_items:
                pcb("Phase 0: filter returned no items", prog=None, lvl="WARN")
    else:
        phase0_header_items = []
        pcb("Phase 0: header scan unavailable (Astropy missing)", prog=None, lvl="WARN")

    phase0_lookup = {item["path"]: item for item in phase0_header_items if isinstance(item, dict) and item.get("path")}
    per_frame_info = _estimate_per_frame_cost_mb(phase0_header_items)
    auto_caps_info = _compute_auto_tile_caps(
        resource_probe_info,
        per_frame_info,
        policy_max=0,
        policy_min=8,
        user_max_override=int(max_raw_per_master_tile_config) if max_raw_per_master_tile_config else None,
    )
    try:
        msg = (
            "AutoCaps: per_frame≈{pf:.1f} MB, RAM_free≈{rf:.0f} MB → "
            "frames_by_ram={fbr}, cap={cap}, memmap={mm}, GPUHint={gpu}, parallel={par}".format(
                pf=auto_caps_info.get("per_frame_mb", 0.0),
                rf=resource_probe_info.get("ram_available_mb", 0.0) or 0.0,
                fbr=auto_caps_info.get("frames_by_ram", 0),
                cap=auto_caps_info.get("cap"),
                mm="on" if auto_caps_info.get("memmap") else "off",
                gpu=auto_caps_info.get("gpu_batch_hint") or "n/a",
                par=auto_caps_info.get("parallel_groups", 1),
            )
        )
        _log_and_callback(msg, prog=None, lvl="INFO_DETAIL", callback=progress_callback)
    except Exception:
        pass
    auto_resource_strategy = {
        "cap": auto_caps_info.get("cap"),
        "min_cap": auto_caps_info.get("min_cap"),
        "memmap": auto_caps_info.get("memmap"),
        "memmap_budget_mb": auto_caps_info.get("memmap_budget_mb"),
        "gpu_batch_hint": auto_caps_info.get("gpu_batch_hint"),
        "parallel_groups": auto_caps_info.get("parallel_groups"),
        "per_frame_mb": auto_caps_info.get("per_frame_mb"),
    }

    
    # --- Détermination du nombre de workers de BASE ---
    effective_base_workers = 0
    num_logical_processors = os.cpu_count() or 1 
    
    if num_base_workers_config <= 0: # Mode automatique (0 de la GUI)
        desired_auto_ratio = 0.75
        effective_base_workers = max(1, int(np.ceil(num_logical_processors * desired_auto_ratio)))
        pcb(f"WORKERS_CONFIG: Mode Auto. Base de workers calculée: {effective_base_workers} ({desired_auto_ratio*100:.0f}% de {num_logical_processors} processeurs logiques)", prog=None, lvl="INFO_DETAIL")
    else: # Mode manuel
        effective_base_workers = min(num_base_workers_config, num_logical_processors)
        if effective_base_workers < num_base_workers_config:
             pcb(f"WORKERS_CONFIG: Demande GUI ({num_base_workers_config}) limitée à {effective_base_workers} (total processeurs logiques: {num_logical_processors}).", prog=None, lvl="WARN")
        pcb(f"WORKERS_CONFIG: Mode Manuel. Base de workers: {effective_base_workers}", prog=None, lvl="INFO_DETAIL")
    
    if effective_base_workers <= 0: # Fallback
        effective_base_workers = 1
        pcb(f"WORKERS_CONFIG: AVERT - effective_base_workers était <= 0, forcé à 1.", prog=None, lvl="WARN")

    # Calcul du nombre de workers pour la Phase 1
    actual_num_workers_ph1 = _compute_phase_workers(
        effective_base_workers,
        num_total_raw_files,
        DEFAULT_PHASE_WORKER_RATIO,
    )
    pcb(
        f"WORKERS_PHASE1: Utilisation de {actual_num_workers_ph1} worker(s). (Base: {effective_base_workers}, Fichiers: {num_total_raw_files})",
        prog=None,
        lvl="INFO",
    )  # Log mis à jour pour plus de clarté
    
    start_time_phase1 = time.monotonic()
    all_raw_files_processed_info_dict = {} # Pour stocker les infos des fichiers traités avec succès
    files_processed_count_ph1 = 0      # Compteur pour les fichiers soumis au ThreadPoolExecutor

    with ThreadPoolExecutor(max_workers=actual_num_workers_ph1, thread_name_prefix="ZeMosaic_Ph1_") as executor_ph1:
        batch_size = 200
        for i in range(0, len(fits_file_paths), batch_size):
            batch = fits_file_paths[i:i+batch_size]
            future_to_filepath_ph1 = {
                executor_ph1.submit(
                    get_wcs_and_pretreat_raw_file,
                    f_path,
                    astap_exe_path,
                    astap_data_dir_param,
                    astap_search_radius_config,
                    astap_downsample_config,
                    astap_sensitivity_config,
                    180,
                    progress_callback,
                    temp_image_cache_dir,
                    solver_settings
                ): f_path for f_path in batch
            }

            for future in as_completed(future_to_filepath_ph1):
                file_path_original = future_to_filepath_ph1[future]
                files_processed_count_ph1 += 1  # Incrémenter pour chaque future terminée

                # Update GUI stage progress (files read / total)
                try:
                    if progress_callback and callable(progress_callback):
                        progress_callback("phase1_scan", int(files_processed_count_ph1), int(num_total_raw_files))
                    # Mirror the count so the GUI can show X/N files
                    pcb(f"RAW_FILE_COUNT_UPDATE:{files_processed_count_ph1}/{num_total_raw_files}", prog=None, lvl="ETA_LEVEL")
                except Exception:
                    pass

                telemetry.maybe_emit_stats(
                    _telemetry_context(
                        {
                            "phase_name": "Phase 1: Preprocessing",
                            "phase_index": 1,
                            "files_done": files_processed_count_ph1,
                            "files_total": num_total_raw_files,
                        }
                    )
                )

                prog_step_phase1 = base_progress_phase1 + int(
                    PROGRESS_WEIGHT_PHASE1_RAW_SCAN * (files_processed_count_ph1 / max(1, num_total_raw_files))
                )

                try:
                    # Récupérer le résultat de la tâche
                    img_data_adu, wcs_obj_solved, header_obj_updated, hp_mask_path = future.result()

                    # Si la tâche a réussi (ne retourne pas que des None)
                    if (
                        img_data_adu is not None
                        and wcs_obj_solved is not None
                        and header_obj_updated is not None
                    ):
                        # Sauvegarder les données prétraitées en .npy
                        cache_file_basename = f"preprocessed_{Path(file_path_original).stem}_{files_processed_count_ph1}.npy"
                        cached_image_path = Path(temp_image_cache_dir) / cache_file_basename
                        try:
                            np.save(str(cached_image_path), img_data_adu)
                        except Exception as e_save_npy:
                            pcb(
                                "run_error_phase1_save_npy_failed",
                                prog=prog_step_phase1,
                                lvl="ERROR",
                                filename=_safe_basename(file_path_original),
                                error=str(e_save_npy),
                            )
                            logger.error(f"Erreur sauvegarde NPY pour {file_path_original}:", exc_info=True)
                        else:
                            # Stocker les informations pour les phases suivantes
                            entry = {
                                'path_raw': file_path_original,
                                'path_preprocessed_cache': str(cached_image_path),
                                'path_hotpix_mask': hp_mask_path,
                                'wcs': wcs_obj_solved,
                                'header': header_obj_updated,
                                'preprocessed_shape': tuple(int(dim) for dim in getattr(img_data_adu, 'shape', []) or ()),
                            }
                            meta = phase0_lookup.get(file_path_original)
                            if isinstance(meta, dict):
                                if 'index' in meta:
                                    entry['phase0_index'] = meta.get('index')
                                if 'center' in meta:
                                    entry['phase0_center'] = meta.get('center')
                                if 'shape' in meta:
                                    entry['phase0_shape'] = meta.get('shape')
                                if 'wcs' in meta and 'wcs' not in entry:
                                    entry['phase0_wcs'] = meta.get('wcs')
                            all_raw_files_processed_info_dict[file_path_original] = entry
                        finally:
                            # Libérer la mémoire des données image dès que possible
                            del img_data_adu
                            gc.collect()
                    else:
                        # Le fichier a échoué (ex: WCS non résolu et déplacé)
                        # get_wcs_and_pretreat_raw_file a déjà loggué l'échec spécifique.
                        pcb(
                            "run_warn_phase1_wcs_pretreat_failed_or_skipped_thread",
                            prog=prog_step_phase1,
                            lvl="WARN",
                            filename=_safe_basename(file_path_original),
                        )
                        if img_data_adu is not None:
                            del img_data_adu
                            gc.collect()

                except Exception as exc_thread:
                    # Erreur imprévue dans la future elle-même
                    pcb(
                        "run_error_phase1_thread_exception",
                        prog=prog_step_phase1,
                        lvl="ERROR",
                        filename=_safe_basename(file_path_original),
                        error=str(exc_thread),
                    )
                    logger.error(
                        f"Exception non gérée dans le thread Phase 1 pour {file_path_original}:",
                        exc_info=True,
                    )

                # Log de mémoire et ETA
                if (
                    files_processed_count_ph1 % max(1, num_total_raw_files // 10) == 0
                    or files_processed_count_ph1 == num_total_raw_files
                ):
                    _log_memory_usage(
                        progress_callback,
                        f"Phase 1 - Traité {files_processed_count_ph1}/{num_total_raw_files}",
                    )

                elapsed_phase1 = time.monotonic() - start_time_phase1
                if files_processed_count_ph1 > 0:
                    time_per_raw_file_wcs = elapsed_phase1 / files_processed_count_ph1
                    eta_phase1_sec = (num_total_raw_files - files_processed_count_ph1) * time_per_raw_file_wcs
                    current_progress_in_run_percent = base_progress_phase1 + (
                        files_processed_count_ph1 / max(1, num_total_raw_files)
                    ) * PROGRESS_WEIGHT_PHASE1_RAW_SCAN
                    time_per_percent_point_global = (
                        (time.monotonic() - start_time_total_run) / max(1, current_progress_in_run_percent)
                        if current_progress_in_run_percent > 0
                        else (time.monotonic() - start_time_total_run)
                    )
                    total_eta_sec = eta_phase1_sec + (
                        100 - current_progress_in_run_percent
                    ) * time_per_percent_point_global
                    update_gui_eta(total_eta_sec)

    # Construire la liste finale des informations des fichiers traités avec succès
    all_raw_files_processed_info = [
        all_raw_files_processed_info_dict[fp] 
        for fp in fits_file_paths 
        if fp in all_raw_files_processed_info_dict
    ]
    
    if not all_raw_files_processed_info: 
        pcb("run_error_phase1_no_valid_raws_after_cache", prog=(base_progress_phase1 + PROGRESS_WEIGHT_PHASE1_RAW_SCAN), lvl="ERROR")
        return # Sortie anticipée si aucun fichier n'a pu être traité avec succès

    current_global_progress = base_progress_phase1 + PROGRESS_WEIGHT_PHASE1_RAW_SCAN
    _log_memory_usage(progress_callback, "Fin Phase 1 (Prétraitement)")
    pcb("run_info_phase1_finished_cache", prog=current_global_progress, lvl="INFO", num_valid_raws=len(all_raw_files_processed_info))
    # --- Optional interactive filtering between Phase 1 and Phase 2 ---
    try:
        raw_files_with_wcs = all_raw_files_processed_info
        try:
            raw_files_with_wcs = raw_files_with_wcs
            # Keep the same variable name used by subsequent phases
            all_raw_files_processed_info = raw_files_with_wcs
        except ImportError:
            # Optional module not present: silently skip
            pass
        except Exception as e_opt:
            logger.warning(f"Filtrage facultatif désactivé suite à une erreur : {e_opt}")
    except Exception as e_hook:
        # Any unexpected issue in the hook wrapper: continue unchanged
        logger.warning(f"Filtrage facultatif non appliqué: {e_hook}")
    if time_per_raw_file_wcs: 
        pcb(f"    Temps moyen/brute (P1): {time_per_raw_file_wcs:.2f}s", prog=None, lvl="DEBUG")

    # --- Phase 2 (Clustering) ---
    base_progress_phase2 = current_global_progress
    _log_memory_usage(progress_callback, "Début Phase 2 (Clustering)")
    pcb("run_info_phase2_started", prog=base_progress_phase2, lvl="INFO")
    pcb("PHASE_UPDATE:2", prog=None, lvl="ETA_LEVEL")
    # Use order-invariant connected-components clustering for robustness
    preplan_groups_active = False
    # If we successfully map preplanned group(s) from the Filter UI then
    # we enable a strict mode which prevents automatic re-splitting.
    preplan_groups_strict = False
    if preplan_groups_override_paths:
        try:
            path_lookup = {
                _normalize_path_for_matching(info.get("path_raw") or info.get("path")): info
                for info in all_raw_files_processed_info
                if isinstance(info, dict)
            }
            used_paths: set[str] = set()
            mapped_info_groups: list[list[dict]] = []
            missing_preplan: list[str] = []
            for group_paths in preplan_groups_override_paths:
                current_group: list[dict] = []
                for path_norm in group_paths:
                    if not path_norm:
                        continue
                    info = path_lookup.get(path_norm)
                    if info is not None:
                        current_group.append(info)
                        used_paths.add(path_norm)
                    else:
                        missing_preplan.append(path_norm)
                if current_group:
                    mapped_info_groups.append(current_group)
            if mapped_info_groups:
                leftovers = [
                    info
                    for info in all_raw_files_processed_info
                    if _normalize_path_for_matching(info.get("path_raw") or info.get("path")) not in used_paths
                ]
                if leftovers:
                    mapped_info_groups.append(leftovers)
                seestar_stack_groups = mapped_info_groups
                preplan_groups_active = True
                # Respect preplanned groups strictly: avoid any automatic
                # splitting or caps that would change the number of groups.
                preplan_groups_strict = True
                _log_and_callback(
                    f"Phase 2: using {len(mapped_info_groups)} preplanned group(s) from filter UI (strict mode).",
                    prog=None,
                    lvl="INFO_DETAIL",
                    callback=progress_callback,
                )
                if missing_preplan:
                    try:
                        preview = ", ".join(_safe_basename(p) for p in missing_preplan[:5] if p)
                    except Exception:
                        preview = ""
                    _log_and_callback(
                        "Phase 2: some preplanned paths were not found after preprocessing: "
                        + (preview if preview else str(len(missing_preplan))),
                        prog=None,
                        lvl="WARN",
                        callback=progress_callback,
                    )
        except Exception as e_preplan_map:
            _log_and_callback(
                f"Phase 2: failed to map preplanned groups ({e_preplan_map}). Falling back to clustering.",
                prog=None,
                lvl="WARN",
                callback=progress_callback,
            )
            preplan_groups_active = False

    if not preplan_groups_active:
        seestar_stack_groups = cluster_seestar_stacks_connected(
            all_raw_files_processed_info,
            SEESTAR_STACK_CLUSTERING_THRESHOLD_DEG,
            progress_callback,
            orientation_split_threshold_deg=ORIENTATION_SPLIT_THRESHOLD_DEG,
        )
        if STACK_RAM_BUDGET_BYTES > 0 and seestar_stack_groups:
            seestar_stack_groups, ram_budget_adjustments = _apply_ram_budget_to_groups(
                seestar_stack_groups,
                STACK_RAM_BUDGET_BYTES,
                float(SEESTAR_STACK_CLUSTERING_THRESHOLD_DEG),
                float(ORIENTATION_SPLIT_THRESHOLD_DEG),
            )
            for adj in ram_budget_adjustments:
                method = adj.get("method")
                if method == "recluster":
                    _log_and_callback(
                        "clusterstacks_warn_ram_budget_recluster",
                        prog=None,
                        lvl="WARN",
                        callback=progress_callback,
                        group_index=adj.get("group_index"),
                        original_frames=adj.get("original_frames"),
                        num_subgroups=adj.get("num_subgroups"),
                        new_threshold_deg=adj.get("new_threshold_deg"),
                        attempts=adj.get("attempts"),
                        estimated_mb=adj.get("estimated_mb"),
                        budget_mb=adj.get("budget_mb"),
                    )
                elif method == "split":
                    _log_and_callback(
                        "clusterstacks_warn_ram_budget_split",
                        prog=None,
                        lvl="WARN",
                        callback=progress_callback,
                        group_index=adj.get("group_index"),
                        original_frames=adj.get("original_frames"),
                        num_subgroups=adj.get("num_subgroups"),
                        segment_size=adj.get("segment_size"),
                        estimated_mb=adj.get("estimated_mb"),
                        budget_mb=adj.get("budget_mb"),
                    )
                    if adj.get("still_over_budget"):
                        _log_and_callback(
                            "clusterstacks_warn_ram_budget_split_still_over",
                            prog=None,
                            lvl="WARN",
                            callback=progress_callback,
                            group_index=adj.get("group_index"),
                            segment_size=adj.get("segment_size"),
                            budget_mb=adj.get("budget_mb"),
                        )
                elif method == "single_over_budget":
                    _log_and_callback(
                        "clusterstacks_warn_ram_budget_single_over",
                        prog=None,
                        lvl="WARN",
                        callback=progress_callback,
                        group_index=adj.get("group_index"),
                        estimated_mb=adj.get("estimated_mb"),
                        budget_mb=adj.get("budget_mb"),
                    )
    # Diagnostic: nearest-neighbor separation percentiles to help tune eps
    try:
        panel_centers_sky_dbg = []
        for info in all_raw_files_processed_info:
            wcs_obj = info.get("wcs")
            if not (wcs_obj and getattr(wcs_obj, "is_celestial", False)):
                continue
            try:
                if getattr(wcs_obj, "pixel_shape", None):
                    cx = wcs_obj.pixel_shape[0] / 2.0
                    cy = wcs_obj.pixel_shape[1] / 2.0
                    center_world = wcs_obj.pixel_to_world(cx, cy)
                elif hasattr(wcs_obj, "wcs") and hasattr(wcs_obj.wcs, "crval"):
                    center_world = SkyCoord(
                        ra=float(wcs_obj.wcs.crval[0]) * u.deg,
                        dec=float(wcs_obj.wcs.crval[1]) * u.deg,
                        frame="icrs",
                    )
                else:
                    continue
                panel_centers_sky_dbg.append(center_world)
            except Exception:
                continue
        if len(panel_centers_sky_dbg) >= 2:
            coords_dbg = SkyCoord(ra=[c.ra for c in panel_centers_sky_dbg], dec=[c.dec for c in panel_centers_sky_dbg], frame="icrs")
            try:
                _, sep_nn, _ = coords_dbg.match_to_catalog_sky(coords_dbg, nthneighbor=1)
                nn = np.asarray(sep_nn.deg, dtype=float)
                p10 = float(np.nanpercentile(nn, 10.0))
                p50 = float(np.nanpercentile(nn, 50.0))
                p90 = float(np.nanpercentile(nn, 90.0))
                _log_and_callback(
                    f"Cluster NN stats (deg): P10={p10:.4f}, P50={p50:.4f}, P90={p90:.4f}",
                    prog=None,
                    lvl="DEBUG_DETAIL",
                    callback=progress_callback,
                )
            except Exception:
                pass
    except Exception:
        pass
    # If clustering is pathologically conservative (almost one group per image),
    # auto-relax the threshold based on nearest-neighbor distances to avoid
    # producing hundreds of master tiles for tightly-dithered panels.
    try:
        total_inputs_for_cluster = len(all_raw_files_processed_info)
        groups_initial = len(seestar_stack_groups)
        if total_inputs_for_cluster > 2 and groups_initial >= max(3, int(0.9 * total_inputs_for_cluster)):
            # Compute a robust suggested threshold from the 90th percentile of
            # nearest-neighbor separations between panel centers.
            # Rebuild centers the same way as clustering helpers do.
            panel_centers_sky = []
            for info in all_raw_files_processed_info:
                wcs_obj = info.get("wcs")
                if not (wcs_obj and getattr(wcs_obj, "is_celestial", False)):
                    continue
                try:
                    if getattr(wcs_obj, "pixel_shape", None):
                        cx = wcs_obj.pixel_shape[0] / 2.0
                        cy = wcs_obj.pixel_shape[1] / 2.0
                        center_world = wcs_obj.pixel_to_world(cx, cy)
                    elif hasattr(wcs_obj, "wcs") and hasattr(wcs_obj.wcs, "crval"):
                        center_world = SkyCoord(
                            ra=float(wcs_obj.wcs.crval[0]) * u.deg,
                            dec=float(wcs_obj.wcs.crval[1]) * u.deg,
                            frame="icrs",
                        )
                    else:
                        continue
                    panel_centers_sky.append(center_world)
                except Exception:
                    continue

            if len(panel_centers_sky) >= 2:
                coords = SkyCoord(
                    ra=[c.ra for c in panel_centers_sky],
                    dec=[c.dec for c in panel_centers_sky],
                    frame="icrs",
                )
                try:
                    # Nearest neighbor (excluding self). Astropy handles wrap.
                    _, sep2d, _ = coords.match_to_catalog_sky(coords, nthneighbor=1)
                    nn_deg = np.asarray(sep2d.deg, dtype=float)
                    # Robust high-quantile of dithers; add a small headroom.
                    p90 = float(np.nanpercentile(nn_deg, 90.0)) if nn_deg.size else 0.0
                    # Propose a relaxed threshold within sane bounds.
                    thr_initial = float(SEESTAR_STACK_CLUSTERING_THRESHOLD_DEG)
                    thr_candidate = max(thr_initial, p90 * 1.2)
                    thr_candidate = float(min(max(thr_candidate, 0.01), 1.0))  # clamp 0.01°..1.0°

                    if thr_candidate > thr_initial:
                        _log_and_callback(
                            f"Cluster AUTO: threshold {thr_initial:.3f}° too conservative -> {groups_initial}/{total_inputs_for_cluster} groups.",
                            prog=None,
                            lvl="INFO_DETAIL",
                            callback=progress_callback,
                        )
                        _log_and_callback(
                            f"Cluster AUTO: relaxing to {thr_candidate:.3f}° (≈1.2×P90 NN={p90:.3f}°) and re-clustering...",
                            prog=None,
                            lvl="INFO_DETAIL",
                            callback=progress_callback,
                        )
                        seestar_stack_groups = cluster_seestar_stacks_connected(
                            all_raw_files_processed_info, thr_candidate, progress_callback
                        )
                        groups_after = len(seestar_stack_groups)
                        _log_and_callback(
                            f"Cluster AUTO: re-clustered into {groups_after} groups (was {groups_initial}).",
                            prog=None,
                            lvl="INFO_DETAIL",
                            callback=progress_callback,
                        )
                except Exception as e_auto_relax:
                    _log_and_callback(
                        f"Cluster AUTO: failed to compute NN-based relax: {e_auto_relax}",
                        prog=None,
                        lvl="DEBUG_DETAIL",
                        callback=progress_callback,
                    )
    except Exception as e_cluster_guard:
        _log_and_callback(
            f"Cluster AUTO: guard exception: {e_cluster_guard}", prog=None, lvl="DEBUG_DETAIL", callback=progress_callback
        )

    # Optional: drive clustering to a target number of groups by relaxing
    # the threshold via a bounded search. Disabled when target <= 0.
    try:
        target_groups = int(cluster_target_groups_config or 0)
    except Exception:
        target_groups = 0
    if (not preplan_groups_active) and target_groups > 0 and len(seestar_stack_groups) != target_groups:
        try:
            # Build coordinates array
            panel_centers_sky = []
            for info in all_raw_files_processed_info:
                wcs_obj = info.get("wcs")
                if not (wcs_obj and getattr(wcs_obj, "is_celestial", False)):
                    continue
                try:
                    if getattr(wcs_obj, "pixel_shape", None):
                        cx = wcs_obj.pixel_shape[0] / 2.0
                        cy = wcs_obj.pixel_shape[1] / 2.0
                        center_world = wcs_obj.pixel_to_world(cx, cy)
                    elif hasattr(wcs_obj, "wcs") and hasattr(wcs_obj.wcs, "crval"):
                        center_world = SkyCoord(
                            ra=float(wcs_obj.wcs.crval[0]) * u.deg,
                            dec=float(wcs_obj.wcs.crval[1]) * u.deg,
                            frame="icrs",
                        )
                    else:
                        continue
                    panel_centers_sky.append(center_world)
                except Exception:
                    continue

            if len(panel_centers_sky) >= 2:
                coords = SkyCoord(
                    ra=[c.ra for c in panel_centers_sky],
                    dec=[c.dec for c in panel_centers_sky],
                    frame="icrs",
                )
                # Establish an upper bound big enough that all panels connect
                # (max pairwise separation). Clamp to 5 degrees to avoid
                # pathological values.
                try:
                    sep_mat_deg = coords.separation(coords).deg
                    max_pair_deg = float(np.nanmax(sep_mat_deg)) if np.size(sep_mat_deg) else 0.5
                except Exception:
                    max_pair_deg = 0.5
                thr_current = float(SEESTAR_STACK_CLUSTERING_THRESHOLD_DEG)
                def _count_groups(thr: float) -> tuple[int, list]:
                    g = cluster_seestar_stacks_connected(
                        all_raw_files_processed_info,
                        float(thr),
                        None,
                        orientation_split_threshold_deg=ORIENTATION_SPLIT_THRESHOLD_DEG,
                    )
                    return len(g), g
                cnt_cur = len(seestar_stack_groups)
                # Direction: if too many groups, increase threshold; if too few, decrease.
                if cnt_cur > target_groups:
                    lo = thr_current
                    hi = float(min(max(max_pair_deg, lo * 2.0, 0.05), 5.0))
                    cnt_hi, groups_hi = _count_groups(hi)
                    # Expand hi until we get <= target (fewer groups) or cap
                    expand_iter = 0
                    while cnt_hi > target_groups and hi < 5.0 and expand_iter < 8:
                        hi = min(hi * 1.5 + 1e-6, 5.0)
                        cnt_hi, groups_hi = _count_groups(hi)
                        expand_iter += 1
                    best_thr = hi
                    best_groups = groups_hi
                    for _ in range(14):
                        mid = 0.5 * (lo + hi)
                        cnt_mid, groups_mid = _count_groups(mid)
                        if cnt_mid > target_groups:
                            lo = mid
                        else:
                            hi = mid
                            best_thr = mid
                            best_groups = groups_mid
                else:
                    # Need more groups ⇒ lower the threshold
                    hi = thr_current
                    lo = max(1e-6, hi / 2.0)
                    cnt_lo, groups_lo = _count_groups(lo)
                    shrink_iter = 0
                    while cnt_lo < target_groups and lo > 1e-6 and shrink_iter < 12:
                        hi = lo
                        lo = max(1e-6, lo / 1.5)
                        cnt_lo, groups_lo = _count_groups(lo)
                        shrink_iter += 1
                    best_thr = lo
                    best_groups = groups_lo
                    # Binary search upward to approach target from the high side (more stable)
                    for _ in range(14):
                        mid = 0.5 * (lo + hi)
                        cnt_mid, groups_mid = _count_groups(mid)
                        if cnt_mid < target_groups:
                            # still too few groups ⇒ lower threshold more
                            hi = mid
                        else:
                            lo = mid
                            best_thr = mid
                            best_groups = groups_mid
                _log_and_callback(
                    f"Cluster AUTO Target: threshold -> {best_thr:.4f}° for ≈{len(best_groups)} groups (target {target_groups}).",
                    prog=None,
                    lvl="INFO_DETAIL",
                    callback=progress_callback,
                )
                seestar_stack_groups = best_groups
        except Exception as e_target:
            _log_and_callback(
                f"Cluster AUTO Target: search failed: {e_target}", prog=None, lvl="DEBUG_DETAIL", callback=progress_callback
            )
    if not seestar_stack_groups:
        pcb("run_error_phase2_no_groups", prog=(base_progress_phase2 + PROGRESS_WEIGHT_PHASE2_CLUSTERING), lvl="ERROR")
        return
    auto_split_cap_value: int | None = None
    if (not preplan_groups_active) and auto_caps_info and seestar_stack_groups:
        try:
            cap_value = int(auto_caps_info.get("cap", 0))
            min_value = int(auto_caps_info.get("min_cap", 8))
        except Exception:
            cap_value = 0
            min_value = 8
        auto_split_cap_value = cap_value if cap_value > 0 else None
        if cap_value > 0:
            original_count = len(seestar_stack_groups)
            seestar_stack_groups = _auto_split_groups(
                seestar_stack_groups,
                cap_value,
                min_value,
                progress_callback=progress_callback,
            )
            if len(seestar_stack_groups) != original_count:
                try:
                    _log_and_callback(
                        f"AutoSplit summary: {original_count} -> {len(seestar_stack_groups)} subgroup(s) (cap={cap_value})",
                        prog=None,
                        lvl="INFO_DETAIL",
                        callback=progress_callback,
                    )
                except Exception:
                    pass
            if min_value > 0:
                seestar_stack_groups = _merge_small_groups(
                    seestar_stack_groups,
                    min_size=min_value,
                    cap=cap_value,
                )

    # Do not subdivide groups if a target group count is set; respect clustering first.
    if (
        not preplan_groups_active
        and (cluster_target_groups_config is None or int(cluster_target_groups_config) <= 0)
        and max_raw_per_master_tile_config
        and max_raw_per_master_tile_config > 0
    ):
        new_groups = []
        for g in seestar_stack_groups:
            for i in range(0, len(g), max_raw_per_master_tile_config):
                new_groups.append(g[i:i + max_raw_per_master_tile_config])
        if len(new_groups) != len(seestar_stack_groups):
            pcb(
                "clusterstacks_info_groups_split_manual_limit",
                prog=None,
                lvl="INFO_DETAIL",
                original=len(seestar_stack_groups),
                new=len(new_groups),
                limit=max_raw_per_master_tile_config,
            )
        seestar_stack_groups = new_groups
    overlap_cap = None
    if max_raw_per_master_tile_config and max_raw_per_master_tile_config > 0:
        overlap_cap = int(max_raw_per_master_tile_config)
    elif auto_split_cap_value:
        overlap_cap = int(auto_split_cap_value)
    if overlap_fraction_config > 0.0 and overlap_cap and seestar_stack_groups:
        overlapping_groups: list[list[dict]] = []
        effective_step = max(1, int(overlap_cap * (1.0 - overlap_fraction_config)))
        for group in seestar_stack_groups:
            ordered_group = _sort_group_for_overlap(group)
            batches_idx = make_overlapping_batches(list(range(len(ordered_group))), overlap_cap, overlap_fraction_config)
            if not batches_idx and ordered_group:
                batches_idx = [list(range(len(ordered_group)))]
            for batch_indices in batches_idx:
                batch = [ordered_group[i] for i in batch_indices if 0 <= i < len(ordered_group)]
                if batch:
                    overlapping_groups.append(batch)
        if overlapping_groups:
            pcb(
                "[Batching] cap overlap applied",
                prog=None,
                lvl="INFO_DETAIL",
                cap=int(overlap_cap),
                overlap=float(overlap_fraction_config),
                step=int(effective_step),
                batches=int(len(overlapping_groups)),
            )
            seestar_stack_groups = overlapping_groups
        else:
            pcb(
                "[Batching] overlap skipped (no groups produced)",
                prog=None,
                lvl="DEBUG_DETAIL",
                cap=int(overlap_cap),
                overlap=float(overlap_fraction_config),
            )
    cpu_total = os.cpu_count() or 1
    winsor_worker_limit = max(1, min(int(winsor_worker_limit_config), cpu_total))
    winsor_max_frames_per_pass = max(0, int(winsor_max_frames_per_pass_config))
    global_wcs_plan["winsor_worker_limit"] = int(winsor_worker_limit)
    global_wcs_plan["winsor_max_frames_per_pass"] = int(winsor_max_frames_per_pass)
    global_wcs_plan["use_align_helpers"] = True
    global_wcs_plan["prefer_gpu_helpers"] = bool(use_gpu_phase5_flag)
    pcb(
        f"Winsor worker limit set to {winsor_worker_limit}" + (
            " (ProcessPoolExecutor enabled)" if winsor_worker_limit > 1 else ""
        ),
        prog=None,
        lvl="INFO",
    )
    if winsor_max_frames_per_pass > 0:
        pcb(
            f"Winsor streaming limit set to {winsor_max_frames_per_pass} frame(s) per pass",
            prog=None,
            lvl="INFO_DETAIL",
        )
    sds_stack_params = {
        "stack_reject_algo": stack_reject_algo,
        "stack_weight_method": stack_weight_method,
        "stack_norm_method": stack_norm_method,
        "stack_kappa_low": stack_kappa_low,
        "stack_kappa_high": stack_kappa_high,
        "stack_final_combine": stack_final_combine,
        "parsed_winsor_limits": parsed_winsor_limits,
        "winsor_worker_limit": winsor_worker_limit,
        "winsor_max_frames_per_pass": winsor_max_frames_per_pass,
        "apply_radial_weight": apply_radial_weight_config,
        "radial_feather_fraction": radial_feather_fraction_config,
        "radial_shape_power": radial_shape_power_config,
        "poststack_equalize_rgb": poststack_equalize_rgb_config,
    }
    manual_limit = max_raw_per_master_tile_config
    memmap_streaming_enabled = bool(auto_caps_info and auto_caps_info.get("memmap"))
    allow_auto_limit = (
        auto_limit_frames_per_master_tile_config
        and (cluster_target_groups_config is None or int(cluster_target_groups_config) <= 0)
        and not memmap_streaming_enabled
    )
    if preplan_groups_strict and seestar_stack_groups:
        # A preplan came from the Filter UI and mapped successfully. In
        # strict mode we must not change the number or composition of the
        # groups the user specified — skip auto-limit splitting.
        _log_and_callback(
            f"Phase 2: preplanned groups present (strict mode) — skipping runtime auto-limit splitting (groups kept = {len(seestar_stack_groups)})",
            prog=None,
            lvl="INFO_DETAIL",
            callback=progress_callback,
        )
    elif allow_auto_limit and seestar_stack_groups:
        try:
            sample_path = None
            for group in seestar_stack_groups:
                if group:
                    sample_path = group[0].get('path_preprocessed_cache')
                    if sample_path:
                        break
            if sample_path is None:
                raise RuntimeError("auto-limit sample path unavailable")
            sample_arr = np.load(sample_path, mmap_mode='r')
            bytes_per_frame = sample_arr.nbytes
            sample_shape = sample_arr.shape
            sample_arr = None
            available_bytes = psutil.virtual_memory().available
            expected_workers = max(1, int(effective_base_workers * ALIGNMENT_PHASE_WORKER_RATIO))
            # Be more conservative: align/stack create extra buffers; use a larger safety factor
            limit = max(
                1,
                int(
                    available_bytes // (expected_workers * bytes_per_frame * 12)
                ),
            )
            # Clamp to a reasonable upper bound if no manual cap is set
            if manual_limit <= 0:
                limit = min(limit, 100)
            if manual_limit > 0:
                limit = min(limit, manual_limit)
            winsor_worker_limit = min(winsor_worker_limit, limit)
            new_groups = []
            for g in seestar_stack_groups:
                for i in range(0, len(g), limit):
                    new_groups.append(g[i:i+limit])
            if len(new_groups) != len(seestar_stack_groups):
                pcb(
                    "clusterstacks_info_groups_split_auto_limit",
                    prog=None,
                    lvl="INFO_DETAIL",
                    original=len(seestar_stack_groups),
                    new=len(new_groups),
                    limit=limit,
                    shape=str(sample_shape),
                )
            seestar_stack_groups = new_groups
            if manual_limit > 0 and limit != manual_limit:
                logger.info(
                    "Manual frame limit (%d) is lower than auto limit, using manual value.",
                    manual_limit,
                )
        except Exception as e_auto:
            pcb("clusterstacks_warn_auto_limit_failed", prog=None, lvl="WARN", error=str(e_auto))
    current_global_progress = base_progress_phase2 + PROGRESS_WEIGHT_PHASE2_CLUSTERING
    num_seestar_stacks_to_process = len(seestar_stack_groups)
    telemetry.maybe_emit_stats(
        _telemetry_context(
            {
                "phase_name": "Phase 2: Clustering",
                "phase_index": 2,
                "files_total": num_total_raw_files,
                "tiles_total": num_seestar_stacks_to_process,
            }
        )
    )
    _log_memory_usage(progress_callback, "Fin Phase 2"); pcb("run_info_phase2_finished", prog=current_global_progress, lvl="INFO", num_groups=num_seestar_stacks_to_process)


    # --- IO-aware adaptation (bench read speed on cache + write speed on output) ---
    io_read_mbps, io_write_mbps = None, None
    io_read_cat, io_write_cat = "unknown", "unknown"
    try:
        sample_cache_for_read = None
        # Try to pick a representative cached image path from the first group
        if seestar_stack_groups and seestar_stack_groups[0]:
            sample_cache_for_read = seestar_stack_groups[0][0].get('path_preprocessed_cache')
        if sample_cache_for_read and _path_exists(sample_cache_for_read):
            io_read_mbps = _measure_sequential_read_mbps(sample_cache_for_read)
            io_read_cat = _categorize_io_speed(io_read_mbps)
        # Write speed on output folder
        if output_folder and _path_isdir(output_folder):
            io_write_mbps = _measure_sequential_write_mbps(output_folder)
            io_write_cat = _categorize_io_speed(io_write_mbps)
        pcb(
            f"IO_BENCH: read {io_read_mbps:.1f} MB/s ({io_read_cat}), write {io_write_mbps:.1f} MB/s ({io_write_cat})"
            if (io_read_mbps is not None and io_write_mbps is not None)
            else f"IO_BENCH: read={io_read_mbps}, write={io_write_mbps}"
            ,
            prog=None,
            lvl="DEBUG",
        )
    except Exception as e_io_bench:
        pcb(f"IO_BENCH: failed ({e_io_bench})", prog=None, lvl="WARN")

    # Derive conservative caps from read speed (dominant in Phase 3) on Windows/slow disks
    io_ph3_cap = None
    io_cache_read_slots = None
    new_winsor_limit = winsor_worker_limit
    if os.name == 'nt':
        if io_read_cat == "very_slow":
            io_ph3_cap = 1
            io_cache_read_slots = 1
            new_winsor_limit = min(new_winsor_limit, 1)
        elif io_read_cat == "slow":
            io_ph3_cap = 2
            io_cache_read_slots = 1
            new_winsor_limit = min(new_winsor_limit, 1)
        elif io_read_cat == "medium":
            io_ph3_cap = 3
            io_cache_read_slots = 2
            new_winsor_limit = min(new_winsor_limit, 2)
        elif io_read_cat == "fast":
            io_ph3_cap = 4
            io_cache_read_slots = 2
            # Keep winsor limit as computed
        # Apply winsor limit adjustment if changed
        if new_winsor_limit != winsor_worker_limit:
            pcb(
                f"IO_ADAPT: winsor_worker_limit reduced {winsor_worker_limit} -> {new_winsor_limit} due to IO ({io_read_cat})",
                prog=None,
                lvl="INFO_DETAIL",
            )
            winsor_worker_limit = new_winsor_limit
        # Adjust cache IO semaphore (controls concurrent npy reads)
        try:
            if io_cache_read_slots and io_cache_read_slots > 0:
                global _CACHE_IO_SEMAPHORE
                _CACHE_IO_SEMAPHORE = threading.Semaphore(int(io_cache_read_slots))
                pcb(
                    f"IO_ADAPT: cache read slots set to {io_cache_read_slots}",
                    prog=None,
                    lvl="INFO_DETAIL",
                )
        except Exception:
            pass


    final_output_wcs = None
    final_output_shape_hw = None
    final_mosaic_data_HWC = None
    final_mosaic_coverage_HW = None
    final_alpha_map = None
    sds_fallback_logged = False
    alpha_final: np.ndarray | None = None
    master_tiles_results_list: list[tuple[str, Any]] = []
    final_quality_pipeline_cfg = {
        "quality_crop_enabled": bool(quality_crop_enabled_config),
        "quality_crop_band_px": int(quality_crop_band_px_config),
        "quality_crop_k_sigma": float(quality_crop_k_sigma_config),
        "quality_crop_margin_px": int(quality_crop_margin_px_config),
        "quality_crop_min_run": int(quality_crop_min_run_config),
        "altaz_cleanup_enabled": bool(altaz_cleanup_enabled_config),
        "altaz_margin_percent": float(altaz_margin_percent_config),
        "altaz_decay": float(altaz_decay_config),
        "altaz_nanize": bool(altaz_nanize_config),
    }

    global_anchor_shift: tuple[float, float] | None = None
    sds_runtime_tile_dir: str | None = None

    def _build_phase45_options_dict(base_progress: float) -> dict[str, Any]:
        stack_cfg_phase45 = {
            "kappa_low": float(stack_kappa_low),
            "kappa_high": float(stack_kappa_high),
            "winsor_limits": parsed_winsor_limits,
            "winsor_max_frames_per_pass": winsor_max_frames_per_pass_config,
            "winsor_worker_limit": winsor_worker_limit_config,
            "normalize_method": stack_norm_method,
            "stacking_normalize_method": stack_norm_method,
            "weight_method": stack_weight_method,
            "reject_algo": stack_reject_algo,
            "final_combine": stack_final_combine,
            "stack_norm_method": stack_norm_method,
            "stack_weight_method": stack_weight_method,
            "stack_reject_algo": stack_reject_algo,
            "stack_final_combine": stack_final_combine,
            "intertile_sky_percentile": intertile_sky_percentile_tuple,
            "quality_crop_enabled": bool(quality_crop_enabled_config),
            "quality_crop_band_px": int(quality_crop_band_px_config),
            "quality_crop_k_sigma": float(quality_crop_k_sigma_config),
            "quality_crop_margin_px": int(quality_crop_margin_px_config),
            "quality_crop_min_run": int(quality_crop_min_run_config),
            "altaz_cleanup_enabled": bool(altaz_cleanup_enabled_config),
            "altaz_margin_percent": float(altaz_margin_percent_config),
            "altaz_decay": float(altaz_decay_config),
            "altaz_nanize": bool(altaz_nanize_config),
        }
        return {
            "enable": bool(inter_master_merge_enable_config),
            "base_progress": base_progress,
            "progress_weight": PROGRESS_WEIGHT_PHASE4_5_INTER_MASTER,
            "overlap_threshold": inter_master_overlap_threshold_config,
            "min_group_size": inter_master_min_group_size_config,
            "stack_method": inter_master_stack_method_config,
            "memmap_policy": inter_master_memmap_policy_config,
            "local_scale": inter_master_local_scale_config,
            "max_group_size": inter_master_max_group_config,
            "worker_config": worker_config_cache,
            "stack_cfg": stack_cfg_phase45,
        }

    def _build_phase5_options_dict(base_progress: float, *, final_method: str | None = None) -> dict[str, Any]:
        current_parallel_plan = getattr(zconfig, "parallel_plan", worker_config_cache.get("parallel_plan"))
        target_method = final_method or final_assembly_method_config
        tile_weighting_allowed = bool(tile_weighting_enabled_config)
        if str(target_method or "").lower().strip() != "reproject_coadd":
            tile_weighting_allowed = False
        caps_candidate = None
        try:
            caps_candidate = getattr(zconfig, "parallel_capabilities")
        except Exception:
            caps_candidate = None
        if caps_candidate is None:
            try:
                caps_candidate = worker_config_cache.get("parallel_capabilities")
            except Exception:
                caps_candidate = None
        return {
            "base_progress": base_progress,
            "progress_weight": PROGRESS_WEIGHT_PHASE5_ASSEMBLY,
            "final_assembly_method": target_method,
            "apply_master_tile_crop": apply_master_tile_crop_config,
            "quality_crop_enabled": quality_crop_enabled_config,
            "master_tile_crop_percent": master_tile_crop_percent_config,
            "intertile_match_flag": intertile_match_flag,
            "match_background_flag": match_background_flag,
            "feather_parity_flag": feather_parity_flag,
            "two_pass_enabled": two_pass_enabled,
            "two_pass_sigma_px": two_pass_sigma_px,
            "two_pass_gain_clip": gain_clip_tuple,
            "two_pass_coverage_renorm": two_pass_coverage_renorm_config,
            "final_mosaic_rgb_equalize_enabled": bool(final_mosaic_rgb_equalize_enabled),
            "use_gpu_phase5": use_gpu_phase5_flag,
            "assembly_process_workers": assembly_process_workers_config,
            "intertile_preview_size": intertile_preview_size_config,
            "intertile_overlap_min": intertile_overlap_min_config,
            "intertile_sky_percentile": intertile_sky_percentile_tuple,
            "intertile_robust_clip_sigma": intertile_robust_clip_sigma_config,
            "intertile_global_recenter": intertile_global_recenter_config,
            "intertile_recenter_clip": intertile_recenter_clip_tuple,
            "use_auto_intertile": use_auto_intertile_config,
            "coadd_use_memmap": coadd_use_memmap_config,
            "coadd_memmap_dir": coadd_memmap_dir_config,
            "global_anchor_shift": global_anchor_shift,
            "parallel_plan": current_parallel_plan,
            "parallel_capabilities": caps_candidate,
            "telemetry": telemetry,
            "sds_mode": bool(sds_mode_flag),
            "tile_weighting_enabled": tile_weighting_allowed,
            "tile_weight_mode": tile_weight_mode_config,
        }

    def _ensure_plan_descriptor_loaded(plan: dict[str, Any]) -> None:
        if not plan.get("enabled"):
            return
        if plan.get("wcs") is not None and plan.get("width") and plan.get("height"):
            return
        fits_path = plan.get("fits_path")
        json_path = plan.get("json_path")
        descriptor_refresh = _load_global_wcs_descriptor_safe(fits_path, json_path)
        if descriptor_refresh:
            plan["descriptor"] = descriptor_refresh
            plan["wcs"] = descriptor_refresh.get("wcs")
            plan["width"] = descriptor_refresh.get("width")
            plan["height"] = descriptor_refresh.get("height")
            if not plan.get("meta"):
                meta_payload = descriptor_refresh.get("metadata")
                if isinstance(meta_payload, dict):
                    plan["meta"] = _coerce_to_builtin(meta_payload)

    def _plan_has_descriptor_fields(plan: dict[str, Any]) -> bool:
        if not plan.get("enabled"):
            return False
        if plan.get("wcs") is None:
            return False
        try:
            width_ok = int(plan.get("width") or 0) > 0
            height_ok = int(plan.get("height") or 0) > 0
        except Exception:
            width_ok = height_ok = False
        return width_ok and height_ok

    def _disable_invalid_plan(reason: str, *, emit_warn: bool) -> None:
        if not global_wcs_plan.get("enabled"):
            return
        if emit_warn:
            pcb("sds_warn_runtime_wcs_failed", prog=None, lvl="WARN", error=reason)
        global_wcs_plan["enabled"] = False
        global_wcs_plan["wcs"] = None
        global_wcs_plan["width"] = None
        global_wcs_plan["height"] = None

    sds_post_context: dict[str, Any] = {}
    plan_mode_value = str(global_wcs_plan.get("mode") or "").strip().lower()
    need_global_plan = bool(plan_mode_value == "seestar")

    _ensure_plan_descriptor_loaded(global_wcs_plan)
    if global_wcs_plan.get("enabled") and not _plan_has_descriptor_fields(global_wcs_plan):
        _disable_invalid_plan("global WCS descriptor missing metadata", emit_warn=(sds_mode_flag or need_global_plan))

    if (sds_mode_flag or need_global_plan) and not global_wcs_plan.get("enabled"):
        built = _runtime_build_global_wcs_plan(
            seestar_groups=seestar_stack_groups,
            global_plan=global_wcs_plan,
            worker_config=worker_config_cache,
            output_dir=output_folder,
            pcb=pcb,
        )
        if built:
            _ensure_plan_descriptor_loaded(global_wcs_plan)
        if global_wcs_plan.get("enabled") and not _plan_has_descriptor_fields(global_wcs_plan):
            _disable_invalid_plan("runtime WCS descriptor incomplete", emit_warn=True)

    plan_width: int | None = None
    plan_height: int | None = None
    if global_wcs_plan.get("enabled"):
        try:
            plan_width = int(global_wcs_plan.get("width") or 0)
            plan_height = int(global_wcs_plan.get("height") or 0)
        except Exception:
            plan_width = plan_height = None
        if not sds_mode_flag:
            # SDS is OFF: skip mega-tile flows and force the classic master-tile pipeline.
            final_mosaic_data_HWC = None
            final_mosaic_coverage_HW = None
            final_alpha_map = None
            pcb("sds_off_classic_mastertile_pipeline", prog=None, lvl="INFO")
        else:
            pcb("sds_on_mega_tile_pipeline", prog=None, lvl="INFO")
            (
                sds_mosaic_data_HWC,
                sds_coverage_HW,
                sds_alpha_map,
            ) = assemble_global_mosaic_sds(
                seestar_stack_groups,
                global_plan=global_wcs_plan,
                progress_callback=progress_callback,
                match_background=match_background_flag,
                base_progress_phase=base_progress_phase2 + PROGRESS_WEIGHT_PHASE2_CLUSTERING,
                progress_weight_phase=(
                    PROGRESS_WEIGHT_PHASE3_MASTER_TILES
                    + PROGRESS_WEIGHT_PHASE4_GRID_CALC
                    + PROGRESS_WEIGHT_PHASE4_5_INTER_MASTER
                    + PROGRESS_WEIGHT_PHASE5_ASSEMBLY
                ),
                start_time_total_run=start_time_total_run,
                cache_root=output_folder,
                stack_params=sds_stack_params,
                coverage_threshold=sds_coverage_threshold_config,
                min_batch_size=sds_min_batch_size_config,
                target_batch_size=sds_target_batch_size_config,
                preplan_path_groups=preplan_groups_override_paths,
                postprocess_context=sds_post_context,
                parallel_plan=getattr(zconfig, "parallel_plan", worker_config_cache.get("parallel_plan")),
            )
            sds_tile_records = list(sds_post_context.pop("sds_tile_records", []) or [])
            temp_dir_candidate = sds_post_context.pop("sds_tile_temp_dir", None)
            if temp_dir_candidate:
                sds_runtime_tile_dir = temp_dir_candidate
            sds_two_pass_tile_pairs = sds_post_context.pop("two_pass_tile_pairs", None)

            sds_polish_succeeded = False
            if sds_mosaic_data_HWC is not None:
                final_output_wcs = global_wcs_plan.get("wcs")
                target_hw = (
                    (sds_mosaic_data_HWC.shape[0], sds_mosaic_data_HWC.shape[1])
                    if hasattr(sds_mosaic_data_HWC, "shape")
                    else None
                )
                pcb("phase5_sds_polish_start", prog=None, lvl="INFO")
                if logger:
                    logger.info("[SDS] Phase 5 polish on global SDS mosaic (skipping master tiles)")
                final_mosaic_data_HWC, final_mosaic_coverage_HW, alpha_final = _finalize_sds_global_mosaic(
                    sds_mosaic_data_HWC,
                    sds_coverage_HW,
                    zconfig=zconfig,
                    pcb=pcb,
                    sds_config={"min_coverage_keep": sds_min_coverage_keep_config},
                    collected_tiles=sds_two_pass_tile_pairs,
                    final_output_wcs=final_output_wcs,
                    final_output_shape_hw=target_hw,
                    pipeline_cfg=final_quality_pipeline_cfg,
                    enable_lecropper_pipeline=bool(
                        final_quality_pipeline_cfg.get("quality_crop_enabled")
                        or final_quality_pipeline_cfg.get("altaz_cleanup_enabled")
                    ),
                    enable_master_tile_crop=bool(apply_master_tile_crop_config and not quality_crop_enabled_config),
                    master_tile_crop_percent=float(master_tile_crop_percent_config),
                    two_pass_enabled=bool(two_pass_enabled),
                    two_pass_sigma_px=int(two_pass_sigma_px),
                    two_pass_gain_clip=gain_clip_tuple,
                    use_gpu_two_pass=use_gpu_phase5_flag,
                    enable_autocrop=bool(global_wcs_autocrop_enabled_config),
                    autocrop_margin_px=global_wcs_autocrop_margin_px_config,
                    global_plan=global_wcs_plan,
                    fallback_two_pass_loader=None,
                    parallel_plan=getattr(zconfig, "parallel_plan", worker_config_cache.get("parallel_plan")),
                )
                final_alpha_map = alpha_final
                if final_mosaic_data_HWC is not None:
                    final_output_shape_hw = (final_mosaic_data_HWC.shape[0], final_mosaic_data_HWC.shape[1])
                    final_output_wcs = global_wcs_plan.get("wcs")
                    current_global_progress = min(
                        100.0,
                        base_progress_phase2
                        + PROGRESS_WEIGHT_PHASE2_CLUSTERING
                        + PROGRESS_WEIGHT_PHASE3_MASTER_TILES
                        + PROGRESS_WEIGHT_PHASE4_GRID_CALC
                        + PROGRESS_WEIGHT_PHASE4_5_INTER_MASTER
                        + PROGRESS_WEIGHT_PHASE5_ASSEMBLY,
                    )
                    seestar_stack_groups = []
                    sds_polish_succeeded = True
                    pcb("sds_global_finalize_done", prog=None, lvl="INFO", has_alpha=alpha_final is not None)
                else:
                    final_mosaic_coverage_HW = None
                    final_alpha_map = None
            if not sds_polish_succeeded:
                if not sds_tile_records:
                    pcb("sds_failed_fallback_mosaic_first", prog=None, lvl="WARN")
                    mosaic_result = assemble_global_mosaic_first(
                        seestar_stack_groups,
                        global_plan=global_wcs_plan,
                        progress_callback=progress_callback,
                        match_background=match_background_flag,
                        base_progress_phase=base_progress_phase2 + PROGRESS_WEIGHT_PHASE2_CLUSTERING,
                        progress_weight_phase=(
                            PROGRESS_WEIGHT_PHASE3_MASTER_TILES
                            + PROGRESS_WEIGHT_PHASE4_GRID_CALC
                            + PROGRESS_WEIGHT_PHASE4_5_INTER_MASTER
                            + PROGRESS_WEIGHT_PHASE5_ASSEMBLY
                        ),
                        start_time_total_run=start_time_total_run,
                        cache_root=output_folder,
                        parallel_plan=getattr(zconfig, "parallel_plan", worker_config_cache.get("parallel_plan")),
                    ) or (None, None, None)
                    final_mosaic_data_HWC, final_mosaic_coverage_HW, final_alpha_map = mosaic_result
                    if final_mosaic_data_HWC is not None:
                        final_output_wcs = global_wcs_plan.get("wcs")
                        target_hw = (
                            (final_mosaic_data_HWC.shape[0], final_mosaic_data_HWC.shape[1])
                            if hasattr(final_mosaic_data_HWC, "shape")
                            else None
                        )
                        final_mosaic_data_HWC, final_mosaic_coverage_HW, alpha_final = _finalize_sds_global_mosaic(
                            final_mosaic_data_HWC,
                            final_mosaic_coverage_HW,
                            zconfig=zconfig,
                            pcb=pcb,
                            sds_config={"min_coverage_keep": sds_min_coverage_keep_config},
                            collected_tiles=None,
                            final_output_wcs=final_output_wcs,
                            final_output_shape_hw=target_hw,
                            pipeline_cfg=final_quality_pipeline_cfg,
                            enable_lecropper_pipeline=bool(
                                final_quality_pipeline_cfg.get("quality_crop_enabled")
                                or final_quality_pipeline_cfg.get("altaz_cleanup_enabled")
                            ),
                            enable_master_tile_crop=bool(apply_master_tile_crop_config and not quality_crop_enabled_config),
                            master_tile_crop_percent=float(master_tile_crop_percent_config),
                            two_pass_enabled=bool(two_pass_enabled),
                            two_pass_sigma_px=int(two_pass_sigma_px),
                            two_pass_gain_clip=gain_clip_tuple,
                            use_gpu_two_pass=use_gpu_phase5_flag,
                            enable_autocrop=bool(global_wcs_autocrop_enabled_config),
                            autocrop_margin_px=global_wcs_autocrop_margin_px_config,
                            global_plan=global_wcs_plan,
                            fallback_two_pass_loader=None,
                            parallel_plan=getattr(zconfig, "parallel_plan", worker_config_cache.get("parallel_plan")),
                        )
                        final_alpha_map = alpha_final
                        if final_mosaic_data_HWC is not None:
                            final_output_wcs = global_wcs_plan.get("wcs")
                            final_output_shape_hw = (
                                final_mosaic_data_HWC.shape[0],
                                final_mosaic_data_HWC.shape[1],
                            )
                            current_global_progress = min(
                                100.0,
                                base_progress_phase2
                                + PROGRESS_WEIGHT_PHASE2_CLUSTERING
                                + PROGRESS_WEIGHT_PHASE3_MASTER_TILES
                                + PROGRESS_WEIGHT_PHASE4_GRID_CALC
                                + PROGRESS_WEIGHT_PHASE4_5_INTER_MASTER
                                + PROGRESS_WEIGHT_PHASE5_ASSEMBLY,
                            )
                            seestar_stack_groups = []
                            sds_polish_succeeded = True
                            pcb("sds_global_finalize_done", prog=None, lvl="INFO", has_alpha=alpha_final is not None)
                    if not sds_polish_succeeded:
                        if not sds_fallback_logged:
                            pcb("sds_and_mosaic_first_failed_fallback_mastertiles", prog=None, lvl="WARN")
                            sds_fallback_logged = True
                        pcb("global_coadd_error_failed_fallback", prog=None, lvl="WARN")
                        global_wcs_plan["enabled"] = False
                else:
                    target_height = plan_height if isinstance(plan_height, int) and plan_height > 0 else None
                    target_width = plan_width if isinstance(plan_width, int) and plan_width > 0 else None
                    if (target_height is None or target_width is None) and sds_tile_records:
                        probe_path = sds_tile_records[0][0]
                        try:
                            with fits.open(probe_path, memmap=False) as hdul_probe:
                                data_shape = hdul_probe[0].shape if hdul_probe and hdul_probe[0] is not None else None
                                if data_shape and len(data_shape) >= 2:
                                    target_height = int(data_shape[0])
                                    target_width = int(data_shape[1])
                        except Exception:
                            pass
                    target_shape_hw = None
                    if target_height and target_width:
                        target_shape_hw = (target_height, target_width)
                    phase45_base = (
                        base_progress_phase2
                        + PROGRESS_WEIGHT_PHASE2_CLUSTERING
                        + PROGRESS_WEIGHT_PHASE3_MASTER_TILES
                        + PROGRESS_WEIGHT_PHASE4_GRID_CALC
                    )
                    sds_phase45_options = _build_phase45_options_dict(phase45_base)
                    sds_phase5_options = _build_phase5_options_dict(
                        phase45_base + PROGRESS_WEIGHT_PHASE4_5_INTER_MASTER
                    )
                    telemetry.maybe_emit_stats(
                        _telemetry_context(
                            {
                                "phase_name": "Phase 5: Assembly",
                                "phase_index": 5,
                                "tiles_total": len(sds_tile_records),
                            }
                        )
                    )
                    (
                        master_tiles_results_list,
                        final_mosaic_data_HWC,
                        final_mosaic_coverage_HW,
                        final_alpha_map,
                        alpha_final,
                        current_global_progress,
                    ) = _run_shared_phase45_phase5_pipeline(
                        sds_tile_records,
                        final_output_wcs=global_wcs_plan.get("wcs"),
                        final_output_shape_hw=target_shape_hw,
                        temp_master_tile_storage_dir=sds_runtime_tile_dir,
                        output_folder=output_folder,
                        cache_retention_mode=cache_retention_mode,
                        phase45_options=sds_phase45_options,
                        phase5_options=sds_phase5_options,
                        final_quality_pipeline_cfg=final_quality_pipeline_cfg,
                        start_time_total_run=start_time_total_run,
                        progress_callback=progress_callback,
                        pcb=pcb,
                        zconfig=zconfig,
                        logger=logger,
                    )
                    master_tiles_results_list = list(sds_tile_records)
                    autocrop_meta = None
                    if final_mosaic_data_HWC is not None and global_wcs_autocrop_enabled_config:
                        final_mosaic_data_HWC, final_mosaic_coverage_HW, final_alpha_map, autocrop_meta = (
                            _auto_crop_global_mosaic_if_requested(
                                final_mosaic_data_HWC,
                                final_mosaic_coverage_HW,
                                final_alpha_map,
                                enable_autocrop=True,
                                margin_px=global_wcs_autocrop_margin_px_config,
                                pcb=pcb,
                            )
                        )
                        if autocrop_meta:
                            _apply_autocrop_to_global_plan(global_wcs_plan, autocrop_meta)
                    if final_mosaic_data_HWC is not None:
                        final_output_wcs = global_wcs_plan.get("wcs")
                        final_output_shape_hw = (final_mosaic_data_HWC.shape[0], final_mosaic_data_HWC.shape[1])
                        alpha_final = _derive_final_alpha_mask(
                            final_alpha_map,
                            final_mosaic_data_HWC,
                            final_mosaic_coverage_HW,
                            logger,
                        )
                        seestar_stack_groups = []
                    else:
                        if not sds_fallback_logged:
                            pcb("sds_and_mosaic_first_failed_fallback_mastertiles", prog=None, lvl="WARN")
                            sds_fallback_logged = True
                        pcb("global_coadd_error_failed_fallback", prog=None, lvl="WARN")
                        global_wcs_plan["enabled"] = False

    try:
        setattr(zconfig, "winsor_worker_limit", int(winsor_worker_limit))
    except Exception:
        pass
    try:
        setattr(zconfig, "winsor_max_frames_per_pass", int(winsor_max_frames_per_pass))
    except Exception:
        pass



    if final_mosaic_data_HWC is None:
            if sds_mode_flag and not sds_fallback_logged:
                pcb("sds_and_mosaic_first_failed_fallback_mastertiles", prog=None, lvl="WARN")
                sds_fallback_logged = True
            # --- Phase 3 (Création Master Tuiles) ---
            base_progress_phase3 = current_global_progress
            _log_memory_usage(progress_callback, "Début Phase 3 (Master Tuiles)")
            pcb("run_info_phase3_started_from_cache", prog=base_progress_phase3, lvl="INFO")
            pcb("PHASE_UPDATE:3", prog=None, lvl="ETA_LEVEL")
            telemetry.maybe_emit_stats(
                _telemetry_context(
                    {
                        "phase_name": "Phase 3: Master Tiles",
                        "phase_index": 3,
                        "tiles_total": num_seestar_stacks_to_process,
                    }
                )
            )
            temp_master_tile_storage_dir = str(Path(output_folder).expanduser() / "zemosaic_temp_master_tiles")
            try:
                if _path_exists(temp_master_tile_storage_dir): shutil.rmtree(temp_master_tile_storage_dir)
                os.makedirs(temp_master_tile_storage_dir, exist_ok=True)
            except OSError as e_mkdir_mt: 
                pcb("run_error_phase3_mkdir_failed", prog=current_global_progress, lvl="ERROR", directory=temp_master_tile_storage_dir, error=str(e_mkdir_mt)); return
                
            master_tiles_results_list_temp = {}
            start_time_phase3 = time.monotonic()

            tile_id_order = list(range(len(seestar_stack_groups)))
            center_out_context: CenterOutNormalizationContext | None = None
            global_anchor_shift = (1.0, 0.0)
            prestack_anchor_tile_id: int | None = None
            center_out_settings = {
                "enabled": bool(center_out_normalization_p3_config),
                "sky_percentile": tuple((p3_center_sky_percentile_config or (25.0, 60.0))[:2]) if isinstance(p3_center_sky_percentile_config, (list, tuple)) else (25.0, 60.0),
                "clip_sigma": float(p3_center_robust_clip_sigma_config),
                "preview_size": int(p3_center_preview_size_config),
                "min_overlap_fraction": float(p3_center_min_overlap_fraction_config),
            }
            anchor_mode_value = str(center_out_anchor_mode_config or "auto_central_quality").strip()
            anchor_mode_lower = anchor_mode_value.lower()
            anchor_quality_settings = {
                "probe_limit": anchor_quality_probe_limit_config,
                "span_range": anchor_quality_span_range_config,
                "median_clip_sigma": anchor_quality_median_clip_sigma_config,
            }
            try:
                anchor_crop_band = max(4, int(quality_crop_band_px_config))
            except Exception:
                anchor_crop_band = 32
            try:
                anchor_crop_margin = max(0, int(quality_crop_margin_px_config))
            except Exception:
                anchor_crop_margin = 8
            try:
                anchor_crop_sigma = float(quality_crop_k_sigma_config)
                if not math.isfinite(anchor_crop_sigma):
                    raise ValueError
            except Exception:
                anchor_crop_sigma = 2.0
            anchor_crop_sigma = max(0.1, min(anchor_crop_sigma, 10.0))
            anchor_crop_settings = {
                "enabled": bool(ANCHOR_AUTOCROP_AVAILABLE and anchor_mode_lower == "auto_central_quality"),
                "band_px": anchor_crop_band,
                "margin_px": anchor_crop_margin,
                "k_sigma": anchor_crop_sigma,
            }
            if center_out_settings["enabled"] and seestar_stack_groups:
                order_info = _compute_center_out_order(seestar_stack_groups)
                distances = {}
                global_center_coord = None
                if order_info:
                    ordered_indices, global_center_coord, distances = order_info
                    try:
                        seestar_stack_groups = [seestar_stack_groups[i] for i in ordered_indices]
                        tile_id_order = ordered_indices
                    except Exception:
                        tile_id_order = list(range(len(seestar_stack_groups)))
                else:
                    distances = {}
                    global_center_coord = None

                anchor_original_id: int | None = tile_id_order[0] if tile_id_order else None
                if tile_id_order and anchor_mode_lower == "auto_central_quality":
                    selected_anchor = _select_quality_anchor(
                        tile_id_order,
                        distances,
                        seestar_stack_groups,
                        anchor_quality_settings,
                        center_out_settings,
                        anchor_crop_settings,
                        progress_callback,
                    )
                    if selected_anchor is not None and selected_anchor in tile_id_order:
                        if selected_anchor != tile_id_order[0]:
                            try:
                                sel_index = tile_id_order.index(selected_anchor)
                                tile_id_order.insert(0, tile_id_order.pop(sel_index))
                                seestar_stack_groups.insert(0, seestar_stack_groups.pop(sel_index))
                            except Exception:
                                pass
                        anchor_original_id = int(selected_anchor)
                    else:
                        _log_and_callback(
                            "center_anchor_fallback_central_only",
                            lvl="WARN",
                            callback=progress_callback,
                        )
                elif tile_id_order:
                    anchor_original_id = int(tile_id_order[0])

                try:
                    pcb(
                        "phase3_center_out_plan",
                        prog=None,
                        lvl="INFO_DETAIL",
                        anchor=int(anchor_original_id) if anchor_original_id is not None else None,
                        center_ra=f"{global_center_coord.ra.deg:.6f}" if global_center_coord else None,
                        center_dec=f"{global_center_coord.dec.deg:.6f}" if global_center_coord else None,
                    )
                except Exception:
                    pass

                if tile_id_order:
                    anchor_id_final = int(anchor_original_id) if anchor_original_id is not None else int(tile_id_order[0])
                    center_out_context = CenterOutNormalizationContext(
                        anchor_tile_original_id=anchor_id_final,
                        ordered_tile_ids=tile_id_order,
                        tile_distances=distances,
                        settings=center_out_settings,
                        global_center=global_center_coord,
                        logger_instance=logger,
                    )
                    prestack_anchor_tile_id = anchor_id_final
            else:
                center_out_settings["enabled"] = False
            
            # Calcul des workers pour la Phase 3 (alignement/stacking des groupes)
            actual_num_workers_ph3 = _compute_phase_workers(
                effective_base_workers,
                num_seestar_stacks_to_process,
                ALIGNMENT_PHASE_WORKER_RATIO,
            )
            # Boost Phase 3 toward ~90% of logical cores while leaving one core free
            try:
                target_by_cpu = max(
                    1,
                    min(
                        num_logical_processors - 1,
                        int(math.ceil(num_logical_processors * 0.9)),
                    ),
                )
            except Exception:
                target_by_cpu = max(1, num_logical_processors - 1)
            target_ph3_workers = max(1, min(num_seestar_stacks_to_process, target_by_cpu))
            if target_ph3_workers > actual_num_workers_ph3:
                prev_workers = actual_num_workers_ph3
                actual_num_workers_ph3 = target_ph3_workers
                try:
                    pcb(
                        f"WORKERS_PHASE3: Boost {prev_workers} -> {actual_num_workers_ph3} (90% cores target, keeping one free)",
                        prog=None,
                        lvl="INFO_DETAIL",
                    )
                except Exception:
                    pass
            if auto_caps_info:
                try:
                    parallel_cap = int(auto_caps_info.get("parallel_groups", 0))
                except Exception:
                    parallel_cap = 0
                # Only apply auto-cap when it allows more than one worker; when
                # heuristics return 1 (e.g., memmap-enabled or missing probes),
                # let downstream RAM/IO guards decide to avoid pinning Phase 3
                # to a single worker.
                if parallel_cap > 1:
                    prev_workers = actual_num_workers_ph3
                    actual_num_workers_ph3 = max(1, min(actual_num_workers_ph3, parallel_cap))
                    if actual_num_workers_ph3 != prev_workers:
                        try:
                            _log_and_callback(
                                f"AutoCaps: Phase 3 worker cap {prev_workers} -> {actual_num_workers_ph3} (parallel limit)",
                                prog=None,
                                lvl="INFO_DETAIL",
                                callback=progress_callback,
                            )
                        except Exception:
                            pass
            # On Windows, still respect the reserve-one-core rule but avoid hard-capping at 4
            if os.name == 'nt':
                actual_num_workers_ph3 = max(1, min(actual_num_workers_ph3, max(1, num_logical_processors - 1)))
            # Apply IO-based cap if available
            try:
                if io_ph3_cap is not None:
                    prev_workers = actual_num_workers_ph3
                    actual_num_workers_ph3 = max(1, min(actual_num_workers_ph3, int(io_ph3_cap)))
                    if actual_num_workers_ph3 != prev_workers:
                        pcb(
                            f"IO_ADAPT: Phase 3 workers {prev_workers} -> {actual_num_workers_ph3} due to IO ({io_read_cat})",
                            prog=None,
                            lvl="INFO_DETAIL",
                        )
            except Exception:
                pass
            # RAM-aware cap for Phase 3: estimate per-job footprint and clamp concurrency
            try:
                avail_bytes = int(psutil.virtual_memory().available)
                # Determine per-frame bytes via a sample cached image when possible
                per_frame_bytes = None
                try:
                    sample_cache = None
                    if seestar_stack_groups and seestar_stack_groups[0]:
                        sample_cache = seestar_stack_groups[0][0].get('path_preprocessed_cache')
                    if sample_cache and _path_exists(sample_cache):
                        _arr = np.load(sample_cache, mmap_mode='r')
                        per_frame_bytes = int(_arr.size) * int(_arr.dtype.itemsize)
                        _arr = None
                except Exception:
                    per_frame_bytes = None
                if per_frame_bytes is None or per_frame_bytes <= 0:
                    # Conservative default (Seestar 1080x1920 RGB float32)
                    per_frame_bytes = 1080 * 1920 * 3 * 4
                frames_per_pass_cfg = int(winsor_max_frames_per_pass) if winsor_max_frames_per_pass and int(winsor_max_frames_per_pass) > 0 else 256
                try:
                    max_frames_in_group = max(len(g) for g in seestar_stack_groups if g) if seestar_stack_groups else 1
                except Exception:
                    max_frames_in_group = 1
                frames_per_pass = max(1, min(frames_per_pass_cfg, max_frames_in_group))
                fudge = 2.0
                per_job_bytes = int(max(1, frames_per_pass) * per_frame_bytes * fudge)
                allowed = int(avail_bytes * 0.6)
                max_by_ram = max(1, allowed // max(1, per_job_bytes))
                prev_workers = actual_num_workers_ph3
                actual_num_workers_ph3 = max(1, min(actual_num_workers_ph3, int(max_by_ram)))
                if actual_num_workers_ph3 != prev_workers:
                    try:
                        mb_per_job = per_job_bytes / (1024.0 * 1024.0)
                        pcb(
                            f"RAM_CAP: Phase 3 workers {prev_workers} -> {actual_num_workers_ph3} (frames/pass={frames_per_pass}, per-job~{mb_per_job:.1f}MB)",
                            prog=None,
                            lvl="INFO_DETAIL",
                        )
                    except Exception:
                        pass
            except Exception:
                pass
            pcb(
                f"WORKERS_PHASE3: Utilisation de {actual_num_workers_ph3} worker(s). (Base: {effective_base_workers}, Ratio {ALIGNMENT_PHASE_WORKER_RATIO*100:.0f}%, Groupes: {num_seestar_stacks_to_process})",
                prog=None,
                lvl="INFO",
            )  # Log mis à jour pour clarté

            # Initialize adaptive concurrency controls for Phase 3 (I/O + tasks)
            try:
                global _PH3_CONCURRENCY_SEMAPHORE
                _PH3_CONCURRENCY_SEMAPHORE = threading.Semaphore(int(actual_num_workers_ph3))
            except Exception:
                pass

            # Start a lightweight real-time monitor to adapt concurrency while Phase 3 runs
            monitor_stop_evt = threading.Event()

            def _rt_adapt_concurrency():
                try:
                    import psutil as _ps
                except Exception:
                    return  # psutil absent; skip runtime adaptation
                current_ph3_limit = int(actual_num_workers_ph3)
                current_cache_slots = None
                default_cache_slots = 2 if os.name == 'nt' else 3
                last_io = None
                last_t = None
                try:
                    last_io = _ps.disk_io_counters()
                    last_t = time.perf_counter()
                except Exception:
                    last_io, last_t = None, None
                while not monitor_stop_evt.is_set():
                    time.sleep(1.25)
                    # CPU snapshot
                    try:
                        cpu_pct = _ps.cpu_percent(interval=None)
                    except Exception:
                        cpu_pct = None
                    # Disk read throughput MB/s
                    read_mbps = None
                    try:
                        if last_io is not None:
                            now_io = _ps.disk_io_counters()
                            now_t = time.perf_counter()
                            dt = max(1e-3, (now_t - (last_t or now_t)))
                            read_mbps = (max(0, now_io.read_bytes - last_io.read_bytes) / dt) / (1024 * 1024)
                            last_io, last_t = now_io, now_t
                    except Exception:
                        pass

                    new_ph3_limit = current_ph3_limit
                    new_cache_slots = current_cache_slots if current_cache_slots is not None else default_cache_slots

                    if read_mbps is not None:
                        if os.name == 'nt':
                            if read_mbps >= 120:
                                new_ph3_limit = 1
                                new_cache_slots = 1
                            elif read_mbps >= 80:
                                new_ph3_limit = min(new_ph3_limit, 2)
                                new_cache_slots = 1
                            elif read_mbps >= 40:
                                new_cache_slots = 2
                            else:
                                new_cache_slots = default_cache_slots
                        else:
                            if read_mbps >= 200:
                                new_ph3_limit = max(1, min(new_ph3_limit, 2))
                                new_cache_slots = 2
                            elif read_mbps >= 120:
                                new_cache_slots = 2
                            else:
                                new_cache_slots = default_cache_slots

                    if cpu_pct is not None:
                        if cpu_pct >= 90:
                            new_ph3_limit = max(1, min(new_ph3_limit, 2 if os.name == 'nt' else 3))
                        elif cpu_pct <= 45:
                            new_ph3_limit = max(new_ph3_limit, min(int(actual_num_workers_ph3), 3 if os.name == 'nt' else int(actual_num_workers_ph3)))

                    new_ph3_limit = max(1, min(int(actual_num_workers_ph3), int(new_ph3_limit)))
                    new_cache_slots = max(1, int(new_cache_slots))

                    try:
                        if new_ph3_limit != current_ph3_limit:
                            current_ph3_limit = new_ph3_limit
                            try:
                                global _PH3_CONCURRENCY_SEMAPHORE
                                _PH3_CONCURRENCY_SEMAPHORE = threading.Semaphore(int(current_ph3_limit))
                                pcb(f"IO_ADAPT_RT: ph3_workers -> {current_ph3_limit}", prog=None, lvl="INFO_DETAIL")
                            except Exception:
                                pass
                        if (current_cache_slots is None) or (new_cache_slots != current_cache_slots):
                            current_cache_slots = new_cache_slots
                            try:
                                global _CACHE_IO_SEMAPHORE
                                _CACHE_IO_SEMAPHORE = threading.Semaphore(int(current_cache_slots))
                                pcb(f"IO_ADAPT_RT: cache_read_slots -> {current_cache_slots}", prog=None, lvl="INFO_DETAIL")
                            except Exception:
                                pass
                    except Exception:
                        pass

            monitor_thread = threading.Thread(target=_rt_adapt_concurrency, name="ZeMosaic_Ph3_RTAdapt", daemon=True)
            monitor_thread.start()

            tiles_processed_count_ph3 = 0
            # Envoyer l'info initiale avant la boucle
            if num_seestar_stacks_to_process > 0:
                pcb(f"MASTER_TILE_COUNT_UPDATE:{tiles_processed_count_ph3}/{num_seestar_stacks_to_process}", prog=None, lvl="ETA_LEVEL")
            
            executor_ph3 = ThreadPoolExecutor(max_workers=actual_num_workers_ph3, thread_name_prefix="ZeMosaic_Ph3_")

            future_to_tile_id: dict = {}
            tile_input_cache_paths: dict[int, list[str]] = {}
            pending_futures: set = set()
            next_dynamic_tile_id = num_seestar_stacks_to_process

            def _submit_master_tile_group(group_info_list: list[dict], assigned_tile_id: int, processing_rank: int | None = None) -> None:
                future = executor_ph3.submit(
                    create_master_tile,
                    group_info_list,
                    assigned_tile_id,
                    temp_master_tile_storage_dir,
                    stack_norm_method, stack_weight_method, stack_reject_algo,
                    stack_kappa_low, stack_kappa_high, parsed_winsor_limits,
                    stack_final_combine,
                    poststack_equalize_rgb_config,
                    apply_radial_weight_config, radial_feather_fraction_config,
                    radial_shape_power_config, min_radial_weight_floor_config,
                    quality_crop_enabled_config, quality_crop_band_px_config,
                    quality_crop_k_sigma_config, quality_crop_margin_px_config,
                    quality_crop_min_run_config,
                    altaz_cleanup_enabled_config,
                    altaz_margin_percent_config,
                    altaz_decay_config,
                    altaz_nanize_config,
                    quality_gate_enabled_config,
                    quality_gate_threshold_config,
                    quality_gate_edge_band_px_config,
                    quality_gate_k_sigma_config,
                    quality_gate_erode_px_config,
                    quality_gate_move_rejects_config,
                    astap_exe_path, astap_data_dir_param, astap_search_radius_config,
                    astap_downsample_config, astap_sensitivity_config, 180,
                    winsor_worker_limit,
                    winsor_max_frames_per_pass,
                    progress_callback,
                    resource_strategy=auto_resource_strategy,
                    center_out_context=center_out_context,
                    center_out_settings=center_out_settings if center_out_context else None,
                    center_out_rank=processing_rank,
                    parallel_plan=getattr(zconfig, "parallel_plan", worker_config_cache.get("parallel_plan")),
                    dbg_tile_ids=dbg_tile_ids,
                    allow_batch_duplication=bool(allow_duplication_config),
                    target_stack_size=int(target_stack_config),
                    min_safe_stack_size=int(min_safe_stack_config),
                )
                future_to_tile_id[future] = assigned_tile_id
                pending_futures.add(future)
                if cache_retention_mode == "per_tile":
                    cache_paths: list[str] = []
                    for raw_entry in group_info_list or []:
                        if not isinstance(raw_entry, dict):
                            continue
                        cache_path = raw_entry.get('path_preprocessed_cache')
                        if isinstance(cache_path, str):
                            cache_paths.append(cache_path)
                    tile_input_cache_paths[assigned_tile_id] = cache_paths

            for proc_idx, sg_info_list in enumerate(seestar_stack_groups):
                assigned_tile_id = tile_id_order[proc_idx] if proc_idx < len(tile_id_order) else proc_idx
                rank = center_out_context.get_rank(assigned_tile_id) if center_out_context else proc_idx
                _submit_master_tile_group(sg_info_list, assigned_tile_id, rank)

            start_time_loop_ph3 = time.time()
            last_time_loop_ph3 = start_time_loop_ph3
            step_times_ph3 = []

            while pending_futures:
                done_futures, _ = wait(pending_futures, return_when=FIRST_COMPLETED)
                for future in done_futures:
                    pending_futures.discard(future)
                    tile_id_for_future = future_to_tile_id.pop(future, None)
                    if tile_id_for_future is None:
                        continue
                    tiles_processed_count_ph3 += 1
                    cache_paths_for_tile: list[str] = []
                    if cache_retention_mode == "per_tile":
                        cache_paths_for_tile = tile_input_cache_paths.pop(tile_id_for_future, [])

                    pcb(f"MASTER_TILE_COUNT_UPDATE:{tiles_processed_count_ph3}/{num_seestar_stacks_to_process}", prog=None, lvl="ETA_LEVEL")

                    prog_step_phase3 = base_progress_phase3 + int(
                        PROGRESS_WEIGHT_PHASE3_MASTER_TILES * (tiles_processed_count_ph3 / max(1, num_seestar_stacks_to_process))
                    )
                    if progress_callback:
                        try:
                            progress_callback("phase3_master_tiles", tiles_processed_count_ph3, num_seestar_stacks_to_process)
                        except Exception:
                            pass

                    telemetry.maybe_emit_stats(
                        _telemetry_context(
                            {
                                "phase_name": "Phase 3: Master Tiles",
                                "phase_index": 3,
                                "tiles_done": tiles_processed_count_ph3,
                                "tiles_total": num_seestar_stacks_to_process,
                            }
                        )
                    )

                    now = time.time()
                    step_times_ph3.append(now - last_time_loop_ph3)
                    last_time_loop_ph3 = now

                    try:
                        main_result, retry_groups = future.result()
                        mt_result_path, mt_result_wcs = (main_result or (None, None))
                        if mt_result_path and mt_result_wcs:
                            master_tiles_results_list_temp[tile_id_for_future] = (mt_result_path, mt_result_wcs)
                            if cache_retention_mode == "per_tile" and cache_paths_for_tile:
                                removed_count, removed_bytes = _cleanup_per_tile_cache(cache_paths_for_tile)
                                freed_mb = removed_bytes / (1024 * 1024) if removed_bytes else 0.0
                                logger.debug(
                                    "Per-tile cache cleanup for tile %s: removed %d file(s), freed %.3f MiB",
                                    tile_id_for_future,
                                    removed_count,
                                    freed_mb,
                                )
                                try:
                                    pcb(
                                        "run_debug_cache_per_tile_cleanup",
                                        prog=None,
                                        lvl="DEBUG_DETAIL",
                                        tile_id=int(tile_id_for_future),
                                        removed=int(removed_count),
                                        freed_mib=f"{freed_mb:.3f}",
                                    )
                                except Exception:
                                    pass
                        else:
                            pcb(
                                "run_warn_phase3_master_tile_creation_failed_thread",
                                prog=prog_step_phase3,
                                lvl="WARN",
                                stack_num=int(tile_id_for_future) + 1,
                            )
                        if retry_groups:
                            for retry_group in retry_groups:
                                if not retry_group:
                                    continue
                                filtered_retry_group: list[dict] = []
                                dropped_infos: list[dict] = []
                                for raw_info in retry_group:
                                    if isinstance(raw_info, dict):
                                        attempts = int(raw_info.get('retry_attempt', 0))
                                        if attempts > MAX_ALIGNMENT_RETRY_ATTEMPTS:
                                            dropped_infos.append(raw_info)
                                            continue
                                    filtered_retry_group.append(raw_info)
                                for dropped in dropped_infos:
                                    try:
                                        filename = _safe_basename(dropped.get('path_raw', 'UnknownRaw'))
                                    except Exception:
                                        filename = str(dropped)
                                    pcb(
                                        "run_warn_phase3_alignment_retry_abandoned",
                                        prog=None,
                                        lvl="WARN",
                                        tile_id=int(tile_id_for_future),
                                        filename=filename,
                                        attempts=int(dropped.get('retry_attempt', 0)) if isinstance(dropped, dict) else None,
                                    )
                                if not filtered_retry_group:
                                    continue
                                new_tile_id = next_dynamic_tile_id
                                next_dynamic_tile_id += 1
                                num_seestar_stacks_to_process += 1
                                pcb(
                                    "run_info_phase3_retry_submitted",
                                    prog=None,
                                    lvl="INFO_DETAIL",
                                    origin_tile=int(tile_id_for_future),
                                    new_tile=new_tile_id,
                                    frames=len(filtered_retry_group),
                                )
                                retry_rank = center_out_context.get_rank(new_tile_id) if center_out_context else None
                                _submit_master_tile_group(filtered_retry_group, new_tile_id, retry_rank)
                                pcb(
                                    f"MASTER_TILE_COUNT_UPDATE:{tiles_processed_count_ph3}/{num_seestar_stacks_to_process}",
                                    prog=None,
                                    lvl="ETA_LEVEL",
                                )
                                if progress_callback:
                                    try:
                                        progress_callback("phase3_master_tiles", tiles_processed_count_ph3, num_seestar_stacks_to_process)
                                    except Exception:
                                        pass
                    except Exception as exc_thread_ph3:
                        pcb(
                            "run_error_phase3_thread_exception",
                            prog=prog_step_phase3,
                            lvl="ERROR",
                            stack_num=int(tile_id_for_future) + 1,
                            error=str(exc_thread_ph3),
                        )
                        logger.error(f"Exception Phase 3 pour stack {int(tile_id_for_future) + 1}:", exc_info=True)
                    finally:
                        # Aggressively free CuPy memory pools between tiles to avoid device/pinned host growth
                        try:
                            if ZEMOSAIC_UTILS_AVAILABLE and zemosaic_utils and hasattr(zemosaic_utils, "free_cupy_memory_pools"):
                                zemosaic_utils.free_cupy_memory_pools()
                        except Exception:
                            pass
                        try:
                            gc.collect()
                        except Exception:
                            pass

                    if tiles_processed_count_ph3 % max(1, num_seestar_stacks_to_process // 5) == 0 or tiles_processed_count_ph3 == num_seestar_stacks_to_process:
                         _log_memory_usage(progress_callback, f"Phase 3 - Traité {tiles_processed_count_ph3}/{num_seestar_stacks_to_process} tuiles")

                    elapsed_phase3 = time.monotonic() - start_time_phase3
                    time_per_master_tile_creation = elapsed_phase3 / max(1, tiles_processed_count_ph3)
                    eta_phase3_sec = (num_seestar_stacks_to_process - tiles_processed_count_ph3) * time_per_master_tile_creation
                    current_progress_in_run_percent_ph3 = base_progress_phase3 + (tiles_processed_count_ph3 / max(1, num_seestar_stacks_to_process)) * PROGRESS_WEIGHT_PHASE3_MASTER_TILES
                    time_per_percent_point_global_ph3 = (time.monotonic() - start_time_total_run) / max(1, current_progress_in_run_percent_ph3) if current_progress_in_run_percent_ph3 > 0 else (time.monotonic() - start_time_total_run)
                    total_eta_sec_ph3 = eta_phase3_sec + (100 - current_progress_in_run_percent_ph3) * time_per_percent_point_global_ph3
                    update_gui_eta(total_eta_sec_ph3)

            # Toutes les futures sont terminées → fermeture propre
            # Stop the runtime adaptation monitor for Phase 3
            try:
                monitor_stop_evt.set()
                if monitor_thread and monitor_thread.is_alive():
                    monitor_thread.join(timeout=2.0)
            except Exception:
                pass
            executor_ph3.shutdown(wait=True)

            if enable_poststack_anchor_review_config and master_tiles_results_list_temp:
                post_review_cfg = {
                    "probe_limit": poststack_anchor_probe_limit_config,
                    "span_range": poststack_anchor_span_range_config,
                    "median_clip_sigma": poststack_anchor_median_clip_sigma_config,
                    "min_improvement": poststack_anchor_min_improvement_config,
                    "use_overlap_affine": poststack_anchor_use_overlap_affine_config,
                    "sky_percentile": center_out_settings.get("sky_percentile"),
                }
                try:
                    master_tiles_results_list_temp, anchor_shift_candidate = run_poststack_anchor_review(
                        master_tiles_results_list_temp,
                        prestack_anchor_tile_id,
                        post_review_cfg,
                        progress_callback,
                        tile_distances=getattr(center_out_context, "tile_distances", None) if center_out_context else None,
                    )
                    if isinstance(anchor_shift_candidate, tuple):
                        global_anchor_shift = anchor_shift_candidate
                except Exception:
                    logger.warning("Post-stack anchor review failed", exc_info=True)

            master_tiles_results_list = [master_tiles_results_list_temp[i] for i in sorted(master_tiles_results_list_temp.keys())]
            del master_tiles_results_list_temp; gc.collect()
            if not master_tiles_results_list:
                pcb("run_error_phase3_no_master_tiles_created", prog=(base_progress_phase3 + PROGRESS_WEIGHT_PHASE3_MASTER_TILES), lvl="ERROR"); return

            current_global_progress = base_progress_phase3 + PROGRESS_WEIGHT_PHASE3_MASTER_TILES
            _log_memory_usage(progress_callback, "Fin Phase 3");
            if step_times_ph3:
                avg_step = sum(step_times_ph3) / len(step_times_ph3)
                total_elapsed = time.time() - start_time_loop_ph3
                pcb(
                    "phase3_debug_timing",
                    prog=None,
                    lvl="DEBUG_DETAIL",
                    avg=f"{avg_step:.2f}",
                    total=f"{total_elapsed:.2f}",
                )
            pcb("run_info_phase3_finished_from_cache", prog=current_global_progress, lvl="INFO", num_master_tiles=len(master_tiles_results_list))
            
            # Assurer que le compteur final est bien affiché (au cas où la dernière itération n'aurait pas été exactement le total)
            # Bien que la logique dans la boucle devrait déjà le faire. Peut être redondant mais ne fait pas de mal.
            pcb(f"MASTER_TILE_COUNT_UPDATE:{tiles_processed_count_ph3}/{num_seestar_stacks_to_process}", prog=None, lvl="ETA_LEVEL")

            logger.info("All master tiles complete, entering Phase 5 (reproject & coadd)")
            if progress_callback:
                try:
                    progress_callback("run_info_phase3_finished", None, "INFO", num_master_tiles=len(master_tiles_results_list))
                except Exception:
                    logger.warning("progress_callback failed for phase3 finished", exc_info=True)




            
            
            # --- Phase 4 (Calcul Grille Finale) ---
            base_progress_phase4 = current_global_progress
            _log_memory_usage(progress_callback, "Début Phase 4 (Calcul Grille)")
            pcb("run_info_phase4_started", prog=base_progress_phase4, lvl="INFO")
            pcb("PHASE_UPDATE:4", prog=None, lvl="ETA_LEVEL")
            telemetry.maybe_emit_stats(
                _telemetry_context(
                    {
                        "phase_name": "Phase 4: Final Grid",
                        "phase_index": 4,
                        "tiles_total": len(master_tiles_results_list),
                    }
                )
            )
            wcs_list_for_final_grid = []; shapes_list_for_final_grid_hw = []
            start_time_loop_ph4 = time.time(); last_time_loop_ph4 = start_time_loop_ph4; step_times_ph4 = []
            total_steps_ph4 = len(master_tiles_results_list)
            for idx_loop, (mt_path_iter,mt_wcs_iter) in enumerate(master_tiles_results_list, 1):
                # ... (logique de récupération shape, inchangée) ...
                if not (mt_path_iter and _path_exists(mt_path_iter) and mt_wcs_iter and mt_wcs_iter.is_celestial): pcb("run_warn_phase4_invalid_master_tile_for_grid", prog=None, lvl="WARN", path=_safe_basename(mt_path_iter)); continue
                try:
                    h_mt_loc,w_mt_loc=0,0
                    if mt_wcs_iter.pixel_shape and mt_wcs_iter.pixel_shape[0] > 0 and mt_wcs_iter.pixel_shape[1] > 0 : h_mt_loc,w_mt_loc=mt_wcs_iter.pixel_shape[1],mt_wcs_iter.pixel_shape[0] 
                    else: 
                        with fits.open(mt_path_iter,memmap=True, do_not_scale_image_data=True) as hdul_mt_s:
                            if hdul_mt_s[0].data is None: pcb("run_warn_phase4_no_data_in_tile_fits", prog=None, lvl="WARN", path=_safe_basename(mt_path_iter)); continue
                            data_shape = hdul_mt_s[0].shape
                            if len(data_shape) == 3:
                                # data_shape == (height, width, channels)
                                h_mt_loc,w_mt_loc = data_shape[0],data_shape[1]
                            elif len(data_shape) == 2: h_mt_loc,w_mt_loc = data_shape[0],data_shape[1]
                            else: pcb("run_warn_phase4_unhandled_tile_shape", prog=None, lvl="WARN", path=_safe_basename(mt_path_iter), shape=data_shape); continue 
                            if mt_wcs_iter and mt_wcs_iter.is_celestial and mt_wcs_iter.pixel_shape is None:
                                try: mt_wcs_iter.pixel_shape=(w_mt_loc,h_mt_loc)
                                except Exception as e_set_ps: pcb("run_warn_phase4_failed_set_pixel_shape", prog=None, lvl="WARN", path=_safe_basename(mt_path_iter), error=str(e_set_ps))
                    if h_mt_loc > 0 and w_mt_loc > 0: shapes_list_for_final_grid_hw.append((int(h_mt_loc),int(w_mt_loc))); wcs_list_for_final_grid.append(mt_wcs_iter)
                    else: pcb("run_warn_phase4_zero_dimensions_tile", prog=None, lvl="WARN", path=_safe_basename(mt_path_iter))
                    now = time.time(); step_times_ph4.append(now - last_time_loop_ph4); last_time_loop_ph4 = now
                    if progress_callback:
                        try:
                            progress_callback("phase4_grid", idx_loop, total_steps_ph4)
                        except Exception:
                            pass
                    telemetry.maybe_emit_stats(
                        _telemetry_context(
                            {
                                "phase_name": "Phase 4: Final Grid",
                                "phase_index": 4,
                                "tiles_done": idx_loop,
                                "tiles_total": total_steps_ph4,
                            }
                        )
                    )
                except Exception as e_read_tile_shape: pcb("run_error_phase4_reading_tile_shape", prog=None, lvl="ERROR", path=_safe_basename(mt_path_iter), error=str(e_read_tile_shape)); logger.error(f"Erreur lecture shape tuile {_safe_basename(mt_path_iter)}:", exc_info=True); continue
            if not wcs_list_for_final_grid or not shapes_list_for_final_grid_hw or len(wcs_list_for_final_grid) != len(shapes_list_for_final_grid_hw): pcb("run_error_phase4_insufficient_tile_info", prog=(base_progress_phase4 + PROGRESS_WEIGHT_PHASE4_GRID_CALC), lvl="ERROR"); return
            final_mosaic_drizzle_scale = 1.0 
            final_output_wcs, final_output_shape_hw = _calculate_final_mosaic_grid(wcs_list_for_final_grid, shapes_list_for_final_grid_hw, final_mosaic_drizzle_scale, progress_callback)
            if not final_output_wcs or not final_output_shape_hw: pcb("run_error_phase4_grid_calc_failed", prog=(base_progress_phase4 + PROGRESS_WEIGHT_PHASE4_GRID_CALC), lvl="ERROR"); return
            current_global_progress = base_progress_phase4 + PROGRESS_WEIGHT_PHASE4_GRID_CALC
            _log_memory_usage(progress_callback, "Fin Phase 4");
            if step_times_ph4:
                avg_step = sum(step_times_ph4) / len(step_times_ph4)
                total_elapsed = time.time() - start_time_loop_ph4
                pcb(
                    "phase4_debug_timing",
                    prog=None,
                    lvl="DEBUG_DETAIL",
                    avg=f"{avg_step:.2f}",
                    total=f"{total_elapsed:.2f}",
                )
            pcb("run_info_phase4_finished", prog=current_global_progress, lvl="INFO", shape=final_output_shape_hw, crval=final_output_wcs.wcs.crval if final_output_wcs.wcs else 'N/A')

            base_progress_phase4_5 = current_global_progress
            phase45_options = _build_phase45_options_dict(base_progress_phase4_5)
            base_progress_phase5 = base_progress_phase4_5 + PROGRESS_WEIGHT_PHASE4_5_INTER_MASTER
            phase5_options = _build_phase5_options_dict(base_progress_phase5)

            telemetry.maybe_emit_stats(
                _telemetry_context(
                    {
                        "phase_name": "Phase 5: Assembly",
                        "phase_index": 5,
                        "tiles_total": len(master_tiles_results_list),
                    }
                )
            )
            (
                master_tiles_results_list,
                final_mosaic_data_HWC,
                final_mosaic_coverage_HW,
                final_alpha_map,
                alpha_final,
                current_global_progress,
            ) = _run_shared_phase45_phase5_pipeline(
                master_tiles_results_list,
                final_output_wcs=final_output_wcs,
                final_output_shape_hw=final_output_shape_hw,
                temp_master_tile_storage_dir=temp_master_tile_storage_dir,
                output_folder=output_folder,
                cache_retention_mode=cache_retention_mode,
                phase45_options=phase45_options,
                phase5_options=phase5_options,
                final_quality_pipeline_cfg=final_quality_pipeline_cfg,
                start_time_total_run=start_time_total_run,
                progress_callback=progress_callback,
                pcb=pcb,
                zconfig=zconfig,
                logger=logger,
            )
            if final_mosaic_data_HWC is None:
                return

    # --- Phase 6 (Sauvegarde) ---
    base_progress_phase6 = current_global_progress
    pcb("PHASE_UPDATE:6", prog=None, lvl="ETA_LEVEL")
    _log_memory_usage(progress_callback, "Début Phase 6 (Sauvegarde)")
    pcb("run_info_phase6_started", prog=base_progress_phase6, lvl="INFO")
    telemetry.maybe_emit_stats(
        _telemetry_context(
            {
                "phase_name": "Phase 6: Save",
                "phase_index": 6,
                "tiles_total": len(master_tiles_results_list),
            }
        )
    )
    output_base_name = f"zemosaic_MT{len(master_tiles_results_list)}_R{len(all_raw_files_processed_info)}"
    output_folder_path = Path(output_folder).expanduser()
    final_fits_path = output_folder_path / f"{output_base_name}.fits"
    
    final_header = fits.Header() 
    if final_output_wcs:
        try: final_header.update(final_output_wcs.to_header(relax=True))
        except Exception as e_hdr_wcs: pcb("run_warn_phase6_wcs_to_header_failed", error=str(e_hdr_wcs), lvl="WARN")
    
    final_header['SOFTWARE']=('ZeMosaic v4.1.0','Mosaic Software') # Incrémente la version 
    final_header['NMASTILE']=(len(master_tiles_results_list),"Master Tiles combined")
    final_header['NRAWINIT']=(num_total_raw_files,"Initial raw images found")
    final_header['NRAWPROC']=(len(all_raw_files_processed_info),"Raw images with WCS processed")
    # ... (autres clés de config comme ASTAP, Stacking, etc.) ...
    final_header['STK_NORM'] = (str(stack_norm_method), 'Stacking: Normalization Method')
    final_header['STK_WGHT'] = (str(stack_weight_method), 'Stacking: Weighting Method')
    if apply_radial_weight_config:
        final_header['STK_RADW'] = (True, 'Stacking: Radial Weighting Applied')
        final_header['STK_RADFF'] = (radial_feather_fraction_config, 'Stacking: Radial Feather Fraction')
        final_header['STK_RADPW'] = (radial_shape_power_config, 'Stacking: Radial Weight Shape Power')
        final_header['STK_RADFLR'] = (min_radial_weight_floor_config, 'Stacking: Min Radial Weight Floor')
    else:
        final_header['STK_RADW'] = (False, 'Stacking: Radial Weighting Applied')
    final_header['STK_REJ'] = (str(stack_reject_algo), 'Stacking: Rejection Algorithm')
    # ... (kappa, winsor si pertinent pour l'algo de rejet) ...
    final_header['STK_COMB'] = (str(stack_final_combine), 'Stacking: Final Combine Method')
    final_header['ALPHAEXT'] = (1 if alpha_final is not None else 0, 'Alpha mask ext present')
    final_header['ZMASMBMTH'] = (final_assembly_method_config, 'Final Assembly Method')
    final_header['ZM_WORKERS'] = (num_base_workers_config, 'GUI: Base workers config (0=auto)')

    if logger.isEnabledFor(logging.DEBUG):
        _dbg_rgb_stats(
            "P6_PRE_EXPORT",
            final_mosaic_data_HWC,
            coverage=final_mosaic_coverage_HW,
            alpha=alpha_final,
            logger=logger,
        )

    rgb_black_level_info: dict[str, Any] | None = None
    if (
        final_mosaic_black_point_equalize_enabled
        and final_mosaic_data_HWC is not None
        and not sds_mode_phase5
    ):
        coverage_for_bl = None if alpha_final is not None else final_mosaic_coverage_HW
        final_mosaic_data_HWC, rgb_black_level_info = _equalize_rgb_black_level_hwc(
            final_mosaic_data_HWC,
            alpha_mask=alpha_final,
            coverage_mask=coverage_for_bl,
            p_low=final_mosaic_black_point_percentile,
            logger=logger,
        )

    try:
        if not (ZEMOSAIC_UTILS_AVAILABLE and zemosaic_utils):
            raise RuntimeError("zemosaic_utils non disponible pour sauvegarde FITS.")
        legacy_rgb_flag = bool(legacy_rgb_cube_config)
        # Ensure the final mosaic buffer is a contiguous, writeable ndarray for I/O
        save_array = None
        try:
            if isinstance(final_mosaic_data_HWC, np.ndarray):
                save_array = np.ascontiguousarray(final_mosaic_data_HWC)
                if not save_array.flags.writeable:
                    save_array = save_array.copy()
        except Exception:
            save_array = final_mosaic_data_HWC
        def _attach_alpha_extension(target_path: Path, *, log_success: bool = False) -> None:
            path_obj = Path(target_path)
            if (
                alpha_final is None
                or not path_obj
                or not path_obj.exists()
                or not (ASTROPY_AVAILABLE and fits)
                or not (ZEMOSAIC_UTILS_AVAILABLE and zemosaic_utils)
                or not hasattr(zemosaic_utils, "append_alpha_hdu")
            ):
                return
            try:
                with fits.open(str(path_obj), mode="update") as hdul_final:
                    zemosaic_utils.append_alpha_hdu(hdul_final, alpha_final)
                    hdul_final.flush()
                if log_success:
                    logger.info(
                        "phase6: wrote ALPHA extension (uint8, 0–255), shape=%s",
                        getattr(alpha_final, "shape", None),
                    )
            except Exception as exc_alpha:
                logger.warning("phase6: could not write ALPHA extension: %s", exc_alpha)

        is_rgb = (
            isinstance(save_array, np.ndarray)
            and save_array.ndim == 3
            and save_array.shape[-1] == 3
        )
        zemosaic_utils.save_fits_image(
            image_data=save_array,
            output_path=str(final_fits_path),
            header=final_header,
            overwrite=True,
            save_as_float=True,
            legacy_rgb_cube=legacy_rgb_flag,
            progress_callback=progress_callback,
            axis_order="HWC",
            alpha_mask=alpha_final,
        )
        _attach_alpha_extension(final_fits_path, log_success=True)

        if (
            bool(save_final_as_uint16_config)
            and not legacy_rgb_flag
            and ZEMOSAIC_UTILS_AVAILABLE
            and hasattr(zemosaic_utils, "write_final_fits_uint16_color_aware")
        ):
            viewer_fits_path = output_folder_path / f"{output_base_name}_viewer.fits"
            try:
                zemosaic_utils.write_final_fits_uint16_color_aware(
                    str(viewer_fits_path),
                    save_array,
                    header=final_header,
                    force_rgb_planes=is_rgb,
                    legacy_rgb_cube=legacy_rgb_flag,
                    overwrite=True,
                )
                _attach_alpha_extension(viewer_fits_path, log_success=False)
                pcb(
                    "run_info_phase6_viewer_fits_saved",
                    prog=None,
                    lvl="INFO_DETAIL",
                    filename=_safe_basename(viewer_fits_path),
                )
            except Exception as e_viewer:
                pcb(
                    "run_warn_phase6_viewer_fits_failed",
                    prog=None,
                    lvl="WARN",
                    error=str(e_viewer),
                )
        
        if final_mosaic_coverage_HW is not None and np.any(final_mosaic_coverage_HW):
            coverage_path = output_folder_path / f"{output_base_name}_coverage.fits"
            cov_hdr = fits.Header() 
            if ASTROPY_AVAILABLE and final_output_wcs: 
                try: cov_hdr.update(final_output_wcs.to_header(relax=True))
                except: pass 
            cov_hdr['EXTNAME']=('COVERAGE','Coverage Map') 
            cov_hdr['BUNIT']=('count','Pixel contributions or sum of weights')
            zemosaic_utils.save_fits_image(
                final_mosaic_coverage_HW,
                str(coverage_path),
                header=cov_hdr,
                overwrite=True,
                save_as_float=True,
                progress_callback=progress_callback,
                axis_order="HWC",
            )
            pcb("run_info_coverage_map_saved", prog=None, lvl="INFO_DETAIL", filename=_safe_basename(coverage_path))

        logger.info("[Alpha] Final mosaic saved with ALPHA=%s", bool(alpha_final is not None))

        current_global_progress = base_progress_phase6 + PROGRESS_WEIGHT_PHASE6_SAVE
        pcb("run_success_mosaic_saved", prog=current_global_progress, lvl="SUCCESS", filename=_safe_basename(final_fits_path))

        if logger.isEnabledFor(logging.DEBUG):
            _dbg_rgb_stats(
                "P7_POST_EXPORT",
                final_mosaic_data_HWC,
                coverage=final_mosaic_coverage_HW,
                alpha=alpha_final,
                logger=logger,
            )
    except Exception as e_save_m:
        pcb("run_error_phase6_save_failed", prog=(base_progress_phase6 + PROGRESS_WEIGHT_PHASE6_SAVE), lvl="ERROR", error=str(e_save_m))
        logger.error("Erreur sauvegarde FITS final:", exc_info=True)
        # En cas d'échec de sauvegarde, on ne peut pas générer de preview car final_mosaic_data_HWC pourrait être le problème.
        # On essaie quand même de nettoyer avant de retourner.
        if 'final_mosaic_data_HWC' in locals() and final_mosaic_data_HWC is not None: del final_mosaic_data_HWC
        if 'final_mosaic_coverage_HW' in locals() and final_mosaic_coverage_HW is not None: del final_mosaic_coverage_HW
        gc.collect()
        return

    _log_memory_usage(progress_callback, "Fin Sauvegarde FITS (avant preview)")

    # --- MODIFIÉ : Génération de la Preview PNG avec stretch_auto_asifits_like ---
    if final_mosaic_data_HWC is not None and ZEMOSAIC_UTILS_AVAILABLE and zemosaic_utils:
        pcb("run_info_preview_stretch_started_auto_asifits", prog=None, lvl="INFO_DETAIL") # Log mis à jour
        try:
            # Downscale extremely large mosaics for preview to avoid OOM
            step = 1
            alpha_preview: np.ndarray | None = None
            try:
                h_prev, w_prev = int(final_mosaic_data_HWC.shape[0]), int(final_mosaic_data_HWC.shape[1])
                max_preview_dim = 4000  # cap the longest side for preview
                step_h = max(1, h_prev // max_preview_dim)
                step_w = max(1, w_prev // max_preview_dim)
                step = max(step_h, step_w)
                if step > 1:
                    preview_view = final_mosaic_data_HWC[::step, ::step, :]
                    pcb("run_info_preview_downscale", prog=None, lvl="INFO_DETAIL", downscale_step=step, src_shape=str(final_mosaic_data_HWC.shape), preview_shape=str(preview_view.shape))
                else:
                    preview_view = final_mosaic_data_HWC
            except Exception:
                preview_view = final_mosaic_data_HWC
                step = 1

            if alpha_final is not None:
                try:
                    alpha_src = np.asarray(alpha_final, dtype=np.uint8, copy=False)
                    if alpha_src.ndim == 3 and alpha_src.shape[-1] == 1:
                        alpha_src = alpha_src[..., 0]
                    elif alpha_src.ndim > 2:
                        alpha_src = np.squeeze(alpha_src)
                    if alpha_src.shape[:2] != final_mosaic_data_HWC.shape[:2]:
                        logger.debug(
                            "[Alpha] Preview source mask shape mismatch: alpha=%s vs mosaic=%s",
                            getattr(alpha_src, "shape", None),
                            getattr(final_mosaic_data_HWC, "shape", None),
                        )
                        try:
                            import cv2  # type: ignore

                            alpha_src = cv2.resize(
                                alpha_src,
                                (final_mosaic_data_HWC.shape[1], final_mosaic_data_HWC.shape[0]),
                                interpolation=cv2.INTER_NEAREST,
                            )
                        except Exception:
                            alpha_src = None
                    if alpha_src is None:
                        alpha_preview = None
                        raise ValueError("alpha source unavailable for preview masking")
                    alpha_preview = alpha_src[::step, ::step] if step > 1 else alpha_src
                    if alpha_preview.shape[:2] != preview_view.shape[:2]:
                        try:
                            import cv2  # type: ignore

                            alpha_preview = cv2.resize(
                                alpha_preview,
                                (preview_view.shape[1], preview_view.shape[0]),
                                interpolation=cv2.INTER_NEAREST,
                            )
                        except Exception:
                            if (
                                alpha_preview.shape[0] >= preview_view.shape[0]
                                and alpha_preview.shape[1] >= preview_view.shape[1]
                            ):
                                alpha_preview = alpha_preview[
                                    : preview_view.shape[0], : preview_view.shape[1]
                                ]
                            else:
                                alpha_preview = None
                    if alpha_preview is not None and alpha_preview.shape[:2] == preview_view.shape[:2]:
                        mask_zero = alpha_preview == 0
                        if np.any(mask_zero):
                            preview_view = np.array(preview_view, copy=True)
                            try:
                                # Explicitly assign to all channels for the masked pixels
                                preview_view[mask_zero] = [np.nan, np.nan, np.nan]
                            except Exception as e_nan:
                                logger.warning(
                                    "phase6: preview NaN masking failed: %s (shape preview=%s, alpha=%s)",
                                    e_nan,
                                    getattr(preview_view, "shape", None),
                                    getattr(alpha_preview, "shape", None),
                                )
                    else:
                        alpha_preview = None
                except Exception as exc_alpha_prev:
                    logger.warning(
                        "phase6: preview NaN masking failed: %s (shape preview=%s, alpha=%s)",
                        exc_alpha_prev,
                        getattr(preview_view, "shape", None),
                        getattr(alpha_final, "shape", None),
                    )
                    alpha_preview = None

            # Vérifier si la fonction stretch_auto_asifits_like existe dans zemosaic_utils
            if hasattr(zemosaic_utils, 'stretch_auto_asifits_like') and callable(zemosaic_utils.stretch_auto_asifits_like):
                
                # Paramètres pour stretch_auto_asifits_like (à ajuster si besoin)
                # Ces valeurs sont des exemples, tu devras peut-être les affiner
                # ou les rendre configurables plus tard.
                preview_p_low = 2.5  # Percentile pour le point noir (plus élevé que pour asinh seul)
                preview_p_high = 99.8 # Percentile pour le point blanc initial
                                      # Facteur 'a' pour le stretch asinh après la normalisation initiale
                                      # Pour un stretch plus "doux" similaire à ASIFitsView, 'a' peut être plus grand.
                                      # ASIFitsView utilise souvent un 'midtones balance' (gamma-like) aussi.
                                      # Un 'a' de 10 comme dans ton code de test est très doux. Essayons 0.5 ou 1.0.
                preview_asinh_a = 20.0 # Test avec une valeur plus douce pour le 'a' de asinh

                # Prefer GPU stretch when GPU is enabled/available
                if use_gpu_phase5_flag and hasattr(zemosaic_utils, 'stretch_auto_asifits_like_gpu'):
                    m_stretched = zemosaic_utils.stretch_auto_asifits_like_gpu(
                        preview_view,
                        p_low=preview_p_low,
                        p_high=preview_p_high,
                        asinh_a=preview_asinh_a,
                        apply_wb=True,
                    )
                else:
                    m_stretched = zemosaic_utils.stretch_auto_asifits_like(
                        preview_view,
                        p_low=preview_p_low,
                        p_high=preview_p_high,
                        asinh_a=preview_asinh_a,
                        apply_wb=True  # Applique une balance des blancs automatique
                    )

                if m_stretched is not None:
                    img_u8 = (
                        np.nan_to_num(
                            np.clip(m_stretched.astype(np.float32), 0, 1)
                        )
                        * 255
                    ).astype(np.uint8)
                    png_path = output_folder_path / f"{output_base_name}_preview.png"
                    try: 
                        import cv2 # Importer cv2 seulement si nécessaire
                        alpha_png = None
                        if alpha_preview is not None:
                            alpha_png = np.clip(alpha_preview, 0, 255).astype(np.uint8, copy=False)
                            if alpha_png.shape[:2] != img_u8.shape[:2]:
                                alpha_png = cv2.resize(
                                    alpha_png,
                                    (img_u8.shape[1], img_u8.shape[0]),
                                    interpolation=cv2.INTER_NEAREST,
                                )
                        if alpha_png is not None:
                            img_bgra = cv2.cvtColor(img_u8, cv2.COLOR_RGB2BGRA)
                            img_bgra[..., 3] = alpha_png
                            write_success = cv2.imwrite(str(png_path), img_bgra)
                        else:
                            img_bgr = cv2.cvtColor(img_u8, cv2.COLOR_RGB2BGR)
                            write_success = cv2.imwrite(str(png_path), img_bgr)
                        if write_success: 
                            pcb("run_success_preview_saved_auto_asifits", prog=None, lvl="SUCCESS", filename=_safe_basename(png_path))
                            if alpha_png is not None:
                                logger.info("phase6: preview masked (NaN pre-stretch) and saved as RGBA PNG")
                        else: 
                            pcb("run_warn_preview_imwrite_failed_auto_asifits", prog=None, lvl="WARN", filename=_safe_basename(png_path))
                    except ImportError: 
                        pcb("run_warn_preview_opencv_missing_for_auto_asifits", prog=None, lvl="WARN")
                    except Exception as e_cv2_prev: 
                        pcb("run_error_preview_opencv_failed_auto_asifits", prog=None, lvl="ERROR", error=str(e_cv2_prev))
                else:
                    pcb("run_error_preview_stretch_auto_asifits_returned_none", prog=None, lvl="ERROR")
            else:
                pcb("run_warn_preview_stretch_auto_asifits_func_missing", prog=None, lvl="WARN")
                # Fallback sur l'ancienne méthode si stretch_auto_asifits_like n'est pas trouvée
                # (Tu peux supprimer ce fallback si tu es sûr que la fonction existe)
                pcb("run_info_preview_fallback_to_simple_asinh", prog=None, lvl="DEBUG_DETAIL")
                if hasattr(zemosaic_utils, 'stretch_percentile_rgb') and zemosaic_utils.ASTROPY_VISUALIZATION_AVAILABLE:
                     m_stretched_fallback = zemosaic_utils.stretch_percentile_rgb(final_mosaic_data_HWC, p_low=0.5, p_high=99.9, independent_channels=False, asinh_a=0.01 )
                     if m_stretched_fallback is not None:
                        img_u8_fb = (np.clip(m_stretched_fallback.astype(np.float32), 0, 1) * 255).astype(np.uint8)
                        png_path_fb = output_folder_path / f"{output_base_name}_preview_fallback.png"
                        try:
                            import cv2
                            alpha_png_fb = None
                            alpha_source_fb = alpha_final if alpha_final is not None else alpha_preview
                            if alpha_source_fb is not None:
                                alpha_png_fb = np.clip(alpha_source_fb, 0, 255).astype(np.uint8, copy=False)
                                if alpha_png_fb.shape[:2] != img_u8_fb.shape[:2]:
                                    alpha_png_fb = cv2.resize(
                                        alpha_png_fb,
                                        (img_u8_fb.shape[1], img_u8_fb.shape[0]),
                                        interpolation=cv2.INTER_NEAREST,
                                    )
                            if alpha_png_fb is not None:
                                img_bgra_fb = cv2.cvtColor(img_u8_fb, cv2.COLOR_RGB2BGRA)
                                img_bgra_fb[..., 3] = alpha_png_fb
                                cv2.imwrite(str(png_path_fb), img_bgra_fb)
                                logger.info("phase6: preview masked (NaN pre-stretch) and saved as RGBA PNG")
                            else:
                                img_bgr_fb = cv2.cvtColor(img_u8_fb, cv2.COLOR_RGB2BGR)
                                cv2.imwrite(str(png_path_fb), img_bgr_fb)
                            pcb("run_success_preview_saved_fallback", prog=None, lvl="INFO_DETAIL", filename=_safe_basename(png_path_fb))
                        except: pass # Ignorer erreur fallback

        except Exception as e_stretch_main: 
            pcb("run_error_preview_stretch_unexpected_main", prog=None, lvl="ERROR", error=str(e_stretch_main))
            logger.error("Erreur imprévue lors de la génération de la preview:", exc_info=True)
            
    if 'final_mosaic_data_HWC' in locals() and final_mosaic_data_HWC is not None: del final_mosaic_data_HWC
    if 'final_mosaic_coverage_HW' in locals() and final_mosaic_coverage_HW is not None: del final_mosaic_coverage_HW
    gc.collect()

    # Cleanup memmap .dat files now that arrays are released (Windows requires handles closed)
    def _cleanup_memmap_artifacts():
        if not cleanup_temp_artifacts_config:
            return

        if (
            bool(coadd_use_memmap_config)
            and bool(coadd_cleanup_memmap_config)
            and coadd_memmap_dir_config
            and _path_isdir(coadd_memmap_dir_config)
        ):
            try:
                memmap_cleanup_dir = Path(coadd_memmap_dir_config).expanduser()
                for entry in memmap_cleanup_dir.iterdir():
                    name_l = entry.name.lower()
                    if entry.is_file() and name_l.endswith(".dat") and (
                        name_l.startswith("mosaic_")
                        or name_l.startswith("coverage_")
                        or name_l.startswith("zemosaic_")
                    ):
                        try:
                            entry.unlink()
                        except OSError:
                            pass
                        continue
                    if entry.is_dir() and name_l.startswith("mosaic_first_"):
                        try:
                            shutil.rmtree(str(entry), ignore_errors=False)
                        except Exception:
                            pass
            except Exception:
                pass

        try:
            runtime_temp_root = get_runtime_temp_dir()
        except Exception:
            runtime_temp_root = None
        if runtime_temp_root:
            try:
                runtime_path = Path(runtime_temp_root).expanduser()
                if runtime_path.is_dir():
                    for entry in runtime_path.iterdir():
                        if entry.is_dir() and entry.name.lower().startswith("mosaic_first_"):
                            try:
                                shutil.rmtree(str(entry), ignore_errors=False)
                            except Exception:
                                pass
            except Exception:
                pass

        wcs_candidates: list[Path] = []
        try:
            if isinstance(global_wcs_plan, dict):
                if global_wcs_plan.get("fits_path"):
                    wcs_candidates.append(Path(global_wcs_plan.get("fits_path")))
                if global_wcs_plan.get("json_path"):
                    wcs_candidates.append(Path(global_wcs_plan.get("json_path")))
        except Exception:
            pass
        default_wcs_name = str(
            (worker_config_cache or {}).get("global_wcs_output_path", "global_mosaic_wcs.fits")
        ).strip() or "global_mosaic_wcs.fits"
        if output_folder:
            output_folder_path = Path(output_folder).expanduser()
            wcs_candidates.append(output_folder_path / default_wcs_name)
            wcs_candidates.append(output_folder_path / f"{default_wcs_name}.json")
        for candidate in wcs_candidates:
            if not candidate:
                continue
            try:
                candidate_path = Path(candidate)
                if candidate_path.is_dir():
                    continue
                if candidate_path.exists() and "global_mosaic_wcs" in candidate_path.name.lower():
                    candidate_path.unlink()
            except OSError:
                pass

    _cleanup_memmap_artifacts()

    if sds_runtime_tile_dir and cleanup_temp_artifacts_config:
        try:
            shutil.rmtree(sds_runtime_tile_dir, ignore_errors=True)
        except Exception:
            pass


    # --- Phase 7 (Nettoyage) ---
    # ... (contenu Phase 7 inchangé) ...
    base_progress_phase7 = current_global_progress
    _log_memory_usage(progress_callback, "Début Phase 7 (Nettoyage)")
    pcb("run_info_phase7_cleanup_starting", prog=base_progress_phase7, lvl="INFO")
    pcb("PHASE_UPDATE:7", prog=None, lvl="ETA_LEVEL")
    telemetry.maybe_emit_stats(
        _telemetry_context({"phase_name": "Phase 7: Cleanup", "phase_index": 7})
    )
    def _log_cleanup_warning(msg_key: str, directory: str | None, exc: Exception) -> None:
        pcb(
            msg_key,
            prog=None,
            lvl="WARN",
            directory=directory or "<unknown>",
            error=str(exc),
        )

    if cache_retention_mode == "keep":
        if _path_exists(temp_image_cache_dir):
            pcb(
                "run_info_temp_preprocessed_cache_kept",
                prog=None,
                lvl="INFO_DETAIL",
                directory=temp_image_cache_dir,
            )
    else:
        if _path_exists(temp_image_cache_dir):
            try:
                shutil.rmtree(temp_image_cache_dir)
                pcb(
                    "run_info_temp_preprocessed_cache_cleaned",
                    prog=None,
                    lvl="INFO_DETAIL",
                    directory=temp_image_cache_dir,
                )
            except Exception as cache_exc:
                _log_cleanup_warning(
                    "run_warn_phase7_cache_cleanup_failed",
                    temp_image_cache_dir,
                    cache_exc,
                )

    master_tiles_dir = temp_master_tile_storage_dir
    if master_tiles_dir and _path_exists(master_tiles_dir):
        if cleanup_temp_artifacts_config:
            try:
                shutil.rmtree(master_tiles_dir)
                pcb(
                    "run_info_temp_master_tiles_fits_cleaned",
                    prog=None,
                    lvl="INFO_DETAIL",
                    directory=master_tiles_dir,
                )
            except Exception as mt_exc:
                _log_cleanup_warning(
                    "run_warn_phase7_master_tiles_fits_cleanup_failed",
                    master_tiles_dir,
                    mt_exc,
                )
        else:
            pcb(
                "run_info_temp_master_tiles_retained_cleanup_disabled",
                prog=None,
                lvl="INFO_DETAIL",
                directory=master_tiles_dir,
            )
    current_global_progress = base_progress_phase7 + PROGRESS_WEIGHT_PHASE7_CLEANUP; current_global_progress = min(100, current_global_progress)
    _log_memory_usage(progress_callback, "Fin Phase 7"); pcb("CHRONO_STOP_REQUEST", prog=None, lvl="CHRONO_LEVEL"); update_gui_eta(0)
    total_duration_sec = time.monotonic() - start_time_total_run
    pcb("run_success_processing_completed", prog=current_global_progress, lvl="SUCCESS", duration=f"{total_duration_sec:.2f}")
    gc.collect(); _log_memory_usage(progress_callback, "Fin Run Hierarchical Mosaic (après GC final)")
    _log_alignment_warning_summary()
    telemetry.close()
    logger.info(f"===== Run Hierarchical Mosaic COMPLETED in {total_duration_sec:.2f}s =====")
################################################################################
################################################################################
####

def run_hierarchical_mosaic_process(
    progress_queue,
    *args,
    solver_settings_dict=None,
    **kwargs,
):
    """Wrapper for running :func:`run_hierarchical_mosaic` in a separate process."""

    # progress_callback(stage: str, current: int, total: int)

    def queue_callback(*cb_args, **cb_kwargs):
        """Proxy callback used inside the worker process.

        It supports both legacy logging calls and the new progress
        reporting style ``progress_callback(stage, current, total)``.

        Legacy calls are forwarded unchanged as
        ``(message_key_or_raw, progress_value, level, kwargs)`` tuples.
        Stage updates are sent with ``"STAGE_PROGRESS"`` as the message key.
        """
        if (
            len(cb_args) == 3
            and not cb_kwargs
            and isinstance(cb_args[0], str)
            and isinstance(cb_args[1], int)
            and isinstance(cb_args[2], int)
        ):
            stage, current, total = cb_args
            progress_queue.put(("STAGE_PROGRESS", stage, current, {"total": total}))
            return

        message_key_or_raw = cb_args[0] if cb_args else ""
        progress_value = cb_args[1] if len(cb_args) > 1 else None
        level = cb_args[2] if len(cb_args) > 2 else cb_kwargs.pop("level", "INFO")
        if "lvl" in cb_kwargs:
            level = cb_kwargs.pop("lvl")
        # Only forward user-facing or control messages to the GUI queue
        lvl_str = str(level).upper() if isinstance(level, str) else "INFO"
        if lvl_str not in {"INFO", "WARN", "ERROR", "SUCCESS", "ETA_LEVEL", "CHRONO_LEVEL"}:
            return
        progress_queue.put((message_key_or_raw, progress_value, level, cb_kwargs))

    # Prepare arguments for run_hierarchical_mosaic from the incoming kwargs,
    # as the GUI sends everything in kwargs.
    final_kwargs = kwargs.copy()
    final_kwargs['progress_callback'] = queue_callback
    final_kwargs['solver_settings'] = solver_settings_dict

    try:
        level_cfg = final_kwargs.get("logging_level") or final_kwargs.get("logging_level_config")
        if level_cfg:
            os.environ["ZEMOSAIC_LOG_LEVEL"] = str(level_cfg)
    except Exception:
        pass

    # 1. Rename keys from GUI config name to worker function argument name
    rename_map = {
        'input_dir': 'input_folder',
        'output_dir': 'output_folder',
        'astap_executable_path': 'astap_exe_path',
        'astap_data_directory_path': 'astap_data_dir_param',
        'astap_default_search_radius': 'astap_search_radius_config',
        'astap_default_downsample': 'astap_downsample_config',
        'astap_default_sensitivity': 'astap_sensitivity_config',
        'stacking_normalize_method': 'stack_norm_method',
        'stacking_weighting_method': 'stack_weight_method',
        'stacking_rejection_algorithm': 'stack_reject_algo',
        'stacking_final_combine_method': 'stack_final_combine',
        'stacking_kappa_low': 'stack_kappa_low',
        'stacking_kappa_high': 'stack_kappa_high',
        'cluster_panel_threshold': 'cluster_threshold_config',
        'cluster_target_groups': 'cluster_target_groups_config',
        'cluster_orientation_split_deg': 'cluster_orientation_split_deg_config',
    }
    for old_key, new_key in rename_map.items():
        if old_key in final_kwargs:
            final_kwargs[new_key] = final_kwargs.pop(old_key)

    # 2. Handle special parsing for winsor limits
    if 'stacking_winsor_limits' in final_kwargs:
        limits_str = final_kwargs.pop('stacking_winsor_limits')
        try:
            parts = [float(p.strip()) for p in str(limits_str).split(',')]
            final_kwargs['parsed_winsor_limits'] = tuple(parts) if len(parts) == 2 else (0.05, 0.05)
        except:
            final_kwargs['parsed_winsor_limits'] = (0.05, 0.05)

    # 3. Add '_config' suffix where it is the convention
    sig_params = inspect.signature(run_hierarchical_mosaic).parameters
    for key in list(final_kwargs.keys()):
        config_key = f"{key}_config"
        if config_key in sig_params and key not in sig_params:
             final_kwargs[config_key] = final_kwargs.pop(key)

    # 4. Filter out any keys that are not in the function signature
    final_kwargs = {k: v for k, v in final_kwargs.items() if k in sig_params}

    # Provide defaults for required arguments that may not be in the GUI config
    if 'stack_ram_budget_gb_config' not in final_kwargs:
        final_kwargs['stack_ram_budget_gb_config'] = 0.0
    if 'num_base_workers_config' not in final_kwargs:
        final_kwargs['num_base_workers_config'] = 0

    try:
        run_hierarchical_mosaic(**final_kwargs)
    except Exception as e_proc:
        try:
            logger.exception("Worker process crashed before completion")
        except Exception:
            pass
        progress_queue.put(("PROCESS_ERROR", None, "ERROR", {"error": str(e_proc)}))
    finally:
        progress_queue.put(("PROCESS_DONE", None, "INFO", {}))

if __name__ == "__main__":
    import argparse
    import json

    parser = argparse.ArgumentParser(description="ZeMosaic worker")
    parser.add_argument("input_folder", help="Folder with input FITS")
    parser.add_argument("output_folder", help="Destination folder")
    parser.add_argument("--config", default=None, help="Optional config JSON")
    parser.add_argument("--coadd_use_memmap", action="store_true",
                        help="Write sum/cov arrays to disk via numpy.memmap")
    parser.add_argument("--coadd_memmap_dir", default=None,
                        help="Directory to store *.dat blocks")
    parser.add_argument("--coadd_cleanup_memmap", action="store_true",
                        default=True,
                        help="Delete *.dat blocks when the run finishes")
    parser.add_argument("--no_auto_limit_frames", action="store_true",
                        help="Disable automatic frame limit per master tile")
    parser.add_argument("--assembly_process_workers", type=int, default=None,
                        help="Number of processes for final assembly (0=auto)")
    parser.add_argument("-W", "--winsor-workers", type=int, default=None,
                        help="Process workers for Winsorized rejection (1-16)")
    parser.add_argument("--max-raw-per-master-tile", type=int, default=None,
                        help="Cap raw frames per master tile (0=auto)")
    parser.add_argument("--solver-settings", default=None,
                        help="Path to solver settings JSON")
    args = parser.parse_args()

    cfg = {}
    if ZEMOSAIC_CONFIG_AVAILABLE and zemosaic_config:
        cfg.update(zemosaic_config.load_config())
    if args.config:
        try:
            with open(args.config, "r", encoding="utf-8") as f:
                cfg.update(json.load(f))
        except Exception:
            pass

    solver_cfg = {}
    if args.solver_settings:
        try:
            solver_cfg = SolverSettings.load(args.solver_settings).__dict__
        except Exception:
            solver_cfg = {}
    else:
        try:
            solver_cfg = SolverSettings.load_default().__dict__
        except Exception:
            solver_cfg = SolverSettings().__dict__



def _assemble_global_mosaic_first_impl(
    raw_groups: list[list[dict]],
    *,
    global_plan: dict[str, Any],
    progress_callback: callable,
    match_background: bool,
    base_progress_phase: float | None,
    progress_weight_phase: float | None,
    start_time_total_run: float | None,
    cache_root: str | None,
    parallel_plan: ParallelPlan | None = None,
) -> tuple[np.ndarray | None, np.ndarray | None, np.ndarray | None]:
    """Best-effort implementation of the Mosaic-First reprojection+coadd workflow."""

    start_time = time.monotonic()
    pcb = lambda key, prog=None, lvl="INFO", **kwargs: _log_and_callback(
        key,
        prog,
        lvl,
        callback=progress_callback,
        **kwargs,
    )
    helper_progress_cap = 1.0
    helper_wait_fraction = 0.0
    helper_partial_gpu_artifacts = False
    cpu_eta_start_time = None
    cpu_eta_last_emit = 0.0
    CPU_ETA_MIN_INTERVAL = 4.0
    plan_rows_cpu_hint: int | None = None
    plan_rows_gpu_hint: int | None = None
    plan_chunk_cpu_hint: int | None = None
    plan_chunk_gpu_hint: int | None = None
    plan_cpu_workers_hint: int | None = None
    if parallel_plan is not None:
        try:
            cpu_hint = int(getattr(parallel_plan, "cpu_workers", 0) or 0)
        except Exception:
            cpu_hint = 0
        if cpu_hint > 0:
            plan_cpu_workers_hint = cpu_hint
        row_cpu = getattr(parallel_plan, "rows_per_chunk", None)
        if row_cpu is not None:
            try:
                plan_rows_cpu_hint = max(1, int(row_cpu))
            except Exception:
                plan_rows_cpu_hint = None
        row_gpu = getattr(parallel_plan, "gpu_rows_per_chunk", None)
        if row_gpu is not None:
            try:
                plan_rows_gpu_hint = max(1, int(row_gpu))
            except Exception:
                plan_rows_gpu_hint = None
        chunk_cpu = getattr(parallel_plan, "max_chunk_bytes", None)
        if chunk_cpu is not None:
            try:
                plan_chunk_cpu_hint = max(1, int(chunk_cpu))
            except Exception:
                plan_chunk_cpu_hint = None
        chunk_gpu = getattr(parallel_plan, "gpu_max_chunk_bytes", None)
        if chunk_gpu is not None:
            try:
                plan_chunk_gpu_hint = max(1, int(chunk_gpu))
            except Exception:
                plan_chunk_gpu_hint = None

    plan_mode = str(global_plan.get("mode") or "").strip().lower()
    plan_meta = global_plan.get("meta") if isinstance(global_plan.get("meta"), dict) else {}
    plan_meta = _coerce_to_builtin(plan_meta) if plan_meta else {}
    descriptor_payload = {
        "descriptor_mode": plan_mode,
        "descriptor_fits_path": str(global_plan.get("fits_path") or ""),
        "descriptor_json_path": str(global_plan.get("json_path") or ""),
    }
    if plan_meta:
        descriptor_payload["descriptor_meta"] = plan_meta

    def _payload(**kwargs):
        enriched = dict(descriptor_payload)
        enriched.update(kwargs)
        return enriched

    def _log_helper_cpu_resume(helper_name: str, *, reason: str | None = None, discarded: bool = False) -> None:
        """Emit a summary INFO log when resuming on CPU after helper issues."""
        resolved_reason = (reason or "unspecified").strip() or "unspecified"
        resume_key = (
            "global_coadd_info_helper_cpu_resume_discarded"
            if discarded
            else "global_coadd_info_helper_cpu_resume"
        )
        pcb(
            resume_key,
            prog=None,
            lvl="INFO",
            **_payload(helper=helper_name, reason=resolved_reason),
        )

    def _emit_global_coadd_finished_event(
        *,
        frames: int,
        channels: int | None,
        helper_label: str | None,
    ) -> None:
        """Emit a single SUCCESS log for the end of the global coadd phase."""
        final_prog = None
        if base_progress_phase is not None and progress_weight_phase is not None:
            final_prog = base_progress_phase + progress_weight_phase
        elapsed = max(0.0, time.monotonic() - start_time)
        try:
            frame_count = int(frames)
        except Exception:
            frame_count = 0
        frame_count = max(0, frame_count)
        try:
            channel_count = int(channels) if channels is not None else 0
        except Exception:
            channel_count = 0
        if channel_count <= 0:
            channel_count = 1
        payload = _payload(
            W=int(width),
            H=int(height),
            images=frame_count,
            channels=channel_count,
            elapsed_s=float(elapsed),
            method=coadd_method,
        )
        if helper_label:
            payload["helper"] = helper_label
        pcb(
            "p4_global_coadd_finished",
            prog=final_prog,
            lvl="SUCCESS",
            **payload,
        )

    def _fail(reason_key: str = "global_coadd_error_failed_fallback", *, level: str = "WARN", **kwargs):
        if reason_key != "global_coadd_error_failed_fallback":
            pcb(reason_key, prog=None, lvl=level, **_payload(**kwargs))
        pcb(
            "global_coadd_error_failed_fallback",
            prog=None,
            lvl="WARN",
            reason=reason_key,
            **_payload(**kwargs),
        )
        return None, None, None

    if not global_plan or not global_plan.get("enabled"):
        return _fail("global_coadd_error_plan_disabled")

    if not (
        REPROJECT_AVAILABLE
        and reproject_interp
        and ASTROPY_AVAILABLE
        and fits
        and ZEMOSAIC_UTILS_AVAILABLE
        and zemosaic_utils
    ):
        return _fail("global_coadd_error_descriptor_missing")

    plan_wcs = global_plan.get("wcs")
    global_wcs_obj = None
    if plan_wcs is not None and ASTROPY_AVAILABLE and WCS:
        try:
            global_wcs_obj = plan_wcs if isinstance(plan_wcs, WCS) else WCS(plan_wcs)
        except Exception:
            global_wcs_obj = None
    if global_wcs_obj is None:
        return _fail("global_coadd_error_descriptor_missing")

    try:
        width = int(global_plan.get("width") or 0)
        height = int(global_plan.get("height") or 0)
    except Exception:
        width = height = 0
    if width <= 0 or height <= 0:
        return _fail("global_coadd_error_descriptor_missing")

    logger.info("INFO [Worker] Global WCS found → running global reproject+coadd (Mosaic-First).")

    total_images = sum(
        len(group) for group in raw_groups if isinstance(group, (list, tuple))
    )
    if total_images <= 0:
        return _fail("global_coadd_error_no_inputs")

    pcb(
        "global_coadd_plan_descriptor",
        prog=None,
        lvl="INFO",
        **_payload(
            total_groups=len(raw_groups or []),
            total_images=int(total_images),
        ),
    )

    allowed_methods = {"mean", "median", "kappa_sigma", "winsorized"}
    coadd_method = str(global_plan.get("coadd_method", "kappa_sigma") or "kappa_sigma").strip().lower()
    if coadd_method not in allowed_methods:
        coadd_method = "kappa_sigma"
    stack_reject_algo = str(global_plan.get("stack_reject_algo") or coadd_method).strip().lower()
    try:
        kappa_sigma_k = float(global_plan.get("coadd_k", 2.0) or 2.0)
    except Exception:
        kappa_sigma_k = 2.0
    winsor_limits = global_plan.get("winsor_limits", (0.05, 0.05)) or (0.05, 0.05)
    if (
        not isinstance(winsor_limits, (list, tuple))
        or len(winsor_limits) < 2
    ):
        winsor_limits = (0.05, 0.05)
    try:
        low_lim = float(winsor_limits[0])
        high_lim = float(winsor_limits[1])
    except Exception:
        low_lim = high_lim = 0.05
    winsor_limits = (max(0.0, low_lim), max(0.0, high_lim))

    try:
        winsor_worker_limit = max(1, int(global_plan.get("winsor_worker_limit") or 1))
    except Exception:
        winsor_worker_limit = 1
    try:
        winsor_max_frames_per_pass = int(global_plan.get("winsor_max_frames_per_pass") or 0)
    except Exception:
        winsor_max_frames_per_pass = 0
    use_align_helpers_flag = bool(global_plan.get("use_align_helpers", False))
    prefer_gpu_helpers_flag = bool(global_plan.get("prefer_gpu_helpers", False))
    gpu_verify_tolerance = global_plan.get("gpu_helper_verify_tolerance")
    try:
        if gpu_verify_tolerance is not None:
            gpu_verify_tolerance = float(gpu_verify_tolerance)
            if not np.isfinite(gpu_verify_tolerance) or gpu_verify_tolerance <= 0:
                gpu_verify_tolerance = None
    except Exception:
        gpu_verify_tolerance = None
    if use_align_helpers_flag:
        logger.debug("[Worker] Mosaic-First align helpers hint enabled for stacking.")
    if prefer_gpu_helpers_flag and not gpu_is_available():
        logger.debug("[Worker] Mosaic-First GPU helpers requested but unavailable; falling back to CPU.")

    align_helper_fn = None
    if use_align_helpers_flag:
        if (
            ZEMOSAIC_ALIGN_STACK_AVAILABLE
            and zemosaic_align_stack
            and hasattr(zemosaic_align_stack, "equalize_rgb_medians_inplace")
        ):
            align_helper_fn = getattr(zemosaic_align_stack, "equalize_rgb_medians_inplace")
        else:
            helper_reason = "module_unavailable"
            pcb(
                "global_coadd_warn_helper_unavailable",
                prog=None,
                lvl="INFO_DETAIL",
                **_payload(helper="align_photometry", reason=helper_reason),
            )
            _log_helper_cpu_resume("align_photometry", reason=helper_reason, discarded=False)

    gpu_helper_candidate = (
        prefer_gpu_helpers_flag
        and ZEMOSAIC_UTILS_AVAILABLE
        and zemosaic_utils
        and hasattr(zemosaic_utils, "reproject_and_coadd_wrapper")
    )
    if prefer_gpu_helpers_flag and not gpu_helper_candidate:
        helper_reason = "helper_unavailable"
        pcb(
            "global_coadd_warn_helper_unavailable",
            prog=None,
            lvl="INFO_DETAIL",
            **_payload(helper="gpu_reproject", reason=helper_reason),
        )
        _log_helper_cpu_resume("gpu_reproject", reason=helper_reason, discarded=False)
    gpu_supported_methods = {"mean", "median", "winsorized", "kappa_sigma"}
    gpu_helper_supported = gpu_helper_candidate and coadd_method in gpu_supported_methods
    if gpu_helper_candidate and not gpu_helper_supported:
        reason = "unsupported_method"
        if coadd_method in allowed_methods:
            reason = "not_implemented"
        pcb(
            "global_coadd_warn_helper_unavailable",
            prog=None,
            lvl="INFO_DETAIL",
            **_payload(helper="gpu_reproject", reason=reason, method=coadd_method),
        )
        _log_helper_cpu_resume("gpu_reproject", reason=reason, discarded=False)

    start_route_label = "GPU helper (gpu_reproject)" if gpu_helper_supported else "CPU pipeline"
    start_payload = _payload(
        W=int(width),
        H=int(height),
        images=int(total_images),
        method=coadd_method,
        route=start_route_label,
        use_gpu_helper=bool(gpu_helper_supported),
    )
    if gpu_helper_supported:
        start_payload["helper"] = "gpu_reproject"
    pcb(
        "p4_global_coadd_started",
        prog=base_progress_phase if base_progress_phase is not None else None,
        lvl="INFO",
        **start_payload,
    )

    use_memmap = bool(global_plan.get("coadd_use_memmap", False))
    cleanup_temp_files = bool(global_plan.get("coadd_cleanup_memmap", True))
    memmap_root_candidate = global_plan.get("coadd_memmap_dir") or cache_root
    if memmap_root_candidate:
        memmap_root_path = Path(memmap_root_candidate).expanduser()
    else:
        memmap_root_path = ensure_user_config_dir() / "memmap"
    try:
        memmap_root_path.mkdir(parents=True, exist_ok=True)
    except Exception:
        memmap_root_path = get_runtime_temp_dir()
    run_dir_path = memmap_root_path / f"mosaic_first_{uuid.uuid4().hex}"
    try:
        run_dir_path.mkdir(parents=True, exist_ok=True)
    except Exception:
        run_dir_path = Path(
            tempfile.mkdtemp(prefix="mosaic_first_", dir=str(get_runtime_temp_dir()))
        )

    temp_artifacts: list[Path] = []
    memmap_handles: list[np.memmap] = []
    patch_entries: list[dict[str, Any]] = []

    def _allocate_array(shape, dtype, label):
        if use_memmap:
            filename = run_dir_path / f"{label}_{uuid.uuid4().hex}.dat"
            arr = np.memmap(str(filename), mode="w+", dtype=dtype, shape=shape)
            arr[...] = 0
            temp_artifacts.append(filename)
            memmap_handles.append(arr)
            return arr
        return np.zeros(shape, dtype=dtype)

    def _cleanup_temp():
        if not cleanup_temp_files:
            logger.debug("[Worker] Mosaic-First temporary data retained in %s", run_dir_path)
            return
        for handle in memmap_handles:
            try:
                handle.flush()
            except Exception:
                pass
        memmap_handles.clear()
        for artifact in temp_artifacts:
            if not artifact:
                continue
            try:
                artifact_path = Path(artifact)
                if artifact_path.is_file():
                    artifact_path.unlink()
            except Exception:
                pass
        try:
            if run_dir_path.exists():
                shutil.rmtree(run_dir_path)
        except Exception:
            pass

    def _coerce_wcs(candidate):
        if candidate is None or not (ASTROPY_AVAILABLE and WCS):
            return None
        if isinstance(candidate, WCS):
            return candidate
        try:
            return WCS(candidate)
        except Exception:
            try:
                header = candidate.to_header() if hasattr(candidate, "to_header") else candidate
                return WCS(header)
            except Exception:
                return None

    def _extract_affine(entry):
        if not isinstance(entry, dict):
            return 1.0, 0.0
        affine_pairs = [
            ("photometry_gain", "photometry_offset"),
            ("affine_gain", "affine_offset"),
            ("gain", "offset"),
            ("gain_norm", "offset_norm"),
        ]
        for gain_key, offset_key in affine_pairs:
            if gain_key in entry or offset_key in entry:
                try:
                    gain_val = float(entry.get(gain_key, 1.0))
                except Exception:
                    gain_val = 1.0
                try:
                    offset_val = float(entry.get(offset_key, 0.0))
                except Exception:
                    offset_val = 0.0
                if not np.isfinite(gain_val):
                    gain_val = 1.0
                if not np.isfinite(offset_val):
                    offset_val = 0.0
                return gain_val, offset_val
        return 1.0, 0.0

    def _load_frame(entry: dict) -> np.ndarray | None:
        if not isinstance(entry, dict):
            return None
        label = _safe_basename(entry.get("path_raw") or entry.get("path") or "frame")
        cache_paths = [
            entry.get("path_preprocessed_cache"),
            entry.get("path_preprocessed"),
        ]
        arr = None
        for path_candidate in cache_paths:
            if not path_candidate:
                continue
            try:
                if _path_exists(path_candidate):
                    arr = np.load(path_candidate, mmap_mode="r")
                    break
            except Exception:
                arr = None
        if arr is None:
            for key in ("preprocessed_data", "img_data_processed", "img_data"):
                if isinstance(entry.get(key), np.ndarray):
                    arr = entry[key]
                    break
        if arr is None:
            raw_path = entry.get("path_raw") or entry.get("path")
            if (
                raw_path
                and _path_exists(raw_path)
                and hasattr(zemosaic_utils, "load_and_validate_fits")
            ):
                try:
                    loaded = zemosaic_utils.load_and_validate_fits(
                        raw_path,
                        normalize_to_float32=True,
                        attempt_fix_nonfinite=True,
                        progress_callback=None,
                    )
                    arr = loaded[0] if isinstance(loaded, (list, tuple)) and loaded else loaded
                except Exception:
                    arr = None
        if arr is None:
            return None
        try:
            arr_hwc = _ensure_hwc_master_tile(arr, label)
        except Exception:
            return None
        return np.ascontiguousarray(arr_hwc, dtype=np.float32)

    def _prepare_frame(entry: dict) -> np.ndarray | None:
        frame = _load_frame(entry)
        if frame is None:
            return None
        if align_helper_fn and frame.ndim == 3 and frame.shape[-1] == 3:
            try:
                align_helper_fn(frame)
            except Exception:
                logger.debug("Align helper equalization failed", exc_info=True)
        gain_val, offset_val = _extract_affine(entry)
        if abs(gain_val - 1.0) > 1e-6:
            frame = np.multiply(frame, gain_val, dtype=np.float32)
        if offset_val != 0.0:
            frame = frame + np.float32(offset_val)
        return frame

    def _compute_bbox(local_wcs):
        if local_wcs is None:
            return None
        try:
            footprint = local_wcs.calc_footprint()
        except Exception:
            return None
        footprint_arr = np.asarray(footprint, dtype=np.float64)
        if footprint_arr.ndim != 2 or footprint_arr.shape[1] < 2:
            return None
        try:
            xs, ys = global_wcs_obj.world_to_pixel_values(
                footprint_arr[:, 0], footprint_arr[:, 1]
            )
        except Exception:
            try:
                xs, ys = global_wcs_obj.world_to_pixel(
                    footprint_arr[:, 0], footprint_arr[:, 1]
                )
            except Exception:
                return None
        xs = np.asarray(xs, dtype=np.float64)
        ys = np.asarray(ys, dtype=np.float64)
        if xs.size == 0 or ys.size == 0:
            return None
        xmin = int(math.floor(np.nanmin(xs))) - 2
        xmax = int(math.ceil(np.nanmax(xs))) + 2
        ymin = int(math.floor(np.nanmin(ys))) - 2
        ymax = int(math.ceil(np.nanmax(ys))) + 2
        xmin = max(0, xmin)
        ymin = max(0, ymin)
        xmax = min(width, xmax)
        ymax = min(height, ymax)
        if xmin >= xmax or ymin >= ymax:
            return None
        return ymin, ymax, xmin, xmax

    def _reproject_frame(frame: np.ndarray, local_wcs) -> tuple[np.ndarray, np.ndarray, tuple[int, int, int, int]] | None:
        bbox = _compute_bbox(local_wcs)
        if bbox is None:
            return None
        y0, y1, x0, x1 = bbox
        h_patch = y1 - y0
        w_patch = x1 - x0
        if h_patch <= 0 or w_patch <= 0:
            return None
        try:
            sub_wcs = global_wcs_obj.deepcopy()
        except Exception:
            sub_wcs = copy.deepcopy(global_wcs_obj)
        try:
            sub_wcs.wcs.crpix = [
                global_wcs_obj.wcs.crpix[0] - x0,
                global_wcs_obj.wcs.crpix[1] - y0,
            ]
        except Exception:
            pass
        planes: list[np.ndarray] = []
        footprint = None
        for ch in range(frame.shape[-1]):
            plane = frame[..., ch]
            if footprint is None:
                reproj_plane, footprint = reproject_interp(
                    (plane, local_wcs),
                    sub_wcs,
                    shape_out=(h_patch, w_patch),
                    return_footprint=True,
                    order="bilinear",
                    parallel=False,
                )
            else:
                reproj_plane = reproject_interp(
                    (plane, local_wcs),
                    sub_wcs,
                    shape_out=(h_patch, w_patch),
                    return_footprint=False,
                    order="bilinear",
                    parallel=False,
                )
            planes.append(np.asarray(reproj_plane, dtype=np.float32))
        if footprint is None:
            return None
        weight = np.clip(np.nan_to_num(footprint, nan=0.0), 0.0, 1.0).astype(np.float32, copy=False)
        patch = np.stack(planes, axis=-1)
        finite_mask = np.all(np.isfinite(patch), axis=-1)
        weight = np.where(finite_mask, weight, 0.0)
        if not np.any(weight > 0):
            return None
        patch = np.nan_to_num(patch, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32, copy=False)
        if match_background:
            try:
                valid_mask = weight > 0
                if np.any(valid_mask):
                    for channel_idx in range(patch.shape[-1]):
                        channel_view = patch[..., channel_idx]
                        med_val = float(np.nanmedian(channel_view[valid_mask]))
                        if np.isfinite(med_val):
                            channel_view -= np.float32(med_val)
            except Exception:
                logger.debug("Global coadd per-patch background match failed", exc_info=True)
        return patch, weight, bbox

    def _maybe_emit_cpu_eta(done_count: int) -> None:
        nonlocal cpu_eta_start_time, cpu_eta_last_emit
        if helper_progress_cap < 1.0:
            return
        if done_count <= 0 or total_images <= 0:
            return
        now = time.monotonic()
        if cpu_eta_start_time is None:
            cpu_eta_start_time = now
        if now - cpu_eta_last_emit < CPU_ETA_MIN_INTERVAL:
            return
        elapsed = max(0.0, now - cpu_eta_start_time)
        if elapsed <= 0:
            return
        remaining = max(0, total_images - done_count)
        if remaining <= 0:
            return
        seconds_per_frame = elapsed / max(1, done_count)
        eta_seconds = max(0.0, seconds_per_frame * remaining)
        h, rem = divmod(int(eta_seconds + 0.5), 3600)
        m, s = divmod(rem, 60)
        pcb(f"ETA_UPDATE:{h:02d}:{m:02d}:{s:02d}", prog=None, lvl="ETA_LEVEL")
        cpu_eta_last_emit = now

    def _emit_progress(
        done_count: int,
        *,
        entry: dict | None = None,
        group_index: int | None = None,
        entry_index: int | None = None,
        valid_frames_count: int = 0,
    ) -> None:
        prog_value = None
        if (
            base_progress_phase is not None
            and progress_weight_phase is not None
            and total_images > 0
        ):
            frac = done_count / max(1, total_images)
            if helper_progress_cap < 1.0:
                frac = min(frac, helper_progress_cap)
            prog_value = base_progress_phase + progress_weight_phase * frac
        entry_path = None
        if isinstance(entry, dict):
            entry_path = (
                entry.get("path_raw")
                or entry.get("path")
                or entry.get("path_preprocessed_cache")
                or entry.get("path_preprocessed")
            )
        payload: dict[str, Any] = {
            "done": int(done_count),
            "total": int(total_images),
            "valid": int(max(0, valid_frames_count)),
            "method": coadd_method,
        }
        if entry_path:
            payload["path"] = _safe_basename(entry_path)
        if group_index is not None:
            payload["group_index"] = int(group_index)
        if entry_index is not None:
            payload["entry_index"] = int(entry_index)
        pcb(
            "p4_global_coadd_progress",
            prog=prog_value,
            lvl="INFO",
            **_payload(**payload),
        )
        _maybe_emit_cpu_eta(int(done_count))

    def _attempt_gpu_helper_route() -> tuple[np.ndarray, np.ndarray, np.ndarray | None, dict[str, int]] | None:
        nonlocal helper_partial_gpu_artifacts
        helper_entries: list[tuple[dict, Any]] = []
        helper_seen = 0
        helper_valid = 0
        for group_index, group in enumerate(raw_groups or []):
            if not isinstance(group, (list, tuple)):
                continue
            for entry_index, entry in enumerate(group):
                helper_seen += 1
                if not _entry_is_seestar(entry):
                    _emit_progress(
                        helper_seen,
                        entry=entry,
                        group_index=group_index,
                        entry_index=entry_index,
                        valid_frames_count=helper_valid,
                    )
                    continue
                local_wcs = _coerce_wcs(entry.get("wcs") or entry.get("phase0_wcs"))
                if local_wcs is None:
                    _emit_progress(
                        helper_seen,
                        entry=entry,
                        group_index=group_index,
                        entry_index=entry_index,
                        valid_frames_count=helper_valid,
                    )
                    continue
                helper_entries.append((entry, local_wcs))
                helper_valid += 1
                _emit_progress(
                    helper_seen,
                    entry=entry,
                    group_index=group_index,
                    entry_index=entry_index,
                    valid_frames_count=helper_valid,
                )
        if not helper_entries:
            return None
        channel_count = None
        prefetched_entry_id: int | None = None
        prefetched_frame: np.ndarray | None = None
        for entry, _ in helper_entries:
            frame = _prepare_frame(entry)
            if frame is None:
                continue
            channel_count = frame.shape[-1]
            prefetched_entry_id = id(entry)
            prefetched_frame = frame
            break
        if channel_count is None or channel_count <= 0:
            return None
        helper_stats = {"frames": int(helper_valid), "channels": int(channel_count)}
        helper_payload = _payload(
            helper="gpu_reproject",
            frames=int(helper_stats["frames"]),
            channels=int(helper_stats["channels"]),
            grid_w=int(width),
            grid_h=int(height),
        )
        pcb(
            "global_coadd_info_helper_path",
            prog=None,
            lvl="INFO",
            **helper_payload,
        )
        pcb(
            "global_coadd_info_helper_magic_wait",
            prog=None,
            lvl="INFO_DETAIL",
            **helper_payload,
        )
        final_channels: list[np.ndarray] = []
        coverage_map: np.ndarray | None = None

        for channel_idx in range(channel_count):
            channel_start_time = time.monotonic()
            data_list: list[np.ndarray] = []
            wcs_list_local: list[Any] = []
            for entry, local_wcs in helper_entries:
                frame: np.ndarray | None
                if prefetched_entry_id is not None and id(entry) == prefetched_entry_id and prefetched_frame is not None:
                    frame = prefetched_frame
                else:
                    frame = _prepare_frame(entry)
                if frame is None or channel_idx >= frame.shape[-1]:
                    continue
                data_list.append(np.ascontiguousarray(frame[..., channel_idx], dtype=np.float32))
                wcs_list_local.append(local_wcs)
            if not data_list:
                if final_channels:
                    helper_partial_gpu_artifacts = True
                return None
            gpu_reproj_kwargs: dict[str, Any] = {}
            row_hint_gpu = plan_rows_gpu_hint or plan_rows_cpu_hint
            if row_hint_gpu:
                gpu_reproj_kwargs["rows_per_chunk"] = int(max(1, row_hint_gpu))
            chunk_hint_gpu = plan_chunk_gpu_hint or plan_chunk_cpu_hint
            if chunk_hint_gpu:
                gpu_reproj_kwargs["max_chunk_bytes"] = int(max(1, chunk_hint_gpu))
            try:
                chan_mosaic, chan_cov = zemosaic_utils.reproject_and_coadd_wrapper(
                    data_list=data_list,
                    wcs_list=wcs_list_local,
                    shape_out=(height, width),
                    output_projection=global_wcs_obj,
                    reproject_function=reproject_interp,
                    combine_function=coadd_method,
                    stack_reject_algo=stack_reject_algo,
                    winsor_limits=winsor_limits,
                    coadd_k=kappa_sigma_k,
                    cpu_func=reproject_and_coadd,
                    use_gpu=True,
                    allow_cpu_fallback=False,
                    match_background=match_background,
                    progress_callback=pcb,
                    **gpu_reproj_kwargs,
                )
            except Exception as exc:
                logger.warning(
                    "[Worker] Global GPU helper path failed on channel %d: %s",
                    channel_idx,
                    exc,
                )
                if final_channels:
                    helper_partial_gpu_artifacts = True
                return None
            if gpu_verify_tolerance is not None:
                cpu_reproj_kwargs: dict[str, Any] = {}
                cpu_rows_hint = plan_rows_cpu_hint or plan_rows_gpu_hint
                if cpu_rows_hint:
                    cpu_reproj_kwargs["rows_per_chunk"] = int(max(1, cpu_rows_hint))
                cpu_chunk_hint = plan_chunk_cpu_hint or plan_chunk_gpu_hint
                if cpu_chunk_hint:
                    cpu_reproj_kwargs["max_chunk_bytes"] = int(max(1, cpu_chunk_hint))
                if plan_cpu_workers_hint:
                    cpu_reproj_kwargs["process_workers"] = int(plan_cpu_workers_hint)
                try:
                    cpu_mosaic, _ = zemosaic_utils.reproject_and_coadd_wrapper(
                        data_list=data_list,
                        wcs_list=wcs_list_local,
                        shape_out=(height, width),
                        output_projection=global_wcs_obj,
                        reproject_function=reproject_interp,
                        combine_function=coadd_method,
                        stack_reject_algo=stack_reject_algo,
                        winsor_limits=winsor_limits,
                        coadd_k=kappa_sigma_k,
                        cpu_func=reproject_and_coadd,
                        use_gpu=False,
                        match_background=match_background,
                        progress_callback=pcb,
                        **cpu_reproj_kwargs,
                    )
                    try:
                        diff = float(np.nanmax(np.abs(cpu_mosaic - chan_mosaic)))
                    except ValueError:
                        diff = 0.0
                    lvl = "INFO_DETAIL" if diff <= gpu_verify_tolerance else "WARN"
                    pcb(
                        "global_coadd_gpu_verify",
                        prog=None,
                        lvl=lvl,
                        **_payload(helper="gpu_reproject", method=coadd_method, channel=int(channel_idx), max_delta=float(diff)),
                    )
                except Exception as exc:
                    pcb(
                        "global_coadd_gpu_verify",
                        prog=None,
                        lvl="WARN",
                        **_payload(
                            helper="gpu_reproject",
                            method=coadd_method,
                            channel=int(channel_idx),
                            error=str(exc),
                        ),
                    )
            final_channels.append(np.asarray(chan_mosaic, dtype=np.float32))
            if coverage_map is None:
                coverage_map = np.asarray(chan_cov, dtype=np.float32)
            channel_elapsed = max(0.0, time.monotonic() - channel_start_time)
            pcb(
                "global_coadd_helper_channel_progress",
                prog=None,
                lvl="INFO_DETAIL",
                **_payload(
                    helper="gpu_reproject",
                    method=coadd_method,
                    channel=int(channel_idx + 1),
                    channels=int(channel_count),
                    elapsed_s=float(channel_elapsed),
                ),
            )
        if not final_channels or coverage_map is None:
            if final_channels:
                helper_partial_gpu_artifacts = True
            return None
        final_image = (
            final_channels[0][..., np.newaxis]
            if len(final_channels) == 1
            else np.stack(final_channels, axis=-1)
        )
        final_image = np.nan_to_num(final_image, nan=0.0, posinf=0.0, neginf=0.0)
        coverage_map = np.nan_to_num(coverage_map, nan=0.0, posinf=0.0, neginf=0.0)
        alpha_map = None
        if np.any(coverage_map > 0):
            max_cov = float(np.nanmax(coverage_map))
            if max_cov > 0:
                alpha_map = np.clip((coverage_map / max_cov) * 255.0, 0, 255).astype(np.uint8)
        if prefetched_frame is not None:
            del prefetched_frame
        _emit_progress(max(helper_seen, total_images), valid_frames_count=int(helper_valid))
        return final_image, coverage_map, alpha_map, helper_stats

    if gpu_helper_supported:
        helper_wait_fraction = 0.12
        helper_progress_cap = max(0.0, 1.0 - helper_wait_fraction)
        helper_attempt = _attempt_gpu_helper_route()
        if helper_attempt is not None:
            helper_image, helper_coverage, helper_alpha, helper_stats = helper_attempt
            _emit_global_coadd_finished_event(
                frames=int(helper_stats.get("frames", 0)),
                channels=int(helper_stats.get("channels", 0)),
                helper_label="gpu_reproject",
            )
            _emit_coverage_summary_log(
                pcb,
                coverage_array=helper_coverage,
                width=width,
                height=height,
                log_key="global_coadd_coverage_summary",
                base_payload=_payload(route="gpu_helper"),
                label="gpu_helper",
            )
            return (
                helper_image.astype(np.float32, copy=False),
                helper_coverage.astype(np.float32, copy=False),
                helper_alpha,
            )
        else:
            helper_progress_cap = 1.0
            helper_wait_fraction = 0.0
            helper_reason = "helper_failed"
            pcb(
                "global_coadd_warn_helper_fallback",
                prog=None,
                lvl="INFO_DETAIL",
                **_payload(helper="gpu_reproject", reason=helper_reason),
            )
            _log_helper_cpu_resume(
                "gpu_reproject",
                reason=helper_reason,
                discarded=bool(helper_partial_gpu_artifacts),
            )
            helper_partial_gpu_artifacts = False

    pcb(
        "global_coadd_info_cpu_path",
        prog=None,
        lvl="INFO_DETAIL",
        **_payload(
            frames=int(total_images),
            grid_w=int(width),
            grid_h=int(height),
            method=coadd_method,
        ),
    )
    pcb(
        "global_coadd_info_cpu_magic_wait",
        prog=None,
        lvl="INFO_DETAIL",
        **_payload(
            frames=int(total_images),
            grid_w=int(width),
            grid_h=int(height),
        ),
    )

    store_patches = coadd_method in {"median", "winsorized", "kappa_sigma"}
    sum_grid: np.ndarray | None = None
    sumsq_grid: np.ndarray | None = None
    weight_grid: np.ndarray | None = None
    count_grid: np.ndarray | None = None
    channel_count: int | None = None

    images_seen = 0
    valid_frames = 0
    try:
        for group_index, group in enumerate(raw_groups or []):
            if not isinstance(group, (list, tuple)):
                continue
            for entry_index, entry in enumerate(group):
                images_seen += 1
                try:
                    if not _entry_is_seestar(entry):
                        entry_path = (
                            entry.get("path_raw")
                            or entry.get("path")
                            or entry.get("path_preprocessed_cache")
                            or entry.get("path_preprocessed")
                            or ""
                        )
                        pcb(
                            "global_coadd_warn_non_seestar_entry",
                            prog=None,
                            lvl="WARN",
                            **_payload(
                                group_index=int(group_index),
                                entry_index=int(entry_index),
                                path=_safe_basename(entry_path) if entry_path else "unknown",
                            ),
                        )
                        continue
                    local_wcs = _coerce_wcs(entry.get("wcs") or entry.get("phase0_wcs"))
                    if local_wcs is None:
                        logger.warning("[Worker] Global coadd: skipping frame without WCS")
                        continue
                    frame = _prepare_frame(entry)
                    if frame is None:
                        logger.warning(
                            "[Worker] Global coadd: unable to load frame %s",
                            entry.get("path_raw") or entry.get("path") or "unknown",
                        )
                        continue
                    reproj_result = _reproject_frame(frame, local_wcs)
                    if reproj_result is None:
                        continue
                    patch_data, patch_weight, bbox = reproj_result
                    if channel_count is None:
                        channel_count = patch_data.shape[-1]
                        sum_grid = _allocate_array((height, width, channel_count), np.float64, "sum")
                        weight_grid = _allocate_array((height, width), np.float32, "weight")
                        count_grid = _allocate_array((height, width), np.float32, "count")
                        if coadd_method == "kappa_sigma":
                            sumsq_grid = _allocate_array((height, width, channel_count), np.float64, "sumsq")
                    y0, y1, x0, x1 = bbox
                    weighted_patch = patch_weight[..., None]
                    sum_grid[y0:y1, x0:x1, :] += (patch_data * weighted_patch).astype(np.float64)
                    weight_grid[y0:y1, x0:x1] += patch_weight.astype(np.float32)
                    count_grid[y0:y1, x0:x1] += (patch_weight > 0).astype(np.float32)
                    if sumsq_grid is not None:
                        sumsq_grid[y0:y1, x0:x1, :] += ((patch_data ** 2) * weighted_patch).astype(np.float64)
                    if store_patches:
                        data_path = run_dir_path / f"patch_{valid_frames:05d}.npy"
                        weight_path = run_dir_path / f"weight_{valid_frames:05d}.npy"
                        np.save(str(data_path), patch_data, allow_pickle=False)
                        np.save(str(weight_path), patch_weight, allow_pickle=False)
                        temp_artifacts.extend([data_path, weight_path])
                        patch_entries.append(
                            {
                                "data_path": str(data_path),
                                "weight_path": str(weight_path),
                                "bbox": bbox,
                            }
                        )
                    valid_frames += 1
                except MemoryError:
                    pcb(
                        "global_coadd_error_failed_fallback",
                        prog=None,
                        lvl="ERROR",
                        **_payload(error="memory_error"),
                    )
                    raise
                except Exception as exc:
                    logger.warning("[Worker] Global coadd: frame skipped (%s)", exc)
                finally:
                    _emit_progress(
                        images_seen,
                        entry=entry,
                        group_index=group_index,
                        entry_index=entry_index,
                        valid_frames_count=valid_frames,
                    )
        if valid_frames == 0 or sum_grid is None or weight_grid is None:
            return _fail("global_coadd_error_no_valid_frames")

        final_image: np.ndarray | None = None
        coverage_map: np.ndarray | None = None

        def _finalize_mean() -> tuple[np.ndarray, np.ndarray]:
            weight_expanded = np.expand_dims(weight_grid, axis=-1)
            with np.errstate(invalid="ignore", divide="ignore"):
                result = sum_grid / weight_expanded
            result = np.nan_to_num(result, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32, copy=False)
            coverage = np.nan_to_num(weight_grid, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32, copy=False)
            return result, coverage

        def _finalize_kappa_sigma() -> tuple[np.ndarray, np.ndarray]:
            weight_expanded = np.expand_dims(weight_grid, axis=-1)
            with np.errstate(invalid="ignore", divide="ignore"):
                mean_map = sum_grid / weight_expanded
                second_moment = sumsq_grid / weight_expanded
            mean_map = np.nan_to_num(mean_map, nan=0.0, posinf=0.0, neginf=0.0)
            second_moment = np.nan_to_num(second_moment, nan=0.0, posinf=0.0, neginf=0.0)
            variance = np.maximum(second_moment - (mean_map ** 2), 0.0)
            std_map = np.sqrt(variance, dtype=np.float64)
            std_map = np.nan_to_num(std_map, nan=0.0)
            clip_sum = np.zeros_like(sum_grid, dtype=np.float64)
            clip_weight = np.zeros_like(weight_grid, dtype=np.float32)
            min_sigma = np.percentile(std_map[np.isfinite(std_map)], 5) if np.any(np.isfinite(std_map)) else 0.0
            min_sigma = max(min_sigma, 1e-4)
            for entry in patch_entries:
                data_mm = np.load(entry["data_path"], mmap_mode='r')
                weight_mm = np.load(entry["weight_path"], mmap_mode='r')
                y0, y1, x0, x1 = entry["bbox"]
                data_slice = data_mm
                weight_slice = weight_mm
                mean_slice = mean_map[y0:y1, x0:x1, :]
                std_slice = std_map[y0:y1, x0:x1, :]
                count_slice = count_grid[y0:y1, x0:x1]
                thresholds = kappa_sigma_k * np.maximum(std_slice, min_sigma)
                diff = np.abs(data_slice - mean_slice)
                pixel_accept = np.all(diff <= thresholds, axis=-1)
                pixel_accept = np.where(count_slice <= 1.5, True, pixel_accept)
                if not np.any(pixel_accept):
                    continue
                weights = weight_slice * pixel_accept.astype(np.float32)
                if not np.any(weights > 0):
                    continue
                clip_weight[y0:y1, x0:x1] += weights
                clip_sum[y0:y1, x0:x1, :] += (data_slice * weights[..., None]).astype(np.float64)
            with np.errstate(invalid="ignore", divide="ignore"):
                clipped = clip_sum / np.expand_dims(clip_weight, axis=-1)
            clipped = np.where(
                np.expand_dims(clip_weight, axis=-1) > 0,
                clipped,
                mean_map,
            )
            coverage = np.where(clip_weight > 0, clip_weight, weight_grid)
            return clipped.astype(np.float32, copy=False), coverage.astype(np.float32, copy=False)

        def _compute_chunk_height() -> int:
            if height <= 0:
                return 0
            entries = max(1, len(patch_entries))
            if winsor_max_frames_per_pass > 0:
                entries = min(entries, winsor_max_frames_per_pass)
            entries = min(entries, winsor_worker_limit)
            bytes_per_row = width * max(1, channel_count or 1) * 4 * entries
            target_bytes = 256 * 1024 * 1024
            if bytes_per_row <= 0:
                return min(height, 128)
            chunk = target_bytes // bytes_per_row
            chunk = max(8, min(height, int(chunk)))
            return chunk or min(height, 64)

        def _finalize_chunked(method: str) -> tuple[np.ndarray, np.ndarray]:
            chunk_h = _compute_chunk_height()
            if chunk_h <= 0:
                chunk_h = min(height, 128)
            final = np.zeros((height, width, channel_count), dtype=np.float32)
            coverage = np.zeros((height, width), dtype=np.float32)
            for y0 in range(0, height, chunk_h):
                y1 = min(height, y0 + chunk_h)
                overlaps = [
                    entry
                    for entry in patch_entries
                    if not (entry["bbox"][1] <= y0 or entry["bbox"][0] >= y1)
                ]
                if not overlaps:
                    continue
                stack = np.full(
                    (len(overlaps), y1 - y0, width, channel_count),
                    np.nan,
                    dtype=np.float32,
                )
                weight_stack = np.zeros((len(overlaps), y1 - y0, width), dtype=np.float32)
                for idx, entry in enumerate(overlaps):
                    data_mm = np.load(entry["data_path"], mmap_mode='r')
                    weight_mm = np.load(entry["weight_path"], mmap_mode='r')
                    y_start, y_end, x_start, x_end = entry["bbox"]
                    sub_y0 = max(y0, y_start)
                    sub_y1 = min(y1, y_end)
                    if sub_y0 >= sub_y1:
                        continue
                    local_y0 = sub_y0 - y_start
                    local_y1 = sub_y1 - y_start
                    global_y0 = sub_y0 - y0
                    global_y1 = sub_y1 - y0
                    stack[idx, global_y0:global_y1, x_start:x_end, :] = data_mm[local_y0:local_y1, :, :]
                    weight_stack[idx, global_y0:global_y1, x_start:x_end] = weight_mm[local_y0:local_y1, :]
                if method == "median":
                    chunk_result = np.nanmedian(stack, axis=0)
                    chunk_weight = np.nansum(weight_stack, axis=0)
                else:
                    low_pct = max(0.0, min(100.0, winsor_limits[0] * 100.0))
                    high_pct = max(0.0, min(100.0, 100.0 - winsor_limits[1] * 100.0))
                    lower = np.nanpercentile(stack, low_pct, axis=0)
                    upper = np.nanpercentile(stack, high_pct, axis=0)
                    clipped = np.clip(stack, lower, upper)
                    weighted = clipped * weight_stack[..., None]
                    chunk_weight = np.nansum(weight_stack, axis=0)
                    with np.errstate(invalid="ignore", divide="ignore"):
                        chunk_result = np.nansum(weighted, axis=0) / np.expand_dims(chunk_weight, axis=-1)
                chunk_result = np.nan_to_num(chunk_result, nan=0.0).astype(np.float32, copy=False)
                chunk_weight = np.nan_to_num(chunk_weight, nan=0.0).astype(np.float32, copy=False)
                final[y0:y1, :, :] = chunk_result
                coverage[y0:y1, :] = chunk_weight
            return final, coverage

        if coadd_method == "mean":
            final_image, coverage_map = _finalize_mean()
        elif coadd_method == "kappa_sigma":
            final_image, coverage_map = _finalize_kappa_sigma()
        else:
            final_image, coverage_map = _finalize_chunked(coadd_method)

        if final_image is None or coverage_map is None:
            return _fail("global_coadd_error_finalize_failed")

        final_image = np.nan_to_num(final_image, nan=0.0, posinf=0.0, neginf=0.0)
        coverage_map = np.nan_to_num(coverage_map, nan=0.0, posinf=0.0, neginf=0.0)
        alpha_map = None
        if np.any(coverage_map > 0):
            max_cov = float(np.nanmax(coverage_map))
            if max_cov > 0:
                alpha_map = np.clip((coverage_map / max_cov) * 255.0, 0, 255).astype(np.uint8)
        computed_channels: int | None = channel_count
        if computed_channels is None and final_image is not None:
            computed_channels = final_image.shape[-1] if final_image.ndim >= 3 else 1
        _emit_global_coadd_finished_event(
            frames=int(valid_frames),
            channels=computed_channels,
            helper_label="cpu",
        )
        sum_grid = None
        sumsq_grid = None
        weight_grid = None
        count_grid = None
        _emit_coverage_summary_log(
            pcb,
            coverage_array=coverage_map,
            width=width,
            height=height,
            log_key="global_coadd_coverage_summary",
            base_payload=_payload(route="cpu"),
            label="cpu",
        )
        return final_image.astype(np.float32, copy=False), coverage_map.astype(np.float32, copy=False), alpha_map
    finally:
        _cleanup_temp()


def _build_sds_batches_runtime(
    entry_infos: list[dict[str, Any]],
    grid_h: int,
    grid_w: int,
    *,
    coverage_threshold: float,
    min_batch_size: int,
    target_batch_size: int,
) -> list[list[dict[str, Any]]]:
    """Apply SDS batch policy on entry infos using coverage + min/target sizing."""

    if not entry_infos or grid_h <= 0 or grid_w <= 0:
        return []
    try:
        coverage_threshold = float(coverage_threshold)
    except Exception:
        coverage_threshold = 0.92
    coverage_threshold = max(0.10, min(0.99, coverage_threshold))
    try:
        min_batch_size = int(min_batch_size)
    except Exception:
        min_batch_size = 5
    try:
        target_batch_size = int(target_batch_size)
    except Exception:
        target_batch_size = 10
    min_batch_size = max(1, min_batch_size)
    target_batch_size = max(min_batch_size, target_batch_size)

    total_cells = grid_h * grid_w
    batches: list[list[dict[str, Any]]] = []
    current_batch: list[dict[str, Any]] = []
    coverage_grid = np.zeros((grid_h, grid_w), dtype=np.uint8)
    coverage_cells = 0
    for info in entry_infos:
        bbox = info.get("grid_bbox", (0, 0, 0, 0))
        if not isinstance(bbox, (list, tuple)) or len(bbox) != 4:
            continue
        gy0, gy1, gx0, gx1 = bbox
        if gy1 <= gy0 or gx1 <= gx0:
            continue
        region = coverage_grid[gy0:gy1, gx0:gx1]
        existing = int(region.sum())
        region_cells = (gy1 - gy0) * (gx1 - gx0)
        region[...] = 1
        gain = max(0, region_cells - existing)
        coverage_cells += gain
        current_batch.append(info)
        batch_len = len(current_batch)
        coverage_fraction = (coverage_cells / total_cells) if total_cells else 1.0
        if (batch_len >= min_batch_size and coverage_fraction >= coverage_threshold) or (
            batch_len >= target_batch_size
        ):
            batches.append(current_batch)
            coverage_grid = np.zeros((grid_h, grid_w), dtype=np.uint8)
            coverage_cells = 0
            current_batch = []
    if current_batch:
        if len(current_batch) < min_batch_size and batches:
            batches[-1].extend(current_batch)
        else:
            batches.append(current_batch)
    return batches


def assemble_global_mosaic_sds(
    seastar_groups: list[list[dict]],
    *,
    global_plan: dict[str, Any],
    progress_callback: callable,
    match_background: bool,
    base_progress_phase: float | None,
    progress_weight_phase: float | None,
    start_time_total_run: float | None,
    cache_root: str | None,
    stack_params: dict[str, Any] | None = None,
    coverage_threshold: float = 0.92,
    min_batch_size: int = 5,
    target_batch_size: int = 10,
    preplan_path_groups: list[list[str]] | None = None,
    postprocess_context: dict[str, Any] | None = None,
    parallel_plan: ParallelPlan | None = None,
) -> tuple[np.ndarray | None, np.ndarray | None, np.ndarray | None]:
    """Batch Mosaic-First workflow that stacks per-batch mosaics in the global WCS."""

    if not global_plan or not global_plan.get("enabled"):
        return None, None, None
    if not (ASTROPY_AVAILABLE and WCS):
        return None, None, None

    try:
        width = int(global_plan.get("width") or 0)
        height = int(global_plan.get("height") or 0)
    except Exception:
        width = height = 0
    if width <= 0 or height <= 0:
        return None, None, None

    plan_wcs = global_plan.get("wcs")
    try:
        global_wcs_obj = plan_wcs if isinstance(plan_wcs, WCS) else WCS(plan_wcs)
    except Exception:
        global_wcs_obj = None
    if global_wcs_obj is None:
        return None, None, None

    assembly_start = time.monotonic()

    pcb = lambda key, prog=None, lvl="INFO", **kwargs: _log_and_callback(
        key,
        prog,
        lvl,
        callback=progress_callback,
        **kwargs,
    )

    def _coerce_wcs(candidate):
        if candidate is None:
            return None
        if isinstance(candidate, WCS):
            return candidate
        try:
            return WCS(candidate)
        except Exception:
            try:
                header = candidate.to_header() if hasattr(candidate, "to_header") else candidate
                return WCS(header)
            except Exception:
                return None

    def _extract_shape(entry: dict) -> tuple[int, int] | None:
        shape_candidate = (
            entry.get("shape")
            or entry.get("phase0_shape")
            or entry.get("phase1_shape")
            or entry.get("preprocessed_shape")
        )
        if isinstance(shape_candidate, (list, tuple)) and len(shape_candidate) >= 2:
            try:
                h = int(shape_candidate[0])
                w = int(shape_candidate[1])
                if h > 0 and w > 0:
                    return h, w
            except Exception:
                pass
        height_key = entry.get("height") or entry.get("img_height")
        width_key = entry.get("width") or entry.get("img_width")
        if height_key and width_key:
            try:
                h = int(height_key)
                w = int(width_key)
                if h > 0 and w > 0:
                    return h, w
            except Exception:
                return None
        return None

    tile_records: list[tuple[str, Any]] = []
    tile_temp_dir: str | None = None

    def _ensure_tile_workspace() -> str | None:
        nonlocal tile_temp_dir
        if tile_temp_dir:
            return tile_temp_dir
        base_dir = cache_root or get_runtime_temp_dir()
        try:
            tile_temp_dir = tempfile.mkdtemp(prefix="sds_tiles_", dir=str(base_dir))
        except Exception:
            try:
                tile_temp_dir = tempfile.mkdtemp(prefix="sds_tiles_")
            except Exception:
                tile_temp_dir = None
        return tile_temp_dir

    def _coverage_bbox_from_array(coverage_arr: np.ndarray | None) -> tuple[int, int, int, int] | None:
        if coverage_arr is None:
            return None
        try:
            arr = np.asarray(coverage_arr, dtype=np.float32, copy=False)
            if arr.ndim != 2:
                arr = np.squeeze(arr)
            if arr.ndim != 2:
                return None
            mask = np.isfinite(arr) & (arr > 1e-6)
            if not np.any(mask):
                return None
            ys = np.any(mask, axis=1)
            xs = np.any(mask, axis=0)
            y_idx = np.where(ys)[0]
            x_idx = np.where(xs)[0]
            if y_idx.size == 0 or x_idx.size == 0:
                return None
            return int(y_idx[0]), int(y_idx[-1]) + 1, int(x_idx[0]), int(x_idx[-1]) + 1
        except Exception:
            return None

    def _slice_wcs_for_bbox(tile_wcs: Any, *, x0: int, y0: int, width_px: int, height_px: int) -> Any:
        if tile_wcs is None:
            return None
        try:
            wcs_copy = copy.deepcopy(tile_wcs)
        except Exception:
            wcs_copy = tile_wcs
        try:
            if hasattr(wcs_copy, "wcs") and hasattr(wcs_copy.wcs, "crpix"):
                wcs_copy.wcs.crpix[0] -= float(x0)
                wcs_copy.wcs.crpix[1] -= float(y0)
            if hasattr(wcs_copy, "pixel_shape") and width_px > 0 and height_px > 0:
                try:
                    wcs_copy.pixel_shape = (width_px, height_px)
                except Exception:
                    pass
        except Exception:
            pass
        return wcs_copy

    def _persist_sds_tile(
        mosaic_arr: np.ndarray,
        alpha_arr: np.ndarray | None,
        coverage_arr: np.ndarray | None,
        tile_index: int,
    ) -> tuple[str, Any] | None:
        bbox = _coverage_bbox_from_array(coverage_arr)
        if bbox is None:
            bbox = (0, mosaic_arr.shape[0], 0, mosaic_arr.shape[1])
        y0, y1, x0, x1 = bbox
        y0 = max(0, min(mosaic_arr.shape[0], y0))
        y1 = max(y0 + 1, min(mosaic_arr.shape[0], y1))
        x0 = max(0, min(mosaic_arr.shape[1], x0))
        x1 = max(x0 + 1, min(mosaic_arr.shape[1], x1))
        height_px = y1 - y0
        width_px = x1 - x0
        if height_px <= 0 or width_px <= 0:
            return None
        tile_wcs = _slice_wcs_for_bbox(global_wcs_obj, x0=x0, y0=y0, width_px=width_px, height_px=height_px)
        tile_dir = _ensure_tile_workspace()
        if not tile_dir:
            return None
        tile_path = Path(tile_dir) / f"sds_tile_{tile_index:04d}.fits"
        tile_data = np.asarray(mosaic_arr[y0:y1, x0:x1], dtype=np.float32, copy=False)
        alpha_tile = None
        if alpha_arr is not None:
            try:
                alpha_tile = np.asarray(alpha_arr[y0:y1, x0:x1], dtype=np.uint8, copy=False)
            except Exception:
                alpha_tile = np.asarray(alpha_arr[y0:y1, x0:x1], dtype=np.uint8)
        header = None
        if tile_wcs is not None and hasattr(tile_wcs, "to_header"):
            try:
                header = tile_wcs.to_header(relax=True)
            except Exception:
                header = None
        try:
            if ZEMOSAIC_UTILS_AVAILABLE and hasattr(zemosaic_utils, "save_fits_image"):
                zemosaic_utils.save_fits_image(
                    image_data=np.ascontiguousarray(tile_data),
                    output_path=str(tile_path),
                    header=header,
                    overwrite=True,
                    save_as_float=True,
                    legacy_rgb_cube=False,
                    progress_callback=None,
                    axis_order="HWC",
                    alpha_mask=alpha_tile,
                )
            else:
                primary_data = np.moveaxis(tile_data, -1, 0) if tile_data.ndim == 3 else tile_data
                hdu = fits.PrimaryHDU(primary_data, header=header)
                hdus = [hdu]
                if alpha_tile is not None:
                    alpha_hdu = fits.ImageHDU(alpha_tile.astype(np.uint8, copy=False), name="ALPHA")
                    hdus.append(alpha_hdu)
                fits.HDUList(hdus).writeto(str(tile_path), overwrite=True)
        except Exception as exc:
            pcb("sds_warn_tile_save_failed", prog=None, lvl="WARN", error=str(exc), filename=str(tile_path))
            try:
                if tile_path.exists():
                    tile_path.unlink()
            except Exception:
                pass
            return None
        return str(tile_path), tile_wcs

    entry_infos: list[dict[str, Any]] = []
    wcs_keys = ("wcs", "phase0_wcs", "phase1_wcs", "header", "phase0_header")
    for group in seastar_groups or []:
        if not isinstance(group, (list, tuple)):
            continue
        for entry in group:
            if not isinstance(entry, dict) or not _entry_is_seestar(entry):
                continue
            local_wcs = None
            for key in wcs_keys:
                local_wcs = _coerce_wcs(entry.get(key))
                if local_wcs:
                    break
            if local_wcs is None:
                continue
            shape_hw = _extract_shape(entry)
            if shape_hw is None:
                continue
            h_local, w_local = shape_hw
            if h_local <= 0 or w_local <= 0:
                continue
            try:
                rows = np.array([0, 0, h_local - 1, h_local - 1], dtype=float)
                cols = np.array([0, w_local - 1, 0, w_local - 1], dtype=float)
                world_coords = local_wcs.pixel_to_world(cols, rows)
                g_cols, g_rows = global_wcs_obj.world_to_pixel(world_coords)
            except Exception:
                continue
            if g_cols is None or g_rows is None:
                continue
            try:
                x0 = int(np.floor(np.nanmin(g_cols)))
                x1 = int(np.ceil(np.nanmax(g_cols))) + 1
                y0 = int(np.floor(np.nanmin(g_rows)))
                y1 = int(np.ceil(np.nanmax(g_rows))) + 1
            except Exception:
                continue
            if any(np.isnan(val) for val in (x0, x1, y0, y1)):
                continue
            x0 = max(0, min(width, x0))
            x1 = max(x0, min(width, x1))
            y0 = max(0, min(height, y0))
            y1 = max(y0, min(height, y1))
            if (x1 - x0) <= 0 or (y1 - y0) <= 0:
                continue
            path_norm = _normcase_path(
                entry.get("path_preprocessed_cache") or entry.get("path_raw") or entry.get("path")
            )
            entry_infos.append({"entry": entry, "bbox": (y0, y1, x0, x1), "path_norm": path_norm})

    if not entry_infos:
        pcb("sds_error_no_valid_batches", prog=None, lvl="WARN", reason="no entries with valid WCS")
        return None, None, None

    try:
        coverage_threshold = float(coverage_threshold or 0.92)
    except Exception:
        coverage_threshold = 0.92
    coverage_threshold = max(0.10, min(0.99, coverage_threshold))
    try:
        min_batch_size = max(1, int(min_batch_size))
    except Exception:
        min_batch_size = 5
    try:
        target_batch_size = max(min_batch_size, int(target_batch_size))
    except Exception:
        target_batch_size = max(min_batch_size, 10)

    pcb(
        "sds_info_batch_policy",
        prog=None,
        lvl="INFO_DETAIL",
        coverage_threshold=coverage_threshold,
        min_batch_size=min_batch_size,
        target_batch_size=target_batch_size,
    )

    grid_h = max(1, min(512, height))
    grid_w = max(1, min(512, width))
    scale_y = grid_h / float(height)
    scale_x = grid_w / float(width)
    for info in entry_infos:
        y0, y1, x0, x1 = info["bbox"]
        g_y0 = max(0, min(grid_h, int(math.floor(y0 * scale_y))))
        g_y1 = max(g_y0 + 1, min(grid_h, int(math.ceil(y1 * scale_y))))
        g_x0 = max(0, min(grid_w, int(math.floor(x0 * scale_x))))
        g_x1 = max(g_x0 + 1, min(grid_w, int(math.ceil(x1 * scale_x))))
        info["grid_bbox"] = (g_y0, g_y1, g_x0, g_x1)
    if entry_infos:
        coverage_overview = np.zeros((grid_h, grid_w), dtype=np.uint8)
        for info in entry_infos:
            gy0, gy1, gx0, gx1 = info.get("grid_bbox", (0, 0, 0, 0))
            if gy1 <= gy0 or gx1 <= gx0:
                continue
            coverage_overview[gy0:gy1, gx0:gx1] = 1
        _emit_coverage_summary_log(
            pcb,
            coverage_array=coverage_overview,
            width=width,
            height=height,
            log_key="sds_debug_batch_coverage_summary",
            base_payload={"entries": len(entry_infos), "route": "sds_prebatch"},
            label="prebatch",
        )

    batches_infos: list[list[dict[str, Any]]] | None = None
    if preplan_path_groups:
        path_lookup = {info.get("path_norm"): info for info in entry_infos if info.get("path_norm")}
        mapped_batches: list[list[dict[str, Any]]] = []
        used_paths: set[str] = set()
        mapped_total_entries = 0
        for group_paths in preplan_path_groups:
            if not isinstance(group_paths, (list, tuple)):
                continue
            mapped_group: list[dict[str, Any]] = []
            for path_val in group_paths:
                norm_key = _normcase_path(path_val)
                if not norm_key:
                    continue
                if norm_key in used_paths:
                    continue
                info = path_lookup.get(norm_key)
                if info is None:
                    continue
                mapped_group.append(info)
                used_paths.add(norm_key)
            if mapped_group:
                mapped_batches.append(mapped_group)
        if mapped_batches:
            mapped_total_entries = sum(len(batch) for batch in mapped_batches)
        if mapped_batches and mapped_total_entries == len(entry_infos):
            batches_infos = mapped_batches
            pcb("sds_info_preplan_used", prog=None, lvl="INFO_DETAIL", count=len(batches_infos))
        elif preplan_path_groups:
            total_entries = len(entry_infos)
            if total_entries > 0:
                pcb(
                    "sds_warn_preplan_unused",
                    prog=None,
                    lvl="INFO_DETAIL",
                    matched=mapped_total_entries,
                    total=total_entries,
                )

    if batches_infos is None:
        batches_infos = _build_sds_batches_runtime(
            entry_infos,
            grid_h,
            grid_w,
            coverage_threshold=coverage_threshold,
            min_batch_size=min_batch_size,
            target_batch_size=target_batch_size,
        )

    if not batches_infos:
        pcb("sds_error_no_valid_batches", prog=None, lvl="WARN", reason="no valid batches after policy")
        return None, None, None

    batches: list[list[dict]] = []
    coverage_arrays: list[np.ndarray] = []
    batch_coverages: list[float] = []
    total_cells = grid_h * grid_w
    for info_batch in batches_infos:
        if not info_batch:
            continue
        coverage_grid = np.zeros((grid_h, grid_w), dtype=np.uint8)
        for info in info_batch:
            gy0, gy1, gx0, gx1 = info.get("grid_bbox", (0, 0, 0, 0))
            if gy1 <= gy0 or gx1 <= gx0:
                continue
            coverage_grid[gy0:gy1, gx0:gx1] = 1
        coverage_fraction = (
            float(np.count_nonzero(coverage_grid)) / float(total_cells) if total_cells else 1.0
        )
        entries = [info.get("entry") for info in info_batch if isinstance(info.get("entry"), dict)]
        if not entries:
            continue
        batches.append(entries)
        coverage_arrays.append(coverage_grid)
        batch_coverages.append(coverage_fraction)

    if not batches:
        pcb("sds_error_no_valid_batches", prog=None, lvl="WARN", reason="empty batches after filtering")
        return None, None, None

    sizes = [len(batch) for batch in batches]
    pcb(
        "sds_info_batches_built",
        prog=None,
        lvl="INFO_DETAIL",
        count=len(batches),
        total_entries=len(entry_infos),
    )
    sizes_text = "[" + ", ".join(str(size) for size in sizes) + "]"
    coverage_text = "[" + ", ".join(f"{c:.3f}" for c in batch_coverages) + "]"
    pcb(
        "sds_log_batch_summary",
        prog=None,
        lvl="INFO_DETAIL",
        batch_count=len(batches),
        sizes=sizes_text,
        coverages=coverage_text,
        coverage_threshold=coverage_threshold,
        min_batch_size=min_batch_size,
        target_batch_size=target_batch_size,
    )
    for idx, cov_array in enumerate(coverage_arrays):
        base_payload = {"entries": sizes[idx] if idx < len(sizes) else 0, "route": f"sds_batch_{idx + 1}"}
        _emit_coverage_summary_log(
            pcb,
            coverage_array=cov_array,
            width=width,
            height=height,
            log_key="sds_debug_batch_coverage_summary",
            base_payload=base_payload,
            label=f"batch_{idx + 1}",
        )

    def _safe_asarray(payload, *, dtype=np.float32):
        try:
            return np.asarray(payload, dtype=dtype, copy=False)
        except ValueError:
            return np.asarray(payload, dtype=dtype)

    mosaics: list[np.ndarray] = []
    coverages: list[np.ndarray | None] = []
    alphas: list[np.ndarray | None] = []
    pending_tile_payloads: list[tuple[np.ndarray, np.ndarray | None, np.ndarray | None, int]] = []
    batch_timings: list[float] = []
    total_batches = len(batches)
    weight_total = float(progress_weight_phase or 0.0)
    base_phase = float(base_progress_phase or 0.0)

    def _estimate_megatile_bytes() -> int:
        try:
            channels_hint = int((stack_params or {}).get("stack_output_channels") or 3)
        except Exception:
            channels_hint = 3
        channels_hint = max(1, channels_hint)
        return int(
            max(0, height)
            * max(0, width)
            * channels_hint
            * np.dtype(np.float32).itemsize
        )

    max_parallel_megatiles = 1
    if parallel_plan is not None:
        tiles_hint = getattr(parallel_plan, "tiles_per_chunk", None)
        if tiles_hint:
            try:
                max_parallel_megatiles = max(1, int(tiles_hint))
            except Exception:
                max_parallel_megatiles = 1
        else:
            try:
                cpu_workers_hint = int(getattr(parallel_plan, "cpu_workers", 0) or 0)
            except Exception:
                cpu_workers_hint = 0
            if cpu_workers_hint >= 4:
                max_parallel_megatiles = max(1, min(4, cpu_workers_hint // 2))
        if getattr(parallel_plan, "use_gpu", False):
            max_parallel_megatiles = 1
        chunk_bytes_hint = getattr(parallel_plan, "max_chunk_bytes", None)
        tile_bytes = _estimate_megatile_bytes()
        if chunk_bytes_hint and tile_bytes > 0:
            try:
                mem_based = max(1, int(chunk_bytes_hint) // tile_bytes)
            except Exception:
                mem_based = 1
            if mem_based > 0:
                max_parallel_megatiles = max(1, min(max_parallel_megatiles, mem_based))
    max_parallel_megatiles = max(1, min(total_batches or 1, max_parallel_megatiles))
    if max_parallel_megatiles <= 0:
        max_parallel_megatiles = 1

    try:
        pcb(
            "sds_parallel_batch_plan",
            prog=None,
            lvl="INFO_DETAIL",
            workers=int(max_parallel_megatiles),
            total_batches=int(total_batches),
            plan_cpu_workers=int(getattr(parallel_plan, "cpu_workers", 0) or 0)
            if parallel_plan is not None
            else 0,
            plan_use_gpu=bool(getattr(parallel_plan, "use_gpu", False)) if parallel_plan is not None else False,
        )
    except Exception:
        pass

    def _sds_build_reproject_kwargs(batch_index: int) -> dict[str, Any]:
        """Helper to keep per-batch reprojection kwargs consistent."""

        batch_base = base_phase
        if weight_total and total_batches:
            batch_base = base_phase + weight_total * (batch_index / max(1, total_batches))
        batch_weight = weight_total / max(1, total_batches) if weight_total else 0.0
        return {
            "global_plan": global_plan,
            "progress_callback": progress_callback,
            "match_background": match_background,
            "base_progress_phase": batch_base,
            "progress_weight_phase": batch_weight,
            "start_time_total_run": start_time_total_run,
            "cache_root": cache_root,
            "parallel_plan": parallel_plan,
        }

    def _process_sds_batch(idx: int, batch: list[dict]) -> dict[str, Any]:
        batch_start = time.monotonic()
        mosaic_arr, coverage_arr, alpha_arr = _assemble_global_mosaic_first_impl(
            [batch],
            **_sds_build_reproject_kwargs(idx),
        )
        elapsed = max(0.0, time.monotonic() - batch_start)
        tile_shape_text = (
            f"{int(mosaic_arr.shape[0])}x{int(mosaic_arr.shape[1])}"
            if mosaic_arr is not None and mosaic_arr.ndim >= 2
            else "n/a"
        )
        log_message = (
            f"[SDS] Batch {idx + 1}/{total_batches} | frames={len(batch)} "
            f"| shape={tile_shape_text} | elapsed={elapsed:.2f}s | success={mosaic_arr is not None}"
        )
        try:
            pcb(
                log_message,
                prog=None,
                lvl="INFO_DETAIL",
                batch_index=int(idx + 1),
                total_batches=int(total_batches),
                images=len(batch),
                tile_shape=tile_shape_text,
                elapsed_s=float(elapsed),
                success=bool(mosaic_arr is not None),
            )
        except Exception:
            pass
        result: dict[str, Any] = {
            "index": idx,
            "elapsed": elapsed,
            "mosaic": None,
            "coverage": None,
            "alpha_payload": None,
        }
        if mosaic_arr is None:
            return result
        mosaic_sanitized, coverage_sanitized, alpha_sanitized = _sanitize_sds_megatile_payload(
            mosaic_arr,
            coverage_arr,
            alpha_arr,
        )
        if mosaic_sanitized is None:
            return result
        if alpha_sanitized is not None:
            alpha_payload = alpha_sanitized
        elif isinstance(alpha_arr, np.ndarray):
            alpha_payload = _safe_asarray(alpha_arr)
        else:
            alpha_payload = alpha_arr.copy() if alpha_arr is not None else None
        result["mosaic"] = mosaic_sanitized
        result["coverage"] = coverage_sanitized
        result["alpha_payload"] = alpha_payload
        return result

    batch_results: list[dict[str, Any]] = []

    def _accumulate_batch_result(batch_result: dict[str, Any] | None) -> None:
        if not batch_result:
            return
        batch_timings.append(batch_result.get("elapsed", 0.0))
        batch_results.append(batch_result)

    if total_batches and max_parallel_megatiles > 1:
        with ThreadPoolExecutor(
            max_parallel_megatiles,
            thread_name_prefix="ZeMosaic_SDS_",
        ) as executor:
            futures = {
                executor.submit(_process_sds_batch, idx, batch): idx
                for idx, batch in enumerate(batches)
            }
            for future in as_completed(futures):
                try:
                    _accumulate_batch_result(future.result())
                except Exception as exc:
                    pcb("sds_error_batch_exception", prog=None, lvl="WARN", error=str(exc))
    else:
        for idx, batch in enumerate(batches):
            _accumulate_batch_result(_process_sds_batch(idx, batch))

    batch_results.sort(key=lambda payload: payload.get("index", 0))
    for result in batch_results:
        mosaic_payload = result.get("mosaic")
        if mosaic_payload is None:
            continue
        mosaics.append(mosaic_payload)
        coverages.append(result.get("coverage"))
        alphas.append(result.get("alpha_payload"))
        pending_tile_payloads.append(
            (
                mosaic_payload,
                result.get("alpha_payload"),
                result.get("coverage"),
                int(result.get("index", 0)) + 1,
            )
        )

    if not mosaics:
        pcb("sds_error_no_valid_batches", prog=None, lvl="WARN")
        return None, None, None

    mosaics = _normalize_sds_megatiles_photometry(
        mosaics,
        coverages,
        ref_index=0,
        pcb=pcb,
    )
    if len(pending_tile_payloads) == len(mosaics):
        pending_tile_payloads = [
            (
                mosaics[i],
                pending_tile_payloads[i][1],
                coverages[i] if i < len(coverages) else None,
                pending_tile_payloads[i][3],
            )
            for i in range(len(mosaics))
        ]

    for mosaic_arr, alpha_arr, cov_arr, tile_idx in pending_tile_payloads:
        saved_tile = _persist_sds_tile(mosaic_arr, alpha_arr, cov_arr, tile_idx)
        if saved_tile:
            tile_records.append(saved_tile)

    if postprocess_context is not None:
        tile_pairs: list[tuple[np.ndarray, Any, np.ndarray | None]] = []
        for idx, mosaic in enumerate(mosaics):
            if mosaic is None:
                continue
            try:
                wcs_clone = copy.deepcopy(global_wcs_obj)
            except Exception:
                wcs_clone = global_wcs_obj
            coverage_payload = None
            if idx < len(coverages):
                cov_arr = coverages[idx]
                coverage_payload = np.asarray(cov_arr, dtype=np.float32, copy=True) if cov_arr is not None else None
            tile_pairs.append((mosaic, wcs_clone, coverage_payload))
        postprocess_context["two_pass_tile_pairs"] = tile_pairs
        if tile_records:
            postprocess_context["sds_tile_records"] = list(tile_records)
        if tile_temp_dir:
            postprocess_context["sds_tile_temp_dir"] = tile_temp_dir

    def _coverage_weight(cov_arr: np.ndarray | None, batch_len: int) -> float:
        if cov_arr is None:
            return float(max(1, batch_len))
        try:
            arr = np.asarray(cov_arr, dtype=np.float32, copy=False)
            if arr.ndim != 2:
                arr = np.squeeze(arr)
            if arr.ndim != 2:
                return float(max(1, batch_len))
            max_cells = 1_000_000
            stride = max(1, int(math.sqrt(arr.size / max_cells))) if arr.size > max_cells else 1
            sampled = arr[::stride, ::stride] if stride > 1 else arr
            sum_val = float(np.nansum(sampled))
            if not np.isfinite(sum_val) or sum_val <= 0:
                coverage_pixels = float(np.count_nonzero(sampled > 0))
                return coverage_pixels if coverage_pixels > 0 else float(max(1, batch_len))
            return sum_val
        except Exception:
            return float(max(1, batch_len))

    weight_values: list[float] = []
    for idx, cov in enumerate(coverages):
        batch_len = len(batches[idx]) if idx < len(batches) else 1
        weight_values.append(_coverage_weight(cov, batch_len))
    manual_weights: np.ndarray | None
    if len(weight_values) == len(mosaics):
        manual_weights = np.asarray(weight_values, dtype=np.float32).reshape((len(weight_values),))
    else:
        manual_weights = None

    def _stack_mosaics() -> np.ndarray:
        if not (ZEMOSAIC_ALIGN_STACK_AVAILABLE and zemosaic_align_stack):
            stack_cube = np.stack(mosaics, axis=0).astype(np.float32, copy=False)
            if len(mosaics) == 1:
                return stack_cube[0]
            combine = str((stack_params or {}).get("stack_final_combine") or "mean").lower()
            if combine == "median":
                return np.nanmedian(stack_cube, axis=0).astype(np.float32)
            return np.nanmean(stack_cube, axis=0).astype(np.float32)

        algo = str((stack_params or {}).get("stack_reject_algo") or "winsorized_sigma_clip").lower()
        weight_method = (stack_params or {}).get("stack_weight_method", "none")
        kappa_low = float((stack_params or {}).get("stack_kappa_low", 3.0))
        kappa_high = float((stack_params or {}).get("stack_kappa_high", 3.0))
        winsor_limits = (stack_params or {}).get("parsed_winsor_limits", (0.05, 0.05))
        winsor_workers = int((stack_params or {}).get("winsor_worker_limit", 1))
        winsor_frames = int((stack_params or {}).get("winsor_max_frames_per_pass", 0))

        if algo in {"winsorized_sigma_clip", "winsorized", "winsor"} and hasattr(
            zemosaic_align_stack, "stack_winsorized_sigma_clip"
        ):
            stacked, _ = zemosaic_align_stack.stack_winsorized_sigma_clip(
                mosaics,
                weights=manual_weights,
                weight_method=weight_method,
                zconfig=None,
                kappa=kappa_low,
                winsor_limits=winsor_limits,
                apply_rewinsor=True,
                winsor_max_frames_per_pass=winsor_frames,
                winsor_max_workers=winsor_workers,
                parallel_plan=parallel_plan,
            )
            return np.asarray(stacked, dtype=np.float32, copy=False)

        if algo == "kappa_sigma" and hasattr(zemosaic_align_stack, "stack_kappa_sigma_clip"):
            stacked, _ = zemosaic_align_stack.stack_kappa_sigma_clip(
                mosaics,
                weights=manual_weights,
                weight_method=weight_method,
                zconfig=None,
                sigma_low=kappa_low,
                sigma_high=kappa_high,
                parallel_plan=parallel_plan,
            )
            return np.asarray(stacked, dtype=np.float32, copy=False)

        stack_cube = np.stack(mosaics, axis=0).astype(np.float32, copy=False)
        if len(mosaics) == 1:
            return stack_cube[0]
        combine = str((stack_params or {}).get("stack_final_combine") or "mean").lower()
        if combine == "median":
            return np.nanmedian(stack_cube, axis=0).astype(np.float32)
        return np.nanmean(stack_cube, axis=0).astype(np.float32)

    try:
        final_image = _stack_mosaics()
    except Exception as exc:
        pcb("sds_error_final_stack_failed", prog=None, lvl="ERROR", error=str(exc))
        return None, None, None

    final_image = np.asarray(final_image, dtype=np.float32, copy=False)

    final_coverage = None
    for cov in coverages:
        if cov is None:
            continue
        arr = _safe_asarray(cov)
        if final_coverage is None:
            final_coverage = np.zeros_like(arr, dtype=np.float32)
        final_coverage += arr
    if final_coverage is None:
        final_coverage = np.ones((height, width), dtype=np.float32)

    _emit_coverage_summary_log(
        pcb,
        coverage_array=final_coverage,
        width=width,
        height=height,
        log_key="global_coadd_coverage_summary",
        base_payload={"route": "sds_final"},
        label="sds_final",
    )

    alpha_candidates = [_safe_asarray(alpha) for alpha in alphas if alpha is not None]
    final_alpha = None
    if alpha_candidates:
        alpha_stack = np.stack(alpha_candidates, axis=0)
        final_alpha = np.nanmax(alpha_stack, axis=0)
        final_alpha = np.clip(final_alpha, 0, 255).astype(np.uint8, copy=False)
    else:
        max_cov = float(np.nanmax(final_coverage)) if final_coverage is not None else 0.0
        if max_cov > 0:
            normalized = np.clip((final_coverage / max_cov) * 255.0, 0, 255)
            final_alpha = normalized.astype(np.uint8, copy=False)

    total_elapsed = max(0.0, time.monotonic() - assembly_start)
    shape_acc_h = 0.0
    shape_acc_w = 0.0
    valid_shape_count = 0
    for tile in mosaics:
        if tile is None or tile.ndim < 2:
            continue
        shape_acc_h += float(tile.shape[0])
        shape_acc_w += float(tile.shape[1])
        valid_shape_count += 1
    avg_height = (shape_acc_h / valid_shape_count) if valid_shape_count else 0.0
    avg_width = (shape_acc_w / valid_shape_count) if valid_shape_count else 0.0
    avg_batch_time = (sum(batch_timings) / len(batch_timings)) if batch_timings else 0.0
    summary_message = (
        "[SDS] Global summary → mega_tiles={tiles}, avg_shape≈{avg_h:.1f}x{avg_w:.1f}, "
        "elapsed={elapsed:.2f}s, avg_batch={avg_batch:.2f}s"
    ).format(
        tiles=len(mosaics),
        avg_h=avg_height,
        avg_w=avg_width,
        elapsed=total_elapsed,
        avg_batch=avg_batch_time,
    )
    try:
        pcb(
            summary_message,
            prog=None,
            lvl="INFO_DETAIL",
            mega_tiles=int(len(mosaics)),
            avg_height=float(avg_height),
            avg_width=float(avg_width),
            elapsed_s=float(total_elapsed),
            avg_batch_s=float(avg_batch_time),
        )
    except Exception:
        pass

    return final_image, final_coverage, final_alpha


def assemble_global_mosaic_first(
    raw_groups: list[list[dict]],
    *,
    global_plan: dict[str, Any],
    progress_callback: callable,
    match_background: bool,
    base_progress_phase: float | None,
    progress_weight_phase: float | None,
    start_time_total_run: float | None,
    cache_root: str | None = None,
    parallel_plan: ParallelPlan | None = None,
) -> tuple[np.ndarray | None, np.ndarray | None, np.ndarray | None]:
    """Wrapper preserving legacy signature while using the optimized implementation."""

    return _assemble_global_mosaic_first_impl(
        raw_groups,
        global_plan=global_plan,
        progress_callback=progress_callback,
        match_background=match_background,
        base_progress_phase=base_progress_phase,
        progress_weight_phase=progress_weight_phase,
        start_time_total_run=start_time_total_run,
        cache_root=cache_root,
        parallel_plan=parallel_plan,
    )

def _fallback_app_base_dir() -> Path:
    try:
        return Path(__file__).resolve().parent
    except Exception:
        return Path.cwd()


def _fallback_user_config_dir() -> Path:
    root = Path.home() / "ZeMosaic"
    try:
        root.mkdir(parents=True, exist_ok=True)
    except Exception:
        pass
    return root


if ZEMOSAIC_UTILS_AVAILABLE:
    get_app_base_dir = getattr(zemosaic_utils, "get_app_base_dir", _fallback_app_base_dir)
    ensure_user_config_dir = getattr(zemosaic_utils, "ensure_user_config_dir", _fallback_user_config_dir)
    get_runtime_temp_dir = getattr(zemosaic_utils, "get_runtime_temp_dir", _fallback_runtime_temp_dir)
else:
    get_app_base_dir = _fallback_app_base_dir
    ensure_user_config_dir = _fallback_user_config_dir
    get_runtime_temp_dir = _fallback_runtime_temp_dir
