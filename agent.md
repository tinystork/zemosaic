# Agent ‚Äî Global Parallelization & CPU/GPU Utilization (Codex Max HIGH)

## 0. Mode d'intervention (IMPORTANT)

Tu tournes ici en **mode HIGH**, mais le projet est d√©j√† tr√®s avanc√© et proche ‚Äúproduction‚Äù.

üëâ Ton r√¥le n‚Äôest **pas** de r√©inventer le pipeline, mais de :
- **augmenter l‚Äôefficacit√© CPU/GPU**,
- **sans changer la logique fonctionnelle**,
- **sans changer les branchements du pipeline**,
- **sans r√©introduire de limite fixe comme le cap √† 50 images** qui vient d‚Äô√™tre lev√©.

Tu peux :
- modifier les **heuristiques de parall√©lisation** (nombre de workers, tailles de chunks, seuils m√©moire),
- factoriser *l√©g√®rement* du code si n√©cessaire pour √©viter les duplications √©videntes,
- ajouter de la **t√©l√©m√©trie/logging** pour suivre CPU/GPU/m√©moire.

Tu ne dois pas :
- changer le **workflow SDS vs non-SDS**,
- modifier les **phases** (1 ‚Üí 6) ni l‚Äôordre des √©tapes,
- introduire de nouvelles options GUI,
- changer le sens des options existantes,
- remettre des **caps arbitraires** (comme ‚Äúmax 50 frames par tuile‚Äù).

La r√®gle d‚Äôor :  
> **M√™me input ‚Üí m√™me pipeline conceptuel ‚Üí m√™mes outputs** (√† de tr√®s petites diff√©rences num√©riques pr√®s dues √† l‚Äôordre de r√©duction/float).

---

## 1. Mission

**Objectif :**  
Maximiser l‚Äôutilisation des ressources **CPU + GPU** dans toutes les grandes phases num√©riques (stacking, Mosaic-First, SDS, Phase 5), en exploitant au mieux :

- le **parallel plan** existant (ou √† consolider) : nombre de workers, chunking, memmap,
- le **GPU helper** existant pour `reproject_and_coadd` / global coadd,
- les m√©canismes de **streaming/chunking** d√©j√† pr√©sents (alt-az cleanup, Mosaic-First, SDS, Phase 4.5, etc.).

**Sans modifier le comportement fonctionnel**, uniquement les **param√®tres de parall√©lisation** et les **heuristiques d‚Äôautotune**.

---

## 2. Contexte (code & fichiers)

Le projet comporte d√©j√† :

- Un **pipeline hi√©rarchique** avec phases :
  - P1‚ÄìP2 : pr√©-tri et regroupement (‚ÄúSeestar stacks‚Äù),
  - P3 : Master Tiles,
  - P4 : calcul de la grille finale (WCS global),
  - P4.5 : √©ventuel traitement interm√©diaire / super-tuiles,
  - P5 : assemblage final (Incremental / Reproject & Coadd),
  - SDS : mode sp√©cial ‚Äúsuper-stack par lots‚Äù (m√©ga-tuiles) qui **NE DOIT PAS √™tre modifi√© logiquement**.
- Une logique de **Mosaic-First / Global coadd** (helper GPU ou CPU fallback).
- Une logique de **parallel plan** (ou √©quivalent) qui choisit :
  - `cpu_workers`,
  - `rows_per_chunk`, `tiles_per_chunk`,
  - `use_memmap`, `max_chunk_bytes`,
  - `use_gpu` / `gpu_rows_per_chunk`.

Tu dois **t‚Äôappuyer sur cette structure** et ne pas la remplacer.

---

## 3. P√©rim√®tre d‚Äôoptimisation

Tu es autoris√© √† optimiser la parall√©lisation dans les zones suivantes :

1. **Stacking / Master Tiles (Phase 3)**  
   - Alignement intra-stack,
   - empilement des stacks,
   - √©ventuelle utilisation du GPU (si d√©j√† pr√©sent dans ce code),
   - multi-process / multi-thread sur les stacks.

2. **Mosaic-First / Global coadd (Phase 4)**
   - Chemin ‚Äúglobal coadd‚Äù (Mosaic-First) qui assemble les brutes directement sur la grille globale.
   - Utiliser le parallel plan pour :
     - mieux dimensionner le nombre de workers CPU,
     - optimiser `rows_per_chunk` / `max_chunk_bytes`,
     - exploiter le GPU helper plus efficacement.

3. **Phase 4.5 / Super-tuiles / micro-align / photom√©trie**
   - Les boucles qui :
     - reprojectent des tuiles par groupe,
     - appliquent des corrections photom√©triques,
     - font des coadds locaux.

4. **Phase 5 (assemblage final)**
   - Chemin **Reproject & Coadd** (classique, non SDS).
   - Chemin **Incremental** sur disque (si encore utilis√©).
   - Chemin **SDS** (global stack √† partir de m√©ga-tuiles).

5. **SDS ON / SDS OFF**
   - SDS **OFF** : pipeline classique (Master Tiles ‚Üí P4 grid ‚Üí P5 assemble) doit rester inchang√© logiquement.
   - SDS **ON** : pipeline SDS (m√©ga-tuiles + super-stack global) doit rester inchang√© logiquement, mais tu peux mieux r√©partir le travail entre CPU et GPU.

---

## 4. Ce que tu peux/d√©dois faire concr√®tement

### 4.1 Ajuster les heuristiques de parallel plan

- Centraliser les d√©cisions de parall√©lisation dans un **module d√©di√©** (par ex. `parallel_utils.py` / √©quivalent existant) qui :

  - d√©tecte les capacit√©s :
    - nombre de c≈ìurs logiques,
    - RAM totale / disponible,
    - GPU dispo (CUDA) + VRAM totale / libre,
  - calcule pour chaque ‚Äúkind‚Äù (ex. `"master_tiles"`, `"mosaic_first"`, `"phase5_global"`, `"sds_megatiles"`) un plan :
    - `cpu_workers` (plafonn√© par un facteur ex. 0.75‚Äì0.9 de cores),
    - `use_memmap` / `max_chunk_bytes`,
    - `rows_per_chunk` / `tiles_per_chunk`,
    - `use_gpu` + `gpu_rows_per_chunk`.

- **Tu peux modifier les heuristiques** pour viser :
  - **CPU** √† ~70‚Äì90 % sur les phases lourdes,
  - **GPU** √† ~50‚Äì90 % pendant les reprojects lourds,
  - tout en gardant une marge m√©moire (par ex. 20‚Äì30 % de RAM/VRAM libre).

### 4.2 Utilisation CPU

- L√† o√π le code a d√©j√† un `ThreadPoolExecutor` / `ProcessPoolExecutor` ou param√®tre `process_workers` :
  - Remplacer les constantes / configurations ‚Äú√† la main‚Äù par les valeurs du parallel plan.
- Si un code CPU est clairement **s√©quentiel** alors qu‚Äôil it√®re sur :
  - des tiles ind√©pendantes,
  - des m√©ga-tuiles ind√©pendantes,
  - des stacks ind√©pendants,

  tu peux introduire une **parall√©lisation simple** **sans changer la logique** :
  - encapsuler l‚Äôunit√© de travail dans une fonction pure,
  - mapper cette fonction sur un pool (taille dict√©e par `parallel_plan.cpu_workers`),
  - assembler les r√©sultats exactement comme avant.

### 4.3 Utilisation GPU

- Tu dois utiliser le GPU uniquement l√† o√π des hooks existent d√©j√† (par ex. `reproject_and_coadd_wrapper(..., use_gpu=True, ...)` ou √©quivalent).
- Tu peux modifier :
  - `rows_per_chunk`, `max_chunk_bytes` pass√©s au helper GPU,
  - les conditions d‚Äôactivation `use_gpu` en fonction du plan (GPU dispo + VRAM suffisante).

Tu ne dois pas :
- √©crire de nouveaux kernels custom,
- modifier les algos de coadd / kappa-sigma / Winsor,
- changer le comportement des modes SDS / Mosaic-First.

### 4.4 M√©moire & robustesse

- L‚Äôautotune doit respecter **strictement** :
  - ne jamais allouer plus que, mettons, 70‚Äì80 % de la RAM disponible,
  - ne jamais tenter de consommer plus que 50‚Äì65 % de la VRAM disponible pour un job donn√©.
- En cas de `MemoryError` / erreur CUDA :
  - **r√©duire** les chunks ou le nombre de workers,
  - **basculer** en CPU si GPU indisponible,
  - mais ne pas interrompre toute la mosa√Øque si une fallback propre est possible.

### 4.5 Cap 50 images

- Il y avait historiquement un cap √† 50 images par tuile / groupe ‚Üí **il vient d‚Äô√™tre lev√©**.
- Tu **ne dois pas** :
  - remettre de limite fixe type 50/100/200 frames ailleurs,
  - tronquer les listes d‚Äôimages.
- Tes heuristiques doivent √™tre **scalables** :
  - pour 10 images comme pour 10 000+ images,
  - en adaptant les workers / chunks √† la m√©moire disponible.

---

## 5. Non-r√©gressions obligatoires

1. **SDS vs non-SDS**
   - SDS OFF ‚Üí pipeline classique comme aujourd‚Äôhui (Master tiles ‚Üí P4 ‚Üí P5)  
     (aucune nouvelle branche conditionnelle ne doit changer ce chemin).
   - SDS ON ‚Üí pipeline SDS existant (m√©ga-tuiles + super-stack global)  
     (ne pas d√©tourner ce mode vers d‚Äôautres fonctions).

2. **R√©sultats**
   - Pas de changement volontaire du r√©sultat scientifique :
     - m√™me syst√®me de coadd (mean, median, kappa-sigma, winsor),
     - m√™mes normalisations photom√©triques globales,
     - m√™me logique de coverage / alpha / cropping.
   - Des diff√©rences **minimes** de flottant dues √† l‚Äôordre de r√©duction sont acceptables, mais tu ne dois pas changer les formules.

3. **Pas de nouvelle logique GUI**
   - Tu ne touches pas √† l‚Äôaspect fonctionnel des GUI Tk / Qt.  
   - Tu peux seulement :
     - accepter de nouveaux champs de config ‚Äúsourds‚Äù (sans contr√¥le GUI),
     - am√©liorer le logging pour les messages de perf (profil parallel_plan, etc.).

4. **Compatibilit√© multi-OS**
   - Le pipeline **CPU** doit rester pleinement fonctionnel sur Windows / Linux / macOS.
   - Le GPU helper n‚Äôest activ√© que si CUDA+CuPy sont disponibles, sinon fallback CPU.

---

## 6. Crit√®res de succ√®s

- Sur une grosse mosa√Øque :
  - utilisation CPU nettement plus √©lev√©e sur les phases lourdes (P3, P4.5, P5),
  - utilisation GPU significative pendant les reprojects globaux (Mosaic-First, Phase 5, SDS).
- Pas d‚Äôaugmentation notable du taux d‚Äôerreurs m√©moire ou CUDA.
- Pas de changement de comportement SDS ON/OFF, Mosaic-First ON/OFF.
- Les utilisateurs retrouvent leurs habitudes de workflow, mais les traitements sont **sensiblement plus rapides** sur des machines multi-c≈ìurs / GPU.
