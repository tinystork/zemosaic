# Mission : Optimiser et parall√©liser la Second Pass (Phase 5) du pipeline ZeMosaic

## üéØ Objectif global

La Phase 5 comporte deux sous-√©tapes :

1. **Reproject & Coadd classique**  
2. **Second Pass Coverage Renormalization (Two-Pass)**

Le premier bloc a d√©j√† √©t√© stabilis√©.  
La seconde passe, elle, reste **largement s√©quentielle**, tr√®s lente, et n‚Äô√©met presque aucune t√©l√©m√©trie.

üëâ **Ta mission est d‚Äôoptimiser fortement la Second Pass**, en :

- parall√©lisant les op√©rations **au niveau ZeMosaic** (pas reproject lui-m√™me),
- utilisant **cpu_workers** et **chunking ParallelPlan**,
- utilisant le **GPU** quand `use_gpu_phase5 = True`,
- garantissant que **SDS reste strictement intouch√©**.

---

# üß± Contexte technique

La seconde passe est pilot√©e depuis :

### Fichier :
- `zemosaic_worker.py`

### Fonctions cl√©s :
- `run_second_pass_coverage_renorm(...)`
- `compute_per_tile_gains_from_coverage(...)`
- projection du coverage vers chaque tuile
- boucle `for ch in range(n_channels)` pour reprojection par canal
- (toute logique entre `[TwoPass] Second pass requested...` et `[TwoPass] coverage-renorm OK`)

### Probl√®me actuel :

1. **Boucle par tuile** ‚Üí S√©quentielle  
2. **Boucle par canal** ‚Üí S√©quentielle  
3. **Reprojection** ex√©cut√©e dans 1 appel global, sans chunking ZeMosaic  
4. **cpu_workers affich√©s mais non utilis√©s**  
5. **gpu=True logg√© mais la logique reste CPU-bound majoritairement**  
6. **T√©l√©m√©trie Phase 5 minimaliste**  
7. **rows(cpu/gpu)=0/0** √† cause d‚Äôabsence de d√©coupage pour la TwoPass.

---

# ‚úîÔ∏è Ce que tu dois faire

## 1. Parall√©liser compute_per_tile_gains_from_coverage

Dans `compute_per_tile_gains_from_coverage(...)` :

- Chaque tuile est aujourd‚Äôhui trait√©e dans une boucle Python s√©quentielle.
- Tu dois utiliser **ThreadPoolExecutor** ou **ProcessPoolExecutor** suivant ParallelPlan :

  - **Si GPU actif (`use_gpu=True`)** ‚Üí utiliser un **ThreadPoolExecutor**  
    (les op√©rations CuPy lib√®rent le GIL ‚Üí b√©n√©fice imm√©diat).
  
  - **Si GPU inactif** ‚Üí utiliser un **ProcessPoolExecutor**  
    (les op√©rations NumPy/Scipy/CV2 sont CPU-bound).

### D√©tails :

Pour chaque tuile :

- projection coverage ‚Üí WCS tuile
- calcul m√©dian ‚Üí gain
- clamp dans [gain_clip_min, gain_clip_max]

Le parall√©lisme doit :

- respecter `plan.cpu_workers`
- respecter les limites m√©moire (`max_chunk_bytes`) en batchant intelligemment la liste des tuiles
- renvoyer les gains dans l‚Äôordre d‚Äôorigine

‚ö†Ô∏è Interdiction de changer la logique math√©matique.  
Simplement parall√©liser.

---

## 2. Parall√©liser la reprojection per-channel

Aujourd‚Äôhui :

```python
for ch in range(n_channels):
    ...
    chan_mosaic, chan_cov = _invoke_reproj(...)
‚û°Ô∏è Cette boucle doit √™tre parall√©lis√©e :

Strat√©gie :
lancer 1 worker par canal quand n_channels >= 2

sinon 1 seul worker √©videmment

respecter plan.cpu_workers (ne pas d√©passer)

si GPU actif :

autoriser un seul canal √† utiliser le GPU √† la fois
(use_gpu = True uniquement pour 1 task)

les autres canaux ‚Üí CPU
(sinon VRAM satur√©e)

si GPU inactif :

parall√©liser tous les canaux en CPU

Contraintes :
Les r√©sultats doivent √™tre recombin√©s dans l‚Äôordre original [H, W, C].

_invoke_reproj ne doit pas √™tre modifi√©.

Si l‚Äôutilisateur a un GPU 8/12/16 Go ‚Üí parall√®le CPU+GPU hybride automatique.

3. Ajouter un vrai chunking pour la TwoPass (rows_per_chunk)
Actuellement, pour TwoPass :

bash
Copier le code
rows(cpu/gpu) = 0/0
chunk_mb(cpu/gpu) = 1144MB
‚Üí aucune d√©coupe.

Tu dois :

r√©utiliser le ParallelPlan appliqu√© en Phase 5
(celui obtenu juste avant pour Reproject & Coadd),

d√©couper la coverage + la grille finale en blocs de lignes (row-chunks),

ex√©cuter les op√©rations lourdes (gaussian blur, reprojection coverage‚Üítile, gains apply) par chunk.

Les chunk doivent √™tre d√©finis par :

plan.rows_per_chunk (si disponible)

ou plan.max_chunk_bytes / plan.gpu_max_chunk_bytes (fallback)

ou au pire un d√©coupage fixe 512‚Äì1024 lignes par chunk si aucun plan n‚Äôest disponible

‚ö†Ô∏è Encore une fois : pas de changement math√©matique.

4. Ajouter t√©l√©m√©trie Phase 5 compl√®te
Aujourd‚Äôhui, aucun STATS_UPDATE n‚Äôest √©mis pendant la seconde passe.

Tu dois :

envoyer un STATS_UPDATE au d√©but,

un STATS_UPDATE toutes les X tuiles OU tous les X chunks,

un STATS_UPDATE √† la fin.

Le stats_dict doit contenir (m√™mes cl√©s que Phase 3) :

makefile
Copier le code
phase_index=5
phase_name="Phase 5: Two-Pass Coverage Renorm"
cpu_percent
ram_used_mb
gpu_used_mb
cpu_workers=plan.cpu_workers
use_gpu=plan.use_gpu
use_gpu_phase5=true/false
tiles_done=X
tiles_total=Y
chunk_index
chunk_total
Tu peux r√©utiliser _log_and_callback("STATS_UPDATE", ...).

üîí Ce que tu NE DOIS PAS toucher
AUCUN fichier/fonction SDS

AUCUNE logique math√©matique (gaussian blur, gains, clamp)

AUCUN comportement Phase 1/3

AUCUN param√®tre de configuration existant

AUCUN test

AUCUNE signature publique du pipeline

üìÇ Fichiers √† modifier
Exclusivement :

zemosaic_worker.py

zemosaic_utils.py (si n√©cessaire pour ajouter un petit helper de parall√©lisation non-intrusif)

√©ventuellement parallel_utils.py pour exposer un petit helper parallel_map() r√©utilisable (non obligatoire)

‚úîÔ∏è R√©sultat attendu
Apr√®s impl√©mentation :

La seconde passe doit diviser son temps de traitement par 2√ó √† 8√ó selon CPU/GPU.

Le moniteur de ressources doit montrer :

CPU multi-workers actifs

GPU actif si use_gpu_phase5=True

Le log ne doit plus montrer rows(cpu/gpu)=0/0

La t√©l√©m√©trie Phase 5 doit appara√Ætre clairement dans resource_telemetry.csv

Le pipeline SDS reste strictement identique.