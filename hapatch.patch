diff --git a/agent.md b/agent.md
index 71e5520a2e9e3f122d8711397583ed4d84c3eb4d..79cfb7a83ff25c49eb265f511a16a857b17bb9a6 100644
--- a/agent.md
+++ b/agent.md
@@ -1,40 +1,40 @@
 # Mission — “Winsor worker limit: 0 = auto” (patch minimal, user-friendly)
 
 Objectif:
 - Rendre le paramètre `winsor_worker_limit` cohérent avec les autres champs UI:
   - `winsor_worker_limit = 0` => mode AUTO
   - `winsor_worker_limit > 0` => limite manuelle
 - Patch MINIMAL: pas de nouvelles options/config, pas de refactor.
 
 ## Constat (code actuel)
 Dans `zemosaic_worker.py`, plusieurs endroits font:
 - winsor_worker_limit = max(1, min(int(cfg), cpu_total))
 => 0 devient 1 (donc pas d’auto).
 Or `zemosaic_align_stack.py` supporte déjà implicitement:
 - winsor_max_workers = current_workers or plan_cpu_workers
 => si on passe 0, l’auto fonctionne.
 
 ## Fichiers à modifier (minimum)
 - /mnt/data/zemosaic_worker.py (obligatoire)
 
 Optionnel (ne pas faire sauf si nécessaire):
 - /mnt/data/zemosaic_align_stack.py (a priori inutile)
 
 ## Règles de comportement attendues
-1) Phase 4.5 (stacking local via `stack_kwargs["winsor_max_workers"]`):
+- [x] Phase 4.5 (stacking local via `stack_kwargs["winsor_max_workers"]`):
    - Si cfg == 0: passer 0 tel quel (AUTO géré par zemosaic_align_stack via parallel_plan)
    - Sinon: passer max(1, cfg)
 
-2) Global/SDS stacking params (là où on stocke `winsor_worker_limit` dans des dicts type global_wcs_plan / sds_stack_params):
+- [x] Global/SDS stacking params (là où on stocke `winsor_worker_limit` dans des dicts type global_wcs_plan / sds_stack_params):
    - Si cfg <= 0: calculer une valeur effective >= 1 (AUTO) et stocker cette valeur.
    - Sinon: clamp normal (1..cpu_total).
    Raisons: certains endroits utilisent cette valeur comme un cap numérique direct, et 0 casserait la logique.
 
 ## Définition de “AUTO” (simple et robuste)
 AUTO doit privilégier les workers déjà auto-tunés:
 - si `global_parallel_plan` existe et `global_parallel_plan.cpu_workers > 0` => utiliser ça
 - sinon fallback sur `effective_base_workers` (déjà calculé par le worker)
 - sinon fallback sur `cpu_total`
 puis clamp [1..cpu_total].
 
 ## Logging (minimal mais uti
diff --git a/followup.md b/followup.md
index c1fa3deb827b03ab853a4749be8fddfced1ad659..e8aca3e51a32dd4ee9b35898aabe68aa86e0d459 100644
--- a/followup.md
+++ b/followup.md
@@ -60,74 +60,45 @@ AUTO doit privilégier les workers déjà auto-tunés:
 - sinon fallback sur `effective_base_workers` (déjà calculé par le worker)
 - sinon fallback sur `cpu_total`
 puis clamp [1..cpu_total].
 
 ## Logging (minimal mais utile)
 - Quand cfg <= 0, loguer une ligne INFO_DETAIL du style:
   "Winsor worker limit: AUTO (cfg=0) -> resolved=<N> (cpu_total=<M>)"
 - En Phase 4.5, si on laisse passer 0, éviter de loguer "workers=0" de manière trompeuse:
   loguer "workers=AUTO(0)" ou similaire.
 
 ## Contraintes
 - Ne pas changer les algorithmes scientifiques.
 - Ne pas toucher à la GUI.
 - Ne pas introduire de nouvelle variable de config.
 - Patch localisé et lisible.
 ```
 
 ---
 
 ## followup.md
 
 ```markdown
 # Plan d’exécution (Codex)
 
 ## Étape 1 — Identifier les 3 endroits clés dans zemosaic_worker.py
-1) Phase 4.5 stack_kwargs:
-   Actuel: "winsor_max_workers": max(1, worker_limit_val)
-   -> À changer pour laisser passer 0:
-      if worker_limit_val <= 0: winsor_max_workers = 0
-      else: winsor_max_workers = max(1, worker_limit_val)
-
-2) Deux blocs “winsor_worker_limit = max(1, min(int(winsor_worker_limit_config), cpu_total))”
-   (il y en a 2 occurrences dans le fichier)
-   -> Remplacer par une résolution AUTO effective:
-      - cpu_total = os.cpu_count() or 1
-      - cfg = int(winsor_worker_limit_config) (guard try/except)
-      - if cfg <= 0:
-          candidate = global_parallel_plan.cpu_workers si dispo (>0)
-          sinon candidate = effective_base_workers si dispo (>0)
-          sinon candidate = cpu_total
-        else:
-          candidate = cfg
-      - winsor_worker_limit = max(1, min(int(candidate), cpu_total))
-
-   Important: ici on STOCKE une valeur >=1 dans global_wcs_plan / sds_stack_params.
+- [x] Phase 4.5 stack_kwargs: laisser passer 0 → winsor_max_workers = 0, sinon clamp à >=1.
+- [x] Deux blocs “winsor_worker_limit = max(1, min(int(winsor_worker_limit_config), cpu_total))” remplacés par une résolution AUTO effective (global_parallel_plan > effective_base_workers > cpu_total) et stockage clampé >=1.
 
 ## Étape 2 — Logging minimal
-- Au moment où winsor_worker_limit est fixé pour global_wcs_plan:
-  - Si cfg <= 0, loguer "AUTO cfg=0 -> resolved=N"
-  - Sinon conserver le message existant (ou ajouter "(manual)").
-
-- Pour Phase 4.5:
-  - Si worker_limit_val <= 0, loguer workers="AUTO(0)" au lieu de 0.
+- [x] cfg <= 0 dans global_wcs_plan: log INFO_DETAIL "AUTO cfg=0 -> resolved=N".
+- [x] Phase 4.5: workers loggué en "AUTO(0)" quand on laisse passer 0.
 
 ## Étape 3 — Vérifs rapides
-1) py_compile:
-   python -m py_compile zemosaic_worker.py
-
-2) Test fonctionnel simple:
-   - Mettre winsor_worker_limit=0 dans la config/GUI
-   - Vérifier dans les logs:
-     - une ligne "AUTO ... resolved=<N>"
-     - Phase 4.5 n’affiche pas un workers=0 ambigu
-   - Vérifier que le run n’active pas une explosion de workers: N doit rester borné par cpu_total et idéalement proche du parallel plan.
+- [x] py_compile: `python -m py_compile zemosaic_worker.py`
+- [ ] Test fonctionnel simple (config GUI winsor_worker_limit=0, vérifier logs et bornage workers)
 
 ## Notes importantes
 - Ne PAS propager 0 dans global_wcs_plan / sds_stack_params (risque de cap=0 ailleurs).
 - Par contre, laisser passer 0 dans stack_kwargs (Phase 4.5) est OK car `zemosaic_align_stack.py` sait résoudre 0 via parallel_plan.
 - Ne pas modifier zemosaic_align_stack.py (il est déjà compatible).
 ```
 
 ---
 
 Si tu veux, je peux aussi te donner un **mini diff “à la main”** (3 micro-changements) pour vérifier que Codex ne part pas en sucette — mais tel quel, ce brief est normalement assez cadré pour un patch chirurgical.
diff --git a/zemosaic_worker.py b/zemosaic_worker.py
index 90199e19fc27e7ac164816bf19c40d0434fbd184..6cbf4c3310e814bf1a70e8a33a96504cb14cac61 100644
--- a/zemosaic_worker.py
+++ b/zemosaic_worker.py
@@ -4613,55 +4613,56 @@ def _run_phase4_5_inter_master_merge(
 
             chunk_affine_corrections: list[tuple[float, float] | None] = [None] * len(chunk_tiles)
             chunk_photometry_done = False
 
             try:
                 kappa_val = float(stack_cfg.get("kappa_low", 3.0))
             except Exception:
                 kappa_val = 3.0
             limits_val = stack_cfg.get("winsor_limits", (0.05, 0.05))
             try:
                 limits_val = (
                     float(limits_val[0]),
                     float(limits_val[1]),
                 )
             except Exception:
                 limits_val = (0.05, 0.05)
             try:
                 max_pass_val = int(stack_cfg.get("winsor_max_frames_per_pass", 0))
             except Exception:
                 max_pass_val = 0
             try:
                 worker_limit_val = int(stack_cfg.get("winsor_worker_limit", 1))
             except Exception:
                 worker_limit_val = 1
             current_parallel_plan = getattr(zconfig, "parallel_plan", worker_config_cache.get("parallel_plan"))
+            winsor_max_workers_val = 0 if worker_limit_val <= 0 else max(1, worker_limit_val)
             stack_kwargs = {
                 "kappa": kappa_val,
                 "winsor_limits": limits_val,
                 "winsor_max_frames_per_pass": max_pass_val,
-                "winsor_max_workers": max(1, worker_limit_val),
+                "winsor_max_workers": winsor_max_workers_val,
             }
             super_arr = None
             # --- Phase 4.5 (améliorée) : 2.1 → 3.1 à l’intérieur de 4.5 ---
             # Réutiliser les options choisies dans le GUI :
             #   normalize_method, weight_method, reject_algo, final_combine,
             #   kappa/winsor/workers déjà présents dans stack_cfg/stack_kwargs.
             try:
                 norm_method = str(
                     stack_cfg.get(
                         "stacking_normalize_method",
                         stack_cfg.get(
                             "normalize_method",
                             stack_cfg.get("stack_norm_method", "none"),
                         ),
                     )
                 ).lower()
             except Exception:
                 norm_method = "none"
             try:
                 weight_method = str(stack_cfg.get("weight_method", stack_cfg.get("stack_weight_method", "none"))).lower()
             except Exception:
                 weight_method = "none"
             try:
                 reject_algo = str(stack_cfg.get("reject_algo", stack_cfg.get("stack_reject_algo", "winsorized_sigma_clip"))).lower()
             except Exception:
@@ -5250,57 +5251,57 @@ def _run_phase4_5_inter_master_merge(
                                 ref_vals = ref_chan[ref_mask]
                                 src_vals = src_chan[src_mask]
                                 ref_range = np.nanpercentile(ref_vals, [sky_low, sky_high])
                                 src_range = np.nanpercentile(src_vals, [sky_low, sky_high])
                                 ref_sel = (ref_vals >= ref_range[0]) & (ref_vals <= ref_range[1])
                                 src_sel = (src_vals >= src_range[0]) & (src_vals <= src_range[1])
                                 ref_clip = ref_vals[ref_sel] if np.any(ref_sel) else ref_vals
                                 src_clip = src_vals[src_sel] if np.any(src_sel) else src_vals
                                 if ref_clip.size == 0 or src_clip.size == 0:
                                     continue
                                 bg_ref = float(np.nanmedian(ref_clip))
                                 bg_src = float(np.nanmedian(src_clip))
                                 if not (math.isfinite(bg_ref) and math.isfinite(bg_src)):
                                     continue
                                 delta = bg_ref - bg_src
                                 src_chan[src_mask] = src_chan[src_mask] + delta
                 except Exception:
                     logger.debug(
                         "[P4.5][G%03d] Chunk photometric normalization skipped (error)",
                         group_id,
                         exc_info=True,
                     )
 
             # 4.5.c — empilement selon les réglages GUI
             logger.debug(
-                "[P4.5][G%03d] Stack params: reject=%s, combine=%s, kappa=%.2f, winsor_limits=%s, workers=%d, weight=%s",
+                "[P4.5][G%03d] Stack params: reject=%s, combine=%s, kappa=%.2f, winsor_limits=%s, workers=%s, weight=%s",
                 group_id,
                 reject_algo,
                 final_combine,
                 kappa_val,
                 limits_val,
-                stack_kwargs["winsor_max_workers"],
+                "AUTO(0)" if winsor_max_workers_val <= 0 else str(stack_kwargs["winsor_max_workers"]),
                 weight_method,
             )
             _phase45_gui_message(
                 f"Phase 4.5: group {group_id} stacking ({reject_algo}/{final_combine})"
             )
             alpha_out = None
             alpha_sources: list[str] = []
             weights_ready = any(isinstance(w, np.ndarray) for w in frame_weights)
             super_arr = None
             if weights_ready:
                 try:
                     frames_np = np.stack(frames, axis=0).astype(np.float32, copy=False)
                     reference_shape = frames_np.shape[1:3]
                     weight_stack_list: list[np.ndarray] = []
                     for wmap in frame_weights:
                         if isinstance(wmap, np.ndarray) and wmap.shape == reference_shape:
                             weight_stack_list.append(wmap.astype(np.float32, copy=False))
                         else:
                             weight_stack_list.append(np.ones(reference_shape, dtype=np.float32))
                     weight_stack = np.stack(weight_stack_list, axis=0)
                     weight_stack = np.clip(np.nan_to_num(weight_stack, nan=0.0), 0.0, 1.0)
                     weight_expanded = weight_stack[..., None]
                     num = np.nansum(frames_np * weight_expanded, axis=0)
                     den = np.nansum(weight_expanded, axis=0)
                     super_arr = np.where(den > 0, num / den, np.nan)
@@ -17787,59 +17788,80 @@ def run_hierarchical_mosaic_classic_legacy(
                     seestar_stack_groups = _merge_small_groups(
                         seestar_stack_groups,
                         min_size=min_value,
                         cap=cap_value,
                     )
     
         # Do not subdivide groups if a target group count is set; respect clustering first.
         if (
             not preplan_groups_active
             and (cluster_target_groups_config is None or int(cluster_target_groups_config) <= 0)
             and max_raw_per_master_tile_config
             and max_raw_per_master_tile_config > 0
         ):
             new_groups = []
             for g in seestar_stack_groups:
                 for i in range(0, len(g), max_raw_per_master_tile_config):
                     new_groups.append(g[i:i + max_raw_per_master_tile_config])
             if len(new_groups) != len(seestar_stack_groups):
                 pcb(
                     "clusterstacks_info_groups_split_manual_limit",
                     prog=None,
                     lvl="INFO_DETAIL",
                     original=len(seestar_stack_groups),
                     new=len(new_groups),
                     limit=max_raw_per_master_tile_config,
-                )
+            )
             seestar_stack_groups = new_groups
         cpu_total = os.cpu_count() or 1
-        winsor_worker_limit = max(1, min(int(winsor_worker_limit_config), cpu_total))
+        try:
+            winsor_worker_limit_cfg = int(winsor_worker_limit_config)
+        except Exception:
+            winsor_worker_limit_cfg = 1
+        winsor_auto = winsor_worker_limit_cfg <= 0
+        if winsor_auto:
+            candidate = 0
+            if global_parallel_plan and getattr(global_parallel_plan, "cpu_workers", 0) > 0:
+                candidate = int(getattr(global_parallel_plan, "cpu_workers", 0))
+            elif effective_base_workers and effective_base_workers > 0:
+                candidate = int(effective_base_workers)
+            else:
+                candidate = cpu_total
+        else:
+            candidate = winsor_worker_limit_cfg
+        winsor_worker_limit = max(1, min(int(candidate), cpu_total))
         winsor_max_frames_per_pass = max(0, int(winsor_max_frames_per_pass_config))
         global_wcs_plan["winsor_worker_limit"] = int(winsor_worker_limit)
         global_wcs_plan["winsor_max_frames_per_pass"] = int(winsor_max_frames_per_pass)
         global_wcs_plan["use_align_helpers"] = True
         global_wcs_plan["prefer_gpu_helpers"] = bool(use_gpu_phase5_flag)
+        if winsor_auto:
+            pcb(
+                f"Winsor worker limit: AUTO (cfg={winsor_worker_limit_cfg}) -> resolved={winsor_worker_limit} (cpu_total={cpu_total})",
+                prog=None,
+                lvl="INFO_DETAIL",
+            )
         pcb(
             f"Winsor worker limit set to {winsor_worker_limit}" + (
                 " (ProcessPoolExecutor enabled)" if winsor_worker_limit > 1 else ""
             ),
             prog=None,
             lvl="INFO",
         )
         if winsor_max_frames_per_pass > 0:
             pcb(
                 f"Winsor streaming limit set to {winsor_max_frames_per_pass} frame(s) per pass",
                 prog=None,
                 lvl="INFO_DETAIL",
             )
         sds_stack_params = {
             "stack_reject_algo": stack_reject_algo,
             "stack_weight_method": stack_weight_method,
             "stack_norm_method": stack_norm_method,
             "stack_kappa_low": stack_kappa_low,
             "stack_kappa_high": stack_kappa_high,
             "stack_final_combine": stack_final_combine,
             "parsed_winsor_limits": parsed_winsor_limits,
             "winsor_worker_limit": winsor_worker_limit,
             "winsor_max_frames_per_pass": winsor_max_frames_per_pass,
             "apply_radial_weight": apply_radial_weight_config,
             "radial_feather_fraction": radial_feather_fraction_config,
@@ -21930,56 +21952,77 @@ def run_hierarchical_mosaic(
                 batches_idx = [list(range(len(ordered_group)))]
             for batch_indices in batches_idx:
                 batch = [ordered_group[i] for i in batch_indices if 0 <= i < len(ordered_group)]
                 if batch:
                     overlapping_groups.append(batch)
         if overlapping_groups:
             pcb(
                 "[Batching] cap overlap applied",
                 prog=None,
                 lvl="INFO_DETAIL",
                 cap=int(overlap_cap),
                 overlap=float(overlap_fraction_config),
                 step=int(effective_step),
                 batches=int(len(overlapping_groups)),
             )
             seestar_stack_groups = overlapping_groups
         else:
             pcb(
                 "[Batching] overlap skipped (no groups produced)",
                 prog=None,
                 lvl="DEBUG_DETAIL",
                 cap=int(overlap_cap),
                 overlap=float(overlap_fraction_config),
             )
     cpu_total = os.cpu_count() or 1
-    winsor_worker_limit = max(1, min(int(winsor_worker_limit_config), cpu_total))
+    try:
+        winsor_worker_limit_cfg = int(winsor_worker_limit_config)
+    except Exception:
+        winsor_worker_limit_cfg = 1
+    winsor_auto = winsor_worker_limit_cfg <= 0
+    if winsor_auto:
+        candidate = 0
+        if global_parallel_plan and getattr(global_parallel_plan, "cpu_workers", 0) > 0:
+            candidate = int(getattr(global_parallel_plan, "cpu_workers", 0))
+        elif effective_base_workers and effective_base_workers > 0:
+            candidate = int(effective_base_workers)
+        else:
+            candidate = cpu_total
+    else:
+        candidate = winsor_worker_limit_cfg
+    winsor_worker_limit = max(1, min(int(candidate), cpu_total))
     winsor_max_frames_per_pass = max(0, int(winsor_max_frames_per_pass_config))
     global_wcs_plan["winsor_worker_limit"] = int(winsor_worker_limit)
     global_wcs_plan["winsor_max_frames_per_pass"] = int(winsor_max_frames_per_pass)
     global_wcs_plan["use_align_helpers"] = True
     global_wcs_plan["prefer_gpu_helpers"] = bool(use_gpu_phase5_flag)
+    if winsor_auto:
+        pcb(
+            f"Winsor worker limit: AUTO (cfg={winsor_worker_limit_cfg}) -> resolved={winsor_worker_limit} (cpu_total={cpu_total})",
+            prog=None,
+            lvl="INFO_DETAIL",
+        )
     pcb(
         f"Winsor worker limit set to {winsor_worker_limit}" + (
             " (ProcessPoolExecutor enabled)" if winsor_worker_limit > 1 else ""
         ),
         prog=None,
         lvl="INFO",
     )
     if winsor_max_frames_per_pass > 0:
         pcb(
             f"Winsor streaming limit set to {winsor_max_frames_per_pass} frame(s) per pass",
             prog=None,
             lvl="INFO_DETAIL",
         )
     sds_stack_params = {
         "stack_reject_algo": stack_reject_algo,
         "stack_weight_method": stack_weight_method,
         "stack_norm_method": stack_norm_method,
         "stack_kappa_low": stack_kappa_low,
         "stack_kappa_high": stack_kappa_high,
         "stack_final_combine": stack_final_combine,
         "parsed_winsor_limits": parsed_winsor_limits,
         "winsor_worker_limit": winsor_worker_limit,
         "winsor_max_frames_per_pass": winsor_max_frames_per_pass,
         "apply_radial_weight": apply_radial_weight_config,
         "radial_feather_fraction": radial_feather_fraction_config,
