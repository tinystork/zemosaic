diff --git a/agent.md b/agent.md
index 9db0d2cf24d6661d11bd3086ea489e279c2477b1..20cf86d038cefec25628461e255207a4c2bbfbf1 100644
--- a/agent.md
+++ b/agent.md
@@ -20,54 +20,54 @@ Patch minimal, localisé à Phase 5 reproject:
 ## Fichiers probables
 - `zemosaic_worker.py` (ou fichier équivalent orchestration Phase 5)
 - éventuellement `solver_settings.py` si le plan est défini là, mais privilégier un patch dans le worker au moment
   où le plan est finalisé (Phase 5).
 - Ne pas modifier `zemosaic_gpu_safety.py` (sauf si impossible), objectif: patch le plus local possible.
 
 ## Spécification de calcul (simple & conservatrice)
 Estimer un coût "bytes par ligne" pour la reprojection GPU:
 - Hypothèse conservatrice: on manipule au moins 2 buffers float32 (accumulateur + poids),
   et on itère sur N tiles.
 - bytes_per_row ≈ out_w * 4 (float32) * buffers_per_tile_effective * n_tiles_scale
   On n'a pas besoin d'être exact, seulement éviter des valeurs trop grandes.
 
 Proposition robuste (sans connaître tous les détails internes):
 - `bytes_per_row = max(1, out_w * 4 * max(2, buffers))`
 - puis diviser le budget par `max(1, n_tiles)` pour rester conservateur côté mémoire "par tile"
   (même si l'impl GPU ne garde pas tout simultanément).
 - `rows_budget = gpu_max_chunk_bytes // (bytes_per_row * max(1, n_tiles))`
 - `new_rows = clamp(rows_budget, min_rows=current_rows, max_rows=256)`
 - Ajouter un plancher raisonnable (ex: 96) si ça ne dépasse pas le current.
 
 Si des infos plus précises existent déjà (ex: taille réelle des buffers ou estimateur interne),
 les utiliser à la place (mais sans refactor).
 
 ## Étapes
-1. Localiser dans le code le point Phase 5 où:
-   - le plan est créé
-   - `apply_gpu_safety_to_parallel_plan(... operation="global_reproject")` est appelé
-2. Ajouter juste après ce call un petit ajustement conditionnel:
-   - uniquement si `operation == "global_reproject"`
-   - uniquement si `safe_mode == 1`
-   - uniquement si `on_battery == False` (ou `power_plugged == True`)
-3. Recalculer `gpu_rows_per_chunk` selon une estimation simple basée sur:
-   - `plan.gpu_max_chunk_bytes` (ou param correspondant)
-   - `out_w` (largeur de sortie) accessible depuis le contexte Phase 5
-   - `n_tiles` (nombre de master tiles en input) accessible depuis la phase
-4. Ajouter un log INFO clair:
-   - "Phase5: reproject rows_per_chunk bumped from X to Y (not on battery), max_chunk_bytes=..."
-5. S'assurer que si des champs manquent (out_w/n_tiles), on n'échoue pas:
-   - fallback: ne rien changer
+- [x] Localiser dans le code le point Phase 5 où:
+  - le plan est créé
+  - `apply_gpu_safety_to_parallel_plan(... operation="global_reproject")` est appelé
+- [x] Ajouter juste après ce call un petit ajustement conditionnel:
+  - uniquement si `operation == "global_reproject"`
+  - uniquement si `safe_mode == 1`
+  - uniquement si `on_battery == False` (ou `power_plugged == True`)
+- [x] Recalculer `gpu_rows_per_chunk` selon une estimation simple basée sur:
+  - `plan.gpu_max_chunk_bytes` (ou param correspondant)
+  - `out_w` (largeur de sortie) accessible depuis le contexte Phase 5
+  - `n_tiles` (nombre de master tiles en input) accessible depuis la phase
+- [x] Ajouter un log INFO clair:
+  - "Phase5: reproject rows_per_chunk bumped from X to Y (not on battery), max_chunk_bytes=..."
+- [x] S'assurer que si des champs manquent (out_w/n_tiles), on n'échoue pas:
+  - fallback: ne rien changer
 
 ## Critères d'acceptation
 - Sur un run secteur (pas sur batterie), `gpu_rows_per_chunk` augmente (ex: 69 → ~200),
   nombre de chunks réduit sensiblement, meilleure occupation GPU, sans dépassement mémoire.
 - Sur batterie, comportement inchangé.
 - Aucun changement de résultat scientifique attendu (juste la granularité).
 - Aucune régression sur les autres phases.
 
 ## Tests (léger)
-- Ajouter un mini test unitaire si la suite existe:
+- [x] Ajouter un mini test unitaire si la suite existe:
   - Simuler un plan avec gpu_max_chunk_bytes=128MB, out_w=2282, n_tiles=30, current_rows=69
   - Vérifier que new_rows > current_rows et <= 256 quand on_battery=False
   - Vérifier new_rows == current_rows quand on_battery=True
 Si la repo n'a pas de tests, au minimum ajouter un "self-check" dans log (pas de test framework).
diff --git a/followup.md b/followup.md
index 119e2b614d79e09af4464be8912946ec680db505..9a78a20471dc450c8c99d7a27a7ebe56c266e047 100644
--- a/followup.md
+++ b/followup.md
@@ -1,68 +1,68 @@
 # Follow-up: Implémentation & vérifications (Phase 5 rows_per_chunk bump)
 
 ## 1) Localisation exacte
-- Ouvrir `zemosaic_worker.py` et trouver la section Phase 5 "Reproject & Coadd".
-- Repérer l'appel à `apply_gpu_safety_to_parallel_plan(..., operation="global_reproject")`.
-- Repérer où sont disponibles:
+- [x] Ouvrir `zemosaic_worker.py` et trouver la section Phase 5 "Reproject & Coadd".
+- [x] Repérer l'appel à `apply_gpu_safety_to_parallel_plan(..., operation="global_reproject")`.
+- [x] Repérer où sont disponibles:
   - `out_w` (ou une shape/width équivalente de l'image/canvas de sortie)
   - `n_tiles` (len(master_tiles) / tiles list / inputs)
   - flags d'alimentation (`on_battery`, `power_plugged`) si déjà loggés / détectés.
 
 ## 2) Patch minimal (juste après le safety)
-Ajouter un bloc du style (adapter aux noms réels, sans refactor):
+- [x] Ajouter un bloc du style (adapter aux noms réels, sans refactor):
 
 - Conditions:
   - `operation == "global_reproject"`
   - `getattr(safety, "safe_mode", 0) == 1` OU `plan.safe_mode == 1` (selon où c'est stocké)
   - `on_battery is False` (ou `power_plugged is True`)
 - Valeurs:
   - `current = plan.gpu_rows_per_chunk` (si absent: ne rien faire)
   - `max_bytes = plan.gpu_max_chunk_bytes` (sinon: ne rien faire)
   - `out_w = ...`
   - `n_tiles = ...`
 
 - Estimation conservative:
   - `buffers = 2` (accum + weight)
   - `bytes_per_row = max(1, out_w * 4 * buffers)`
   - `den = max(1, n_tiles)`
   - `rows_budget = max_bytes // (bytes_per_row * den)`
   - `candidate = int(rows_budget)`
   - `new_rows = min(256, max(current, candidate))`
   - Optionnel: `new_rows = max(new_rows, min(96, 256))` uniquement si `new_rows > current` sinon rien
   - Si `new_rows > current`: assigner et logguer.
 
 Important: si `candidate` est absurde (0 ou < current), ne rien changer.
 
 ## 3) Logging
-Ajouter une ligne INFO (même logger que le worker):
+- [x] Ajouter une ligne INFO (même logger que le worker):
 - Avant/après + contexte:
   - safe_mode, on_battery/power_plugged, max_chunk_bytes, out_w, n_tiles
 
 Ex:
 "Phase5 GPU: bump rows_per_chunk 69 -> 224 (plugged), max_chunk=128MB, out_w=2282, n_tiles=30"
 
 ## 4) Robustesse
-- Aucun import nouveau lourd.
-- Pas d'exception si attributs manquent:
+- [x] Aucun import nouveau lourd.
+- [x] Pas d'exception si attributs manquent:
   - utiliser `getattr(...)` + early return.
-- Ne pas toucher au multi-threading.
-- Ne pas changer la logique batch size (0 vs >1).
+- [x] Ne pas toucher au multi-threading.
+- [x] Ne pas changer la logique batch size (0 vs >1).
 
 ## 5) Tests rapides
 ### Option A: pytest (si présent)
-Créer/compléter un test simple qui appelle la fonction/helper si tu en crées une mini locale,
+- [x] Créer/compléter un test simple qui appelle la fonction/helper si tu en crées une mini locale,
 ou bien tester via un petit "plan" factice (dataclass/dict) si c'est déjà le style du repo.
 
 Cas:
-- plugged: augmente et <=256
-- on_battery: inchangé
+- [x] plugged: augmente et <=256
+- [x] on_battery: inchangé
 
 ### Option B: smoke run (si pas de tests)
 - Lancer un run court (ex: 10–20 tuiles) en secteur.
 - Vérifier dans le log que `rows_per_chunk` est bump.
 - Observer que la phase reproject fait moins d'itérations/chunks.
 
 ## 6) Résultat attendu
 - Moins de micro-chunks → moins d'overhead.
 - GPU davantage sollicité (sans chercher 100%, mais au moins une montée perceptible).
 - Pas de freeze, car `gpu_max_chunk_bytes` reste identique (128MB).
diff --git a/tests/test_phase5_gpu.py b/tests/test_phase5_gpu.py
index 9fe07aabd2c519aea684c6b48cbe1d7e3d483bd4..f5f1fa4aeda46ea16d9a03e1b0c2521d37fa4a84 100644
--- a/tests/test_phase5_gpu.py
+++ b/tests/test_phase5_gpu.py
@@ -1,57 +1,89 @@
+import logging
 import sys
 from pathlib import Path
 from types import SimpleNamespace
 
 import numpy as np
 import pytest
 
 REPO_ROOT = Path(__file__).resolve().parents[1]
 if str(REPO_ROOT) not in sys.path:
     sys.path.insert(0, str(REPO_ROOT))
 
 import zemosaic_worker as zw
 
 
 def test_should_use_gpu_helper_respects_plan(monkeypatch):
     """should_use_gpu_for_reproject honours config, plan, and GPU availability."""
 
     monkeypatch.setattr(zw, "gpu_is_available", lambda: True)
     zw.reset_phase5_gpu_runtime_state()
     config = {"use_gpu_phase5": True}
     plan = type("Plan", (), {"use_gpu": True})()
     assert zw.should_use_gpu_for_reproject("phase5_reproject_coadd", config, plan)
 
     plan.use_gpu = False
     assert not zw.should_use_gpu_for_reproject("phase5_reproject_coadd", config, plan)
 
     plan.use_gpu = True
     config["use_gpu_phase5"] = False
     assert not zw.should_use_gpu_for_reproject("phase5_reproject_coadd", config, plan)
     zw.reset_phase5_gpu_runtime_state()
 
 
+def test_phase5_rows_per_chunk_bumps_when_plugged(caplog):
+    plan = SimpleNamespace(
+        gpu_rows_per_chunk=69,
+        gpu_max_chunk_bytes=128 * 1024 * 1024,
+        use_gpu=True,
+    )
+    ctx = SimpleNamespace(safe_mode=1, on_battery=False, power_plugged=True)
+
+    with caplog.at_level(logging.INFO):
+        zw._maybe_bump_phase5_gpu_rows_per_chunk(plan, ctx, (100, 2282), 30, logging.getLogger(__name__))
+
+    assert plan.gpu_rows_per_chunk > 69
+    assert plan.gpu_rows_per_chunk <= 256
+    assert any("Phase5 GPU: bump rows_per_chunk" in msg for msg in caplog.messages)
+
+
+def test_phase5_rows_per_chunk_skips_on_battery(caplog):
+    plan = SimpleNamespace(
+        gpu_rows_per_chunk=69,
+        gpu_max_chunk_bytes=128 * 1024 * 1024,
+        use_gpu=True,
+    )
+    ctx = SimpleNamespace(safe_mode=1, on_battery=True, power_plugged=False)
+
+    with caplog.at_level(logging.INFO):
+        zw._maybe_bump_phase5_gpu_rows_per_chunk(plan, ctx, (100, 2282), 30, logging.getLogger(__name__))
+
+    assert plan.gpu_rows_per_chunk == 69
+    assert not any("Phase5 GPU: bump rows_per_chunk" in msg for msg in caplog.messages)
+
+
 def test_two_pass_gpu_error_falls_back_to_cpu(monkeypatch):
     """Simulate a GPU failure and ensure CPU fallback completes."""
 
     monkeypatch.setattr(zw, "REPROJECT_AVAILABLE", True)
     monkeypatch.setattr(zw, "ASTROPY_AVAILABLE", True)
     monkeypatch.setattr(zw, "reproject_and_coadd", lambda *args, **kwargs: None)
     monkeypatch.setattr(
         zw,
         "reproject_interp",
         lambda *args, **kwargs: (np.ones((2, 2), dtype=np.float32), np.ones((2, 2), dtype=np.float32)),
     )
     zw.reset_phase5_gpu_runtime_state()
 
     call_log: list[bool] = []
 
     def fake_reproject_wrapper(*args, **kwargs):
         use_gpu = kwargs.get("use_gpu", False)
         call_log.append(bool(use_gpu))
         if use_gpu:
             raise RuntimeError("gpu boom")
         shape_out = kwargs.get("shape_out") or (2, 2)
         mosaic = np.full(shape_out, 5.0, dtype=np.float32)
         coverage = np.ones(shape_out, dtype=np.float32)
         return mosaic, coverage
 
diff --git a/zemosaic_worker.py b/zemosaic_worker.py
index 2a39fddbc262cb3e9c758df7c46470d706e9ce9c..c8aa032f07a4766a80fcf0f738e0493c0f1faf52 100644
--- a/zemosaic_worker.py
+++ b/zemosaic_worker.py
@@ -2361,50 +2361,116 @@ def _equalize_rgb_black_level_hwc(
                 logger.info(
                     "[RGB-BL] applied=True p_low=%.3f offsets=(%.3f, %.3f, %.3f) valid_min=(%.5f, %.5f, %.5f) valid_px=%d",
                     p_low,
                     offsets[0],
                     offsets[1],
                     offsets[2],
                     mins[0],
                     mins[1],
                     mins[2],
                     valid_px,
                 )
             else:
                 logger.info(
                     "[RGB-BL] applied=True p_low=%.3f offsets=(%.3f, %.3f, %.3f) valid_px=%d",
                     p_low,
                     offsets[0],
                     offsets[1],
                     offsets[2],
                     valid_px,
                 )
         return out, info
 
     return rgb_hwc, info
 
 
+def _maybe_bump_phase5_gpu_rows_per_chunk(
+    plan: Any,
+    gpu_ctx: Any,
+    output_shape_hw: tuple[int, int] | None,
+    tiles_total: int,
+    logger: logging.Logger | None,
+) -> None:
+    """Conservatively increase Phase 5 GPU chunk rows when safe mode allows it."""
+
+    try:
+        safe_mode = int(getattr(gpu_ctx, "safe_mode", 0) or 0)
+    except Exception:
+        return
+
+    if safe_mode != 1:
+        return
+
+    on_battery = getattr(gpu_ctx, "on_battery", None)
+    power_plugged = getattr(gpu_ctx, "power_plugged", None)
+    if not ((on_battery is False) or (power_plugged is True)):
+        return
+
+    try:
+        current_rows = getattr(plan, "gpu_rows_per_chunk", None)
+        max_bytes = getattr(plan, "gpu_max_chunk_bytes", None)
+    except Exception:
+        return
+
+    try:
+        out_w = int(output_shape_hw[1]) if output_shape_hw and len(output_shape_hw) > 1 else None
+    except Exception:
+        out_w = None
+
+    if not current_rows or not max_bytes or not out_w or out_w <= 0:
+        return
+
+    bytes_per_row = max(1, out_w * 4 * 2)
+    rows_budget = int(max_bytes) // (bytes_per_row * max(1, int(tiles_total or 0)))
+    candidate = int(rows_budget)
+    if candidate <= current_rows or candidate <= 0:
+        return
+
+    new_rows = min(256, max(current_rows, candidate))
+    new_rows = max(new_rows, min(96, 256))
+    if new_rows <= current_rows:
+        return
+
+    try:
+        plan.gpu_rows_per_chunk = new_rows
+    except Exception:
+        return
+
+    if logger:
+        try:
+            logger.info(
+                "Phase5 GPU: bump rows_per_chunk %s -> %s (plugged), max_chunk=%.1fMB, out_w=%s, n_tiles=%s",
+                current_rows,
+                new_rows,
+                float(max_bytes) / (1024.0 ** 2),
+                out_w,
+                max(1, int(tiles_total or 0)),
+            )
+        except Exception:
+            pass
+
+
 def _apply_phase5_post_stack_pipeline(
     final_mosaic_data: np.ndarray | None,
     final_mosaic_coverage: np.ndarray | None,
     final_alpha_map: np.ndarray | None,
     *,
     enable_lecropper_pipeline: bool,
     pipeline_cfg: dict[str, Any] | None,
     enable_master_tile_crop: bool,
     master_tile_crop_percent: float,
     two_pass_enabled: bool,
     two_pass_sigma_px: int,
     two_pass_gain_clip: tuple[float, float],
     final_output_wcs: Any,
     final_output_shape_hw: tuple[int, int] | None,
     use_gpu_two_pass: bool,
     logger: logging.Logger | None,
     collected_tiles: list[tuple[np.ndarray, Any]] | None = None,
     fallback_two_pass_loader: Callable[[], tuple[list[np.ndarray], list[Any]]] | None = None,
     parallel_plan: ParallelPlan | None = None,
     telemetry_ctrl: ResourceTelemetryController | None = None,
 ) -> tuple[np.ndarray | None, np.ndarray | None, np.ndarray | None]:
     """Run the reusable Phase 5 post-stack operations."""
 
     if enable_lecropper_pipeline:
         (
@@ -8161,50 +8227,57 @@ def _run_shared_phase45_phase5_pipeline(
                     kind="global_reproject",
                     frame_shape=frame_shape_phase5,
                     n_frames=n_frames_phase5,
                     bytes_per_pixel=4,
                     config=worker_config_cache,
                     caps=caps_for_phase5,
                 )
                 parallel_plan_phase5, gpu_safety_ctx_phase5 = apply_gpu_safety_to_parallel_plan(
                     parallel_plan_phase5,
                     caps_for_phase5,
                     worker_config_cache,
                     operation="global_reproject",
                     logger=logger,
                 )
                 if gpu_safety_ctx_phase5 is not None:
                     try:
                         use_gpu_phase5_flag = apply_gpu_safety_to_phase5_flag(
                             use_gpu_phase5_flag, gpu_safety_ctx_phase5, logger=logger
                         )
                     except Exception:
                         pass
                     try:
                         get_env_safe_mode_flag(gpu_safety_ctx_phase5)
                     except Exception:
                         pass
+                _maybe_bump_phase5_gpu_rows_per_chunk(
+                    parallel_plan_phase5,
+                    gpu_safety_ctx_phase5,
+                    final_output_shape_hw,
+                    tiles_total_phase5,
+                    logger,
+                )
                 worker_config_cache["parallel_plan_phase5"] = parallel_plan_phase5
                 try:
                     setattr(zconfig, "parallel_plan_phase5", parallel_plan_phase5)
                 except Exception:
                     pass
             except Exception as exc_phase5_plan:
                 logger.warning("Phase5 auto-tune failed, falling back to global plan: %s", exc_phase5_plan)
 
         _emit_phase5_stats(0, tiles_total_phase5, force=True, stage="start")
         plan_gpu_allowed = bool(getattr(parallel_plan_phase5, "use_gpu", False))
         if not use_gpu_phase5_flag:
             effective_use_gpu = False
         else:
             effective_use_gpu = bool(plan_gpu_allowed)
             if not effective_use_gpu and use_gpu_phase5_flag:
                 logger.info("[Phase5] GPU disabled by auto-tune (plan.use_gpu=False)")
         try:
             vram_free_mb = None
             if gpu_safety_ctx_phase5 is not None and gpu_safety_ctx_phase5.vram_free_bytes is not None:
                 vram_free_mb = float(gpu_safety_ctx_phase5.vram_free_bytes) / (1024.0 ** 2)
             reasons_str = ",".join(getattr(gpu_safety_ctx_phase5, "reasons", []) or [])
             logger.info(
                 "[GPU_SAFETY] safe_mode=%d vendor=%s hybrid=%s battery=%s vram_free_mb=%s "
                 "-> phase5_gpu=%d plan.use_gpu=%d gpu_max_chunk_mb=%.1f gpu_rows=%s reason=\"%s\"",
                 1 if gpu_safety_ctx_phase5 and gpu_safety_ctx_phase5.safe_mode else 0,
